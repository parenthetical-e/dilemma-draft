\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\bibstyle{vancouver-elife}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{gittins1974dynamic}
\citation{Kearns2002}
\citation{Brafman2002}
\citation{Sutton1990,dayan1996exploration,Sutton1998-kw,daw2006cortical,findling2018computational,Friston2016}
\citation{Thrun1992a,Dayan1996,findling2018computational,gershman2018deconstructing}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{Introduction}{1}{section*.1}}
\@writefile{toc}{\contentsline {section}{Results}{1}{section*.2}}
\@writefile{toc}{\contentsline {subsection}{The dilemma problem}{1}{subsection*.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Two views of exploration and exploitation. \textbf  {A}. The classic dilemma: either exploit an action with a known reward (e.g., return to the previous plant) or explore other actions on the chance they will return a better outcome (e.g., find a plant with more flowers). \textbf  {B.} Here we offer an alternative view of the dilemma, with two different competitive goals: maximize rewards (e.g., keep returning to known flower locations) {or} build a world model by learning new information (e.g., layout of the environment). Exploration here focused on learning in general, not on reward learning specifically. \textit  {Artist credit}: Richard Grant.\relax }}{2}{figure.caption.4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:f1}{{1}{2}{Two views of exploration and exploitation. \textbf {A}. The classic dilemma: either exploit an action with a known reward (e.g., return to the previous plant) or explore other actions on the chance they will return a better outcome (e.g., find a plant with more flowers). \textbf {B.} Here we offer an alternative view of the dilemma, with two different competitive goals: maximize rewards (e.g., keep returning to known flower locations) \texti {or} build a world model by learning new information (e.g., layout of the environment). Exploration here focused on learning in general, not on reward learning specifically. \textit {Artist credit}: Richard Grant.\relax }{figure.caption.4}{}}
\newlabel{eq:shaping}{{1}{2}{}{equation.0.1}{}}
\newlabel{eq:R}{{2}{2}{}{equation.0.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Formality}{2}{subsubsection*.5}}
\citation{Ng1999}
\citation{Ng1999}
\citation{Ng1999}
\citation{Ng1999}
\newlabel{eq:potential}{{3}{3}{}{equation.0.3}{}}
\newlabel{th:potential}{{1}{3}{Exploration bias}{theorem.1}{}}
\citation{mehlhorn2015unpacking,hughes1997intrinsic,Gottlieb2018,kidd2015psychology}
\citation{mehlhorn2015unpacking}
\citation{Ahilan2019,Poucet1993}
\newlabel{sec:dilemma}{{}{4}{}{theorem.1}{}}
\newlabel{conj:1}{{1}{4}{}{conjecture.1}{}}
\newlabel{conj:2}{{2}{4}{}{conjecture.2}{}}
\newlabel{conj:3}{{3}{4}{}{conjecture.3}{}}
\newlabel{eq:dualvalue}{{4}{4}{}{equation.0.4}{}}
\citation{Schmidhuber1991,Ha2018}
\citation{Kakade2002,Bellemare2016,Dayan1993,Schmidhuber1991,Pathak2017,Friston2016,Yang2019,Lopes2012Miller1956,Tulving2002,Park2017,Itti2009,Friston2016,Tenenbaum2006,Kingma2013,Ganguli2008,Ha2018,Schmidhuber2015a,Mante2013}
\citation{Kakade2002}
\citation{Bellemare2016,Dayan1993}
\citation{Itti2009,Friston2016,Tenenbaum2006,Schmidhuber1991,Pathak2017,Friston2016}
\citation{Kingma2013,Schmidhuber2008,Levi-Aharoni2019,Ganguli2010,Ha2018,Schmidhuber2015a,Mante2013,Park2017}
\citation{Sutton2018}
\citation{Kakade2002}
\citation{Bellemare2016,Dayan1993}
\citation{Schmidhuber1991,Pathak2017}
\citation{Pascanu2013}
\citation{Kakade2002}
\citation{Bellemare2016,Dayan1993}
\citation{Schmidhuber1991,Pathak2017}
\citation{Pascanu2013}
\citation{Park2017,Friston2016}
\citation{Haarnoja2018}
\citation{Kolchinsky2018}
\citation{Tishby2000}
\citation{Roughgarden2019,Bellmann1954}
\newlabel{ax:1}{{1}{6}{Axiom of Change}{axiom.1}{}}
\newlabel{def:d}{{2}{6}{Distance}{definition.2}{}}
\newlabel{def:total}{{3}{6}{Total distance}{definition.3}{}}
\newlabel{ax:5}{{2}{6}{Axiom of Equilibrium}{axiom.2}{}}
\citation{Bellmann1954}
\citation{Bellemare2016}
\citation{Bellmann1954,Roughgarden2019}
\citation{Bellmann1954,Roughgarden2019}
\citation{Bellmann1954}
\newlabel{eq:payout}{{5}{7}{}{equation.0.5}{}}
\newlabel{eq:V_E}{{6}{7}{}{equation.0.6}{}}
\citation{Hocker2019,Roughgarden2019}
\citation{Bellmann1954,Roughgarden2019}
\newlabel{eq:bellman_iter}{{7}{8}{}{equation.0.7}{}}
\@writefile{toc}{\contentsline {subsection}{Optimal exploration}{8}{subsection*.6}}
\newlabel{theorem:Z}{{2}{8}{State search: breadth}{theorem.2}{}}
\newlabel{theorem:convergence}{{3}{8}{State search: depth}{theorem.3}{}}
\newlabel{eq:V_R}{{8}{8}{}{equation.0.8}{}}
\newlabel{eq:pipi}{{9}{9}{}{equation.0.9}{}}
\newlabel{theorem:meta_total}{{4}{9}{Total value maximization of $\pi _{\pi }$}{theorem.4}{}}
\newlabel{theorem:meta}{{4}{9}{Total value maximization of $\pi _{\pi }$}{theorem.4}{}}
\newlabel{eq:ax2strict}{{10}{9}{}{equation.0.10}{}}
\newlabel{theorem:meta}{{5}{9}{Independence policy convergence under $\pi _{\pi }$}{theorem.5}{}}
\newlabel{sec:init_ties}{{}{9}{}{theorem.5}{}}
\citation{Sutton2018}
\citation{Sutton2018}
\citation{Sutton2018}
\citation{MacKay2003}
\citation{Mniha,Silver2016b,Silver2018}
\citation{Wu2018}
\citation{Sutton2018}
\citation{Sutton2018}
\citation{Mniha}
\citation{Jaegle2019,Schmidhuber1991}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   Bandits. Reward probabilities for each arm in bandit tasks I-IV. Grey dots highlight the optimal (i.e., highest reward probability) arm. See main text for a complete description.\relax }}{11}{figure.caption.7}}
\newlabel{fig:f2}{{2}{11}{Bandits. Reward probabilities for each arm in bandit tasks I-IV. Grey dots highlight the optimal (i.e., highest reward probability) arm. See main text for a complete description.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   Regret and total accumulated reward across models and bandit task. Median total regret (left column) and median total reward (right column) for simulations of each model type ($N=100$ experiments per model). See main text and Table~\ref  {tab:agents} for description of each model. Error bars in all plots represent median absolute deviation.\relax }}{12}{figure.caption.8}}
\newlabel{fig:f3}{{3}{12}{Regret and total accumulated reward across models and bandit task. Median total regret (left column) and median total reward (right column) for simulations of each model type ($N=100$ experiments per model). See main text and Table~\ref {tab:agents} for description of each model. Error bars in all plots represent median absolute deviation.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Exploration and value dynamics. \textbf  {A}. An example of our dual value learning algorithm during 500 trials on Bandit. The light purple line represents the boredom threshold $\eta $ (Eq.~\ref  {eq:pipi}). \textbf  {B.} An example of exploration dynamics (i.e arm selection) on Bandit. Note how the search is structured, and initially sequential. \textbf  {C-D.} Exploration dynamics for two other agents. \textbf  {C.} The Bayesian agent, which like our algorithm uses active sampling, and values information. Note how this shows a mixture of structures and repeated choices, mixed with seemingly random behavior. \textbf  {D.} The E-greedy agent, which uses purely random sampling. Note how here the agent is either greedy, repeating the same arm, or seemingly random.\relax }}{13}{figure.caption.9}}
\newlabel{fig:supf3}{{4}{13}{Exploration and value dynamics. \textbf {A}. An example of our dual value learning algorithm during 500 trials on Bandit. The light purple line represents the boredom threshold $\eta $ (Eq.~\ref {eq:pipi}). \textbf {B.} An example of exploration dynamics (i.e arm selection) on Bandit. Note how the search is structured, and initially sequential. \textbf {C-D.} Exploration dynamics for two other agents. \textbf {C.} The Bayesian agent, which like our algorithm uses active sampling, and values information. Note how this shows a mixture of structures and repeated choices, mixed with seemingly random behavior. \textbf {D.} The E-greedy agent, which uses purely random sampling. Note how here the agent is either greedy, repeating the same arm, or seemingly random.\relax }{figure.caption.9}{}}
\citation{Sutton2018}
\citation{burKeramati2014}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Artificial agents.\relax }}{14}{table.caption.10}}
\newlabel{tab:agents}{{1}{14}{Artificial agents.\relax }{table.caption.10}{}}
\newlabel{eq:homeo}{{11}{14}{}{equation.0.11}{}}
\citation{Kolchinsky2018,CogliatiDezza2017}
\citation{Kelly1956,Schmidhuber1991,Dayan1996,deAbril2018,Itti2009}
\citation{Sutton2018}
\citation{Schmidhuber1991,Berger-Tal2014,Itti2009,Friston2016}
\citation{Thrun1992a,Dayan1996,Findling2018,Gershman2018b}
\citation{Berlyne1950,Jaegle2019,Pathak2017}
\citation{Valiant1984}
\citation{Pathak2017,Burda2018,Colas2019}
\citation{Valiant1984}
\citation{Even-Dar2002}
\citation{Even-Dar2006,Strehl2009}
\citation{Berlyne1950,Kidd2015,Sutton2018}
\citation{Keramati2014,Juechems2019}
\citation{Valiant1984,Sutton2018}
\newlabel{eq:pipi_h1}{{12}{15}{}{equation.0.12}{}}
\newlabel{eq:pipi_h2}{{13}{15}{}{equation.0.13}{}}
\newlabel{eq:pipi_h3}{{14}{15}{}{equation.0.14}{}}
\citation{Calhoun2014,Song2019a,Gershman2018b,Schulz2018a}
\citation{Ha2018}
\citation{Sutton2018,Shyam2018}
\citation{Yosinski2014,Barreto2018}
\citation{Mniha,Silver2016b,Silver2018}
\citation{Wu2018}
\@writefile{toc}{\contentsline {section}{Methods and Materials}{16}{section*.11}}
\citation{Goodfellow-et-al-2016}
\citation{Itti2009,Friston2016}
\citation{Itti2009}
\citation{Lopez-Fidalgo2007}
\citation{Mackay,Still2012}
\citation{Ay2015}
\citation{Friston2016,Schwartenbeck2019}
\citation{Sutton2018}
\newlabel{eq:KL}{{15}{17}{}{equation.0.15}{}}
\newlabel{eq:TD}{{16}{17}{}{equation.0.16}{}}
\citation{Bergstra}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Hyperparameters for individual bandits (\textbf  {I}-\textbf  {IV}).\relax }}{18}{table.caption.12}}
\newlabel{tab:hp}{{2}{18}{Hyperparameters for individual bandits (\textbf {I}-\textbf {IV}).\relax }{table.caption.12}{}}
\newlabel{th:potential}{{1}{18}{Exploration bias}{theorem.1}{}}
\newlabel{supeq:f}{{17}{18}{}{equation.0.17}{}}
\newlabel{supeq:P}{{18}{18}{}{equation.0.18}{}}
\citation{Roughgarden2019,Sutton2018}
\citation{Roughgarden2019}
\newlabel{theorem:opt_sub}{{2}{19}{Optimal substructure}{theorem.2}{}}
\newlabel{eq:bellman_seq}{{19}{19}{}{equation.0.19}{}}
\citation{Roughgarden2019}
\citation{Keramati2014,Juechems2019}
\newlabel{eq:ineq}{{20}{20}{}{equation.0.20}{}}
\newlabel{theorem:Z}{{3}{20}{State search: breadth}{theorem.3}{}}
\newlabel{theorem:convergence}{{4}{20}{State search: depth}{theorem.4}{}}
\newlabel{sec:opt_pipi}{{}{20}{}{theorem.4}{}}
\newlabel{theorem:meta}{{5}{20}{Independence policy convergence under $\pi _{\pi }$}{theorem.5}{}}
\citation{Bellmann1954,Roughgarden2019}
\citation{Strogatz1994}
\citation{Hocker2019}
\citation{Roughgarden2019}
\bibdata{library}
\bibcite{Ahilan2019}{{1}{2019}{{Ahilan et~al.}}{{Ahilan, Sanjeevan and Solomon, Rebecca B. and Breton, Yannick-Andr{\'e} and Conover, Kent and Niyogi, Ritwik K. and Shizgal, Peter and Dayan, Peter}}}
\bibcite{Ay2015}{{2}{2015}{{Ay}}{{Ay, Nihat}}}
\bibcite{Barreto2018}{{3}{2018}{{Barreto et~al.}}{{Barreto, Andre and Dabney, Will and Munos, Remi and Hunt, Jonathan J and Schaul, Tom}}}
\bibcite{Bellemare2016}{{4}{2016}{{Bellemare et~al.}}{{Bellemare, Marc G. and Srini