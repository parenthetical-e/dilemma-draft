\section*{Methods and Materials}
\subsection*{Tasks}
We studied seven tasks, each designed to test exploration in a common setting or test the limits of curiosity. Despite their differences each task had the same basic structure. On each trial there were a set of $n$ choices, and the learner should try and learn the best one. Each choice action returns a “payout” according to a predetermined probability. Payouts are information, reward, or both (Fig.~\ref{fig:task_diagram}). Note that information was symbolic, denoted by a color code, “yellow” and “blue” and as is custom reward was a positive real number.

\textit{Task 1.} was designed to examine information foraging. At no point does this task generate rewards. There were four choices. Three of these generated either a ``yellow'' or ``blue'' symbol, with a probability of $p=0.99$. These have near zero entropy, and as result are an information poor foraging choice. The final choice had an even probability of generating either ``yellow'' of ``blue''. This was the maximum entropy choice, The ideal choice for information foraging. See Fig.~\ref{fig:payout}\textbf{a}. 

\textit{Task 2.} was designed to examine reward collection, in setting very common in the decision making literature. At no point does the task generate symbolic information. Rewards were 0 or 1. There were again four choices. The best choice had a payout of $p(R=1) = 0.8$. This is a much higher average payout than the others ($p(R=1) = 0.2$). See Fig.~\ref{fig:payout}\textbf{b}. 

\textit{Task 3.} was designed with very sparse rewards \cite{Mniha,Silver2016b,Silver2018}. There were 10 choices. The best choice had a payout of $p(R=1) = 0.02$. The other nine had, $p(R=1) = 0.01$ for all others. See Fig.~\ref{fig:payout}\textbf{c}. 

\textit{Task 4.} was designed with deceptive rewards. By deceptive we mean that the best long-term option presents itself initially with lower value. The best choice had a payout of $p(R>0) = 0.6$. The others had $p(R>0) = 0.4$.  Value for the best arm dips, then recovers. This is the “deception” It happens over the first 20 trials. Reward were real numbers, between 0-1. See Fig.~\ref{fig:payout}\textbf{d}. 

\textit{Task 5.} designed to distract our curiosity algorithm with irrelevant information. Like task 1, there is one best option ($p(R=1) = 0.8$) and 9 others ($p(R=1) = 0.2$). In addition to reward, the task generated information. Information was max entropy and uncorrelated with rewarding payouts. Every choice had even probability of generating either ``yellow'' of ``blue''. See Fig.~\ref{fig:payout}\textbf{e}.  

\textit{Tasks 6-7.} were designed with 121 choices, and a complex payout structure. Tasks of this size are at the limit of human performance \cite{Wu2018}. We first trained all learners on \textit{Task 6}, whose payout can be seen in Fig.\ref{fig:payout}\textbf{e}-\textbf{f}. This task, like the others, had a single best payout $p(R=1) = 0.8$. After training for this was complete, final scores were recorded as reported, and the agents were then challenged by \textit{Task 7}. \textit{Task 7} was identical except for the best option was changed to be the worst $p(R=0) = 0.2$ (Fig.~\ref{fig:payout}\textbf{f}). 

\textit{Task 7} was designed to examine how robust reward collection is to unexpected changes in the environment. It has a payout structure that was identical to \textit{Task 6}, except the best arm was changed to be the worst (($p(R=1) = 0.8$ became $p(R=1) = 0.1$). Learners which have a good model of the other choices values should recover more quickly leading to a greater degree of reward collected. Fig.~\ref{fig:payout}\textbf{b}). 

\subsection*{Task memory}
The tasks fit a simple discrete probabilistic model, with a memory “slot” for each choice. The memory space we defined for this is a probability space Fig~\ref{cartoon1}\textbf{a}..

To measure distances in this memory space we used the Jenson-Shannon metric (JS) \cite{Endres2003}, a variation of the Kullback--Leibler divergence. The Kullback--Leibler divergence (KL) is a widely used information theory, and Bayesian modelling. It measures the information gained or lost by replacing one distribution with another. It is versatile and widely used in machine learning \cite{Goodfellow-et-al-2016}, Bayesian reasoning \cite{Itti2009}, visual neuroscience \cite{Itti2009}, experimental design \cite{Lopez-Fidalgo2007}, compression \cite{Mackay,Still2012} and information geometry \cite{Ay2015}.

We used JS in place of KL to measure information value because it is a norm, while KL is not. The Bayesian exploration strategy described in the results used KL. In practice, we saw little difference in their performance.

Where $P$ and $Q$ are two distributions, KL and JS are given by Eq.~\ref{eq:KL} and ~\ref{eq:JS}. In our implementation $Q$ corresponded to the updated memory $\mathbf{M}_{t_1}$ and $P$ was a “copy” of the older memory $\mathbf{M}$.

\begin{equation}
    \label{eq:KL}
    KL(Q, P) = \sum_{s \in S} Q(s) \text{log} \frac{Q(s)}{P(s)} 
\end{equation}

\begin{equation}
    \label{eq:JS}
    JS(Q, P) = \frac{1}{2} \ \sqrt{KL(Q, P) + KL(P, Q)}
\end{equation}

\subsection*{Tie breaking in search}
By definition a greedy policy like $\pi_E$ does not distinguish between tied values; There is after all no mathematical way to rank equal values. Theorems~\ref{theorem:convergence} and~\ref{theorem:Z} ensure any tie breaking strategy is valid, so in the long term the choice is arbitrary. In the short-term, tie-breaking is important as it will affect choices. We expect different tie-breaking schemes will better fit different animals, their bodies, and environments. 

We studied several breaking strategies, ``take the left/right option'', ``take the closest option'', or ``take the option with the lowest likelihood''. For simplicity we choose to use ``take the next option'' in all our numerical studies. Most any tie breaking scheme is fine, but should it be deterministic.

\subsection*{Initializing $\pi_\pi$}
In these simulations we assume that at the start of learning an animal should have a uniform prior over the possible actions $A \in \mathbb{R}^K$. Thus $p(a_k) = 1/K$ for all $a_k \in A$. We transform this uniform prior into the appropriate units for our JS-based $\hat E$ using by, $E_0 = \sum_K p(a_k)\ \text{log}\ p(a_k)$. 

\subsection*{Regret calculations}
In line with past efforts on studying value loss during optimization we quantified lost value in terms of it regret $G$, where $G$ is defined as $G = V^* - V_t$ and $V$ is the value at the current time point and $V^*$ is the best possible value that has been experienced up until $t$.

\subsection*{Reward learning equations} Reinforcement learning was always done with the TD(0) learning rule \cite{Sutton2018}. This is given as Eq.~\ref{eq:TD} below. 

\begin{equation}
	\label{eq:TD}
	V(s) = V(s) + \alpha (\mathbf{R}_t - V(s)
\end{equation}

Where $V(s)$ is the value for each state (choice), $\mathbf{R}_t$ is the \emph{return} for the current trial, and $\alpha_R$ is the learning rate $(0-1]$. See the \emph{Hyperparameter optimization} section for information on how $\alpha_R$ chosen for each agent and task.

\subsection*{Hyperparameter optimization}
The hyperparameters for each learner were tuned independently for each task by random search \cite{Bergstra2012}. Generally we reported results for the top 10 values, sampled from between 1000 possibilities. Ranked values for all experiments are provided as supplementary material.


