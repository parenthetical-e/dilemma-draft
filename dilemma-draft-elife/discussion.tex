% TODO:
% 
% Learning rule independence means any rule will do. Testing, initially, should therefore be qualitative. For which we have three strong predictions to make. 

% 1. Invariant search strategy (axillary assumptions, learning is possible because there is some novelty, and some observations available)
% 2. Deterministic or highly structured search. Environmental noise is the only source of noise (axillary assumptions, no forgetting or action/parameter/neural noise)
% 3. Well defined pure periods (auxiliary assumptions, no other behavior-fear, avoiding, grooming)
% 4. At the circuit level exploration module for all domains, information and reward (co-activations will vary, as different observations are constructed)


\section*{Discussion}
We have offered a way around the exploration-exploitation dilemma. We proved the classic dilemma, found when optimizing exploration for reward value, can be resolved by exploring instead for curiosity. And that in this form the rational trade-off we are left with is solved with a classic strategy taken from game theory. The win-stay lose-shift strategy, that is.

This is a controversial claim. So we will address some of its criticisms as a series of questions. The final question explains how this approach can be tested, with behavioral experiments. 

\subsection*{Why curiosity?}
Curiosity is an ecologically rational behavoir \cite{Rich2016a}. This arguemnt rests on what we feel are three well established facts. Curiousity is just as important for survival as reward collection \cite{Thrun1992}. Curiosity is a primary drive in most, if not all, animal behavoir \cite{Inglis2001}. It is as strong, if not sometimes stronger, than the drive for reward \cite{Loewenstein1994,Kidd2015,Gottlieb2018}. Curiosity as an algorithm is highly effective at solving difficult optimization problems \cite{Schmidhuber1991,Pathak2017,Stanton2018,Lehman201,Mouret2011b1,Fister2019,Mouret2015,Colas2020,Cully2015,Pathak2017,Schwartenbeck2019.Laversanne-Finot2018}. 

In other words, curiosity is not irrational \cite{}, nor a paradox \cite{}, nor a heuristic \cite{}, nor even quitesssentially cognitive \cite{Berlyne1950}. It is a basic homeostatic drive \cite{Loewenstein1994}. 


\subsection*{What about information theory?}
In short, the problems of communication and value are different problems that require different theories.

Weaver \citep{Shannon1948} in his classic introduction to the topic, describes information as a communication problem with three levels. 1. The technical problem of transmitting accurately. 2. The semantic problem of meaning. 3. The effectiveness problem of changing behavior. He then describes how Shannon's work addresses only problem A. It has turned out that Shannon's separation of the problem allowed the information theory to have an incredibly broad application. 

Unfortunately valuing information is not, at its core, a communications problem. Consider the very simple example Bob and Alice, having a phone conversation and then Tom and Bob having the exact same conversation. The personal history of Bob and Alice will determine what Bob learns in their phone call, and so determines what he values in that phone call. These might be completely different from what he learns when talking to Tom, even if the conversations are identical. What we mean by this example is that the personal history defines what is valuable on a channel, and this is independent of the channel and the messages sent. We have summarized this diagrammatically, in Fig. ~\ref{fig:info1}.

\begin{figure}
\begin{fullwidth}
	\includegraphics[width=0.6\linewidth]{img/info_diagram.pdf} 
    \caption{The relationship between the technical problem of communication with the technical problem of value. Note how value is not derived from the channel directly. Value is derived from learning about observations from the channel, which is in turn dependent on memory and its past history.
    }
    \label{fig:info1} 
\end{fullwidth}
\end{figure}

There is, we argue, an analogous set of levels for information value as those Weaver describes. a. There is the technical problem of judging how much was learned. b. There is the semantic problem of both what this learning ``means'', and also what its consequences are. c. There is an effectiveness problem of using what was learned to some other effect. In many ways b and c are similar to those in the information theory. It is a. which is most different. 

For the same reasons Shannon avoided b and c in his theory, we have avoided them also. Both are very hard to measure, which makes them very hard to study. We feel it necessary to first build a theory of information value that can act as a companion for technical problem of information theory. That is to say, Shannon's account. This is why we took an axiomatic approach, and why we have tried to ape Shannon's axioms, as it made sense to do so.


\subsection*{Is this too complex?}
Perhaps turning a single objective into two, as we have, is too complex an answer. If this is true, then it would mean our strategy is not a parsimonious solution to the dilemma, and so we could or should reject it on that alone. 

Questions about parsimony are resolved by considering the benefits versus the costs. The benefits to curiosity-based search is that it leads to no regret solutions to exploration, to exploration-exploitation, and that it so far seems to do very well in practice, at reward collection. At the same time curiosity-as-exploration can also be building a model of the environment, useful for later planning \citep{Ahilan2019,Poucet1993}, creativity, imagination \citep{Schmidhuber2010}, while also building, in some cases, diverse action strategies \citep{Lehman2011a,Lehman2013,Mouret2015,Colas2020}. In other words, we argue it is a necessary complexity. 


\subsection*{Is this too simple?}
Solutions to the regular dilemma are complex and require sophisticated mathematical efforts and complex computations, when compared to our solution. Put another way, our solution may seem too simple to some. So have we ``cheated'' by changing the problem in the way that we have?

The truth is we might have cheated, in this sense: the dilemma might have to be as hard as it has seemed. But the general case for curiosity as useful in exploration is clear, and backed up by the brute fact of its widespread presence. The question is: is curiosity so useful and so robust that it can be sufficient for all exploration with learning. 

The answer to this question is empirical. If our account does well in describing and predicting animal behavior, that would be some evidence for it \citep{Sumner2019,Wang2019,Jaegle2019,Gottlieb2018,Kidd2015,Berlyne1950,Colas2020a,Rahnev2018,Wilson2020,CogliatiDezza2017b,Berger-Tal2014}. If it predicts neural structures \citep{Cisek2019,Kobayashi2019}, that would be some evidence for it (we discuss this more below). If this theory proves useful in machine learning and artificial intelligence research, that would be some evidence for it \citep{Burda2018,Schmidhuber1991,deAbril2018,Fister2019,Lehman2011a,Stanley2004a,Colas2020,Cully2015,Wilson2020,Pathak2019}. These are empirical predictions that require further investigation.


\subsection*{Is this a slight of hand, theoretically?}
Yes. It is often useful in mathematical studies to take one problem that cannot be solved, and replace it with another related problem that can be. This is what we have done for exploration and the dilemma. The regular view of the problem has no tractable solution, without regret. Ours has a solution, with no regret.

In this case, we swap one highly motivated animal behavoir (reward seeking) for another (information seeking).


\subsection*{Does value as a technical problem even make sense?}
Information value has been taken up as a problem in meaning  \citep{Kolchinsky2018}, or usefulness \citep{Tishby2000}. These are based on what we will call \textit{b-type} questions (a notion we defined in the section above). It is not that b questions are not important or are not valuable. It is that they are second order questions. The first order questions are the 'technical' or a-type questions (also defined above). 
Having a technical definition of value, free from any meaning, might seem counterintuitive for a value measure. We argue that it is not more or less counterintuitive than stripping information of meaning in the first  place. Indeed, this is how Shannon got his information theory. The central question for our account of value is, is it useful? It is enough to ensure a complete but perfectly efficient search for information/learning in any finite space. It is enough to play a critical part in solving/replacing the dilemma, which has for many years looked intractable. 


\subsection*{What about mutual information, or truth?}
In other prototypical examples, information value is based on how well what is learned relates to the environment \citep{Behrens2007,Kolchinsky2018,Tishby2000}, that is the mutual information the internal memory and the external environment. Colloquially, one might call this ``truth". The larger the mutual information between the environment and the memory, the more value it is said that we should then expect. This view seems to us at odds with actual learning behavior, especially in humans. 

As a people we pursue information which is fictional \citep{sternisko2020dark}, or based on analogy \citep{gentner1997reasoning}, or outright wrong \citep{loftus1989misinformation}. Conspiracy theories, disinformation, and our many more mundane errors, are far too commonplace for value to be based on fidelity alone. This does not mean that holding false beliefs cannot harm survival, but this is a second order question as far as learning goes. The fact that humans consistently seek to learn false things calls out us to accept that learning alone is a motivation for behavior.


\subsection*{So you suppose there is always positive value for learning of all fictions, disinformation, and deceptions?}
We must. This is our most unexpected prediction. Yet, logically, it is internally consistent with our work we believe qualitatively consistent with the behavior of humans and animals alike.


\subsection*{What about information foraging?}
Our work is heavily inspired by information foraging \citep{Inglis2001,Reddy2016}. Our motivation in developing our novel strategy was that information value should not depend on only a probabilistic reasoning, as it does in information foraging. This is for the simple reason that many of the problems animals need to learn are not probabilistic problems \textit{from their point of view}. Of course, most any learning problem can be described as probabilistic from an outsider's perspective; the one scientists' rely on. But that means probabilistic approaches are descriptive approximations. We also feel curiosity is so important it needed a theoretical account that is both mechanistic and descriptive at the same time. This is why we developed an axiomatic approach that ended up addressing the technical problem of information value.


\subsection*{Was it necessary to build a general theory for information value just to describe curiosity?}
No. It was an idealistic choice that worked out. The field of curiosity studies has shown that there are many kinds of curiosity. At the extreme limit of this diversity is a notion of curiosity defined for any kind of observation, and any kind of learning. If we were able to develop a theory of value driven search at this limit, we can be sure it will apply in all possible examples of curiosity that we seem sure to discover in future. At this limit we can also be sure that the ideas will span fields, addressing the problem as it appears in computer science, psychology, neuroscience, biology, and perhaps economics.


\subsection*{Doesn't your definition of regret ignore lost rewards when the curiosity policy is in control?}
Yes. Regret is calculated for the policy which is in control of behavior, not the policy which \textit{could} have been in control of behavior.


\subsection*{Is information a reward?}
It depends. If reward is defined as more-or-less any quantity that motivates behavior, then our definition of information value is a reward. This does not mean, however, that information value and environmental rewards are interchangeable. In particular, if you add reward value and information value to motivate search you can drive exploration in practical and useful ways. But this doesn't change the intractability of the dilemma proper \citep{Thrun1992a,Dayan1996,Findling2018,Gershman2018b}. So exploration will still generate substantial regret, as we show in Fig \ref{fig:summary}. But perhaps more important is that these kinds of intrinsic rewards \citep{Schmidhuber1991,Berger-Tal2014,Itti2009,Kobayashi2019} introduce a bias that will drive behavior away from what, could otherwise be, ideal skill learning \citep{Ng1999,Simsek2006}.


\subsection*{But isn't curiosity impractical?}
It does seem curiosity is just as likely to lead away from a needed solution as towards it. Especially so if one watches children who are the prototypical curious explorers \citep{Sumner2019,Kidd2015}. This is why we limit curiosity with boredom, and counter it with a competing drive for reward collecting (i.e., exploitation). 

We believe there is a useful analogy between science and engineering. Science can be considered an open-ended inquiry, and engineering a purpose driven enterprise. They each have their own pursuits but they also learn from each other, often in alternating iterations \citep{Gupta2006}. It is their different objectives though which make them such good long-term collaborators.


\subsection*{But what about curiosity on big problems?}
A significant drawback to curiosity, as an algorithm, is that it will struggle to deliver timely results when the problem is very large, or highly detailed. First, it is worth noting that most learning algorithms struggle with this setting \citep{MacKay2003,Sutton2018}, that is unless significant prior knowledge can be brought to bear \citep{Zhang2020,Sutton2018} or the problem can be itself compressed \citep{Ha2018a,Fister2019}. Because ``size'' is a widespread problem in learning, there are a number of possible solutions and because our definition of learning, memory and curiosity is so general, we can take advantage of most. This does not guarantee curiosity will work out for all problems; all learning algorithms have counterexamples \citep{Wolpert1997}. 


\subsection*{But what about other forms of curiosity?}
We did not intend to dismiss the niceties of curiosity that others have revealed. It is that these prior divisions parcel out the idea too soon. Philosophy and psychology consider whether curiosity is internal or external, perceptual versus epistemic, or specific versus diversive, or kinesthetic \citep{Kidd2015,Berlyne1950,Zhou2020}. Computer science tends to define it as needed, for the task at hand \citep{Stanley2004,Lehman2011a,Lehman2013,Mouret2015,Colas2020}. Before creating a taxonomy of curiosity it is important to first define it, mathematically. Something we have done here.


\subsection*{What is boredom, besides being a tunable parameter?}
A more complete version of the theory would let us derive a useful or even optimal value for boredom, if given a learning problem or environment. This would also answer the question of what boredom is computationally. We cannot do this. It is a next problem we will work on, and it is very important. The central challenge is deriving boredom for any kind of learning algorithm.


\subsection*{Does this mean you are hypothesizing that boredom is actively tuned?}
Yes we are predicting exactly that.


\subsection*{But do people subjectively feel they can tune their boredom?}
From our experience, no. Perhaps it happens unconsciously? For example we seem to alter physiological learning rates without subjectively experiencing it \citep{Behrens2007}.


\subsection*{How can an animal tune boredom in a new setting?}
The short answer is that one would need to guess and check. Our work in Fig \ref{fig:robust} suggests that this can be somewhat robust.


\subsection*{Do you have any evidence in animal behavior and neural circuits?}
There is some evidence for our theory of curiosity in psychology and neuroscience, however, in these fields curiosity and reinforcement learning have largely developed as separate disciplines \citep{Berlyne1950,Kidd2015,Sutton2018}. Indeed, we have highlighted how they are separate problems, with links to different basic needs: gathering resources to maintain physiological homeostasis \citep{Keramati2014,Juechems2019} and gathering information to decide what to learn and to plan for the future \citep{Valiant1984,Sutton2018}. Here we suggest that though they are separate problems, they are problems that can, in large part, solve one another. This insight is the central idea to our view of the explore-exploit decisions. 

Yet there are hints of this independent cooperation of curiosity and reinforcement learning out there. Cisek (2019) has traced the evolution of perception, cognition, and action circuits from the Metazoan to the modern age \citep{Cisek2019}. The circuits for reward exploitation and observation-driven exploration appear to have evolved separately, and act competitively, exactly the model we suggest. In particular he notes that exploration circuits in early animals were closely tied to the primary sense organs (i.e. information) and had no input from the homeostatic circuits \citep{Keramati2014,Cisek2019,Juechems2019}. This neural separation for independent circuits has been observed in ``modern'' high animals, including zebrafish \citep{Marques2019} and monkeys \citep{White2019,Wang2019}. 


\subsection*{What about aversive values?}
We certainly do not believe this is complete theory of rewardind decisions. For example, we do not account for any consequences of learning, or any other aversive events. Accounting for these is another important next step.

The prescence of mnay values fits well within our notion of competition between simple (pure) policies, in a win-stay loose-shift game \cite{Estes1994TowardAS}. To take other values into account we need a different notation, from the inequalities of Eq.~\ref{eq:pipi}. For example, if we added a fear value $F$ to our homeostatic scheme we can move to argmax notation,

\begin{equation}
\label{eq:pipi_f} 
\Pi^{\pi} = \ \Big [ \pi_E,\ \pi_R,\ \pi_F \Big ]
\end{equation}

\begin{equation}
\label{eq:meta_greedy_f} 
	\argmax_{\Pi^{\pi}} \ \Big [ E - \eta,\ \hat R - \rho,\ F + \phi \Big ]
\end{equation}

Adding bias term to each value provides a direct way to break ties, and set default behavoirs. Additional to our standarrd assumptions, if we also assume the fear/aversive term should take precedence over all other option when there is a tie, then $\phi > \rho > \eta$.

\subsection*{What about other values?}
Another more mundane example would be to add a policy for a grooming drive, $C$. 

\begin{equation}
\label{eq:pipi_c} 
\Pi^{\pi} = \ \Big [ \pi_E,\ \pi_R,\ \pi_C \Big ]
\end{equation}

\begin{equation}
\label{eq:meta_greedy_c} 
\argmax_{\Pi^{\pi}} \ \Big [ E - \eta,\ \hat R - \rho,\ C + \chi \Big ]
\end{equation}

Where we assume the grooming term should take precedence over all other options, when there is a tie. That is, $\chi > \rho > \eta$.

\subsection*{What about fitting deterministic models?}
Exploration in the lab is often described as probabilistic \citep{Calhoun2014,Song2019a,Gershman2018b,Schulz2018a}. By definition probabilistic models do not make exact predictions of behavior, only statistical ones. Our approach is deterministic and so can make exact predictions. If, that is, we can fit it and it turns out to be correct. In species ranging from human to \textit{Caenorhabditis elegans}, there are hundreds of exploration-exploitation experiments. A deterministic theory can, in principle, open up entirely new avenues for reanalysis. Testing and exploring these is a critical next step.


\subsection*{Does regret matter?}
Another way around the dilemma is to ignore regret altogether. This is the domain of pure exploration methods, like the Gitten’s Index \citep{Gittins1979}, or probably-approximately correct approaches \citep{Valiant1984}, or other bounded forms of reinforcement learning \citep{Brafman2002}. Such methods can guarantee that you find the best reward value actions, eventually. These methods do not consider regret, as we have. But we argue that regret is critical to consider when resources and time are scarce, as they are in most natural settings. 


\subsection*{Is the algorithmic run time practical?}
It is common in computer science to study the run time of an algorithm as a measure of its efficiency. So what is the run time of our win stay, loose switch solution? There is unfortunately no fixed answer for the average or best case. The worst case algorithmic run time is linear and additive in the independent policies. If it takes $T_E$ steps for $\pi_E$ to converge, and $T_R$ steps for $\pi_R$, then the worst case run time for $\pi_{\pi}$ is $T_E + T_R$. Of course this is the worst case run time and would still be within reasonable ranges for learning in most naturalistic contexts.


\subsection*{Summary}
There will certainly be cases in an animal's life when their exploration must focus on tangible physical rewards. Even here, searching for information about physical rewards, rather than searching for their value, will make the search more robust to deception, and more reliable in the future. We think of this as curiosity about reward. There are many more cases though where curiosity can include observations of the environment and the self. We have argued one should put aside intuitions that suggest an open-ended curious search is too inefficient to be practical. We have shown it can be made very practical. We have argued one should put aside traditional intuitions for what exploratory behavior in animals represents. We have shown how there is a zero-regret way to solve the dilemma, a problem that has seemed unsolvable. Simply be curious.



