\section*{Mathematical Appendix.}
\newcommand{\beginsupplement}{%
        \setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}%
        \setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}%
     }
\beginsupplement
\setcounter{theorem}{0}

% TODO - use F, find a way to state E?  It is time to face the Bellmans in full
% TODO - us E(s) valid? What to write there.
% TODO - step by step notation check. On paper?

\subsection*{Information value as a dynamic programming problem} To find a dynamic programming solution based on the Bellman equation (Eq~\ref{eq:}) we must prove our memory $\Theta$ has optimal substructure. This is because the normal route, which assumes the problem rests in a Markov Space, is closed to us. By optimal substructure we mean that the process of learning in $\Theta$ can be partitioned into a collection, or series of memories, each of which is itself a solution. That is, it lets us find a kind of recursive structure in memory learning, which is what we need to apply the Bellman. Proving optimal substructure also allows for simple proof by induction \cite{Roughgarden2019}, a property we will take advantage of. 

\begin{theorem}[Optimal substructure] \label{theorem:opt_sub} 
    Let $X$, $A$, $\Theta$, $f_{\Theta}$, (Def. 1), $\pi_E$ and $\delta$ be given. Assuming transition function $\delta$ is deterministic, if $V^*_{\pi_E}$ is the optimal information value given by policy $\pi_E$, a memory $\Theta_{t+1}$ has optimal substructure if the the last observation $x_t$ can be removed from $\Theta_t$, by $\Theta_{t} = f^{-1}(\Theta_{t+1}, x_t)$ such that the resulting value $V^*_{t-1} = V^*_{t} - E_{t}$ is also optimal. 
\end{theorem}
\begin{proof}
	Given a known optimal value $V^*$ given by $\pi_E$ we assume for the sake of contradiction there also exists an alternative policy $\hat \pi_E \neq \pi_E$ that gives a memory $\hat M_{t-1} \neq M_{t-1}$ and for which $\hat V^*_{t-1} > V^*_{t-1}$. 
	
	To recover the known optimal memory $M_t$ we lift $\hat M_{t-1}$ to $M_t = f(\hat M_{t-1}, x_t)$. This implies $\hat V^* > V^*$ which in turn contradicts the purported original optimality of $V^*$ and therefore $\hat \pi_E$.
\end{proof}

This proof required two things. First, it was required we extend the idea of memory. We need to introduce a mechanism for forgetting of a very particular kind. We must assume that any last learning step $f(x, \theta) \rightarrow \theta'$ can be undone by a new vector valued function $f^{-1}$, such that $f(x, \theta') \rightarrow \theta$. In other words we must assume what was last remembered, can always be forgotten. 

Second we must also assume the environment $\delta$ is deterministic. Determinism is consistent with the natural world, which does evolve in a deterministic way, at the scales we concerned with. This assumption is however at odds with much of reinforcement learning theory \cite{needed} and past experimental work \cite{needed}. Both tend to study stochastic environments. We'll address this discrepancy later on using numerical simulations.

\subsection*{Bellman solution} Knowing the optimal substructure of $\Theta$ and given an arbitrary starting value $E_0$, the Bellman solution to curiosity optimization of $E$ is given by,

\begin{equation}
	\label{eq:V_E} 
	V_E(x) = E_0 + \argmax_a \ \Big [ E_t \Big ] \\
\end{equation}

To explain this derivation we'll begin simply with the definition of a value function and work from there. A value function $V(x)$ is defined as the best possible value of the objective for $x$. Finding a Bellman solution is discovering what actions $a$ to take to arrive at $V(x)$. Bellman's insight was to break the problem down into a series of smaller problems, leading a recursive solution. Problems that can be broken down this way have optimal substructure. The value function for a finite time horizon $T$ and some payout function $F$ is given by Eq~\ref{eq:V_F1}. It's Bellman form is given by Eq.~\ref{eq:V_F2}

\begin{equation}
	\label{eq:V_F1}
	V(x) = \argmax_a \Big [ \sum_T F(x, a) \Big ]
\end{equation}

\begin{equation}
	\label{eq:V_F2}
	V(x) = \argmax_a \Big [ F(x_0, a_0) + V(x_1) \Big ]
\end{equation}

In reinforcement learning the value function is based on the sum of future rewards giving Eq.~\ref{eq:V_R1}-\ref{eq:V_R2},

\begin{equation}
	\label{eq:V_R1}
	V_R(x) = \argmax_a \Big [ \sum_T R_t \Big ]
\end{equation}

\begin{equation}
	\label{eq:V_R2}
	V_R(x) = \argmax_a \Big [ R_0 + V(x_1) \Big ] \\
\end{equation}

% TODO clean this up
In our objective the best possible value is not found by temporal summation as it is during reinforcement learning. $E$ is necessarily a temporally unstable function. We expect it to vary substantially before contracting to approach 0. It's best possible value is well approximated then by only looking at the maximum value for the last time step, making our optimization myopic, that is based on only last values encountered \cite{Hocker2019}.

\begin{equation}
	\label{eq:V_E1} 
	V_E(x) = \argmax_a \ \Big [ E_t \Big ]
\end{equation}

\begin{equation}
	\label{eq:V_E2} 
	\begin{split}
	V_E(x) &= \argmax_a \Big [ E_0 + V(x_1) \Big ] \\
		   &= E_0 + \argmax_a \ \Big [ E_t \Big ] \\
	\end{split}
\end{equation}

\subsection*{Good exploration by curiosity optimization} 

Recall from the main text we consider that a good exploration should,

\begin{enumerate}
  \item Visit all available states of the environment at least once. 
  \item Should cease only once learning about the environment has plateaued. 
  \item Should take as few steps as possible to achieve criterion 1 and 2.
\end{enumerate}

Limiting $\pi_E$ to a deterministic policy makes proving these three properties amounts to solving sorting problems on $E$. If a state is visited by our algorithm it must have the highest value. So if every state must be visited under a deterministic greedy policy every state must, at one time or another, generate maximum value. This certain if we know that all values will begin contracting towards zero. \textit{Violations of this assumption are catastrophic}. We discuss this limit in the Discussion but note that things are not as dire as they may seem.

\textbf{Definitions.} Let $Z$ be the set of all visited states, where $Z_0$ is the empty set $\{\}$ and $Z$ is built iteratively over a path $P$, such that $Z \rightarrow \{x | x \in X\ \text{and}\ x \not\in Z\}$. 

To formalize the idea of ranking we take an algebraic approach. Give any three real numbers $(a,b,c)$,

\begin{align}\label{eq:ineq} 
	a \leq b \Leftrightarrow \exists \ c;\ b = a + c \\
	a > b \Leftrightarrow (a \neq b) \wedge (b \leq a) 
\end{align}

\begin{theorem}[State search: breadth] \label{theorem:Z} 
	Given some arbitrary value $E_0$, an exploration policy governed by $\pi^*_E$ will visit all states $x \in X$ in finite number of steps $T$.
\end{theorem}
\begin{proof}
	Let $\mathbf{E} = (E_1, E_2, ...)$ be ranked series of $E$ values for all states $X$, such that $(E_1 \geq E_2, \geq ...)$. To swap any pair of values ($E_i \geq E_j$) so ($E_i \leq E_j$) by Eq.~\ref{eq:ineq} $E_i - c = E_j$. 
	
	Therefore, again by Eq.~\ref{eq:ineq}, $\exists \int \delta E(s) \rightarrow -c$. 
	
	\textit{Recall}: $\nabla^2 f_{\Theta}(x) < 0$ after a finite time $T$.
	
	However if we wished to instead swap ($E_i \leq E_j$) so ($E_i \geq E_j$) by definition $\not \exists c; E_i + c = E_j$, as $\not \exists \int \delta \rightarrow c$. 
	
	To complete the proof, assume that some policy $\hat \pi_E \neq \pi^*_E$. By definition policy $\hat \pi_E$ can be any action but the maximum, leaving $k-1$ options. Eventually as $t \rightarrow T$ the only possible swap is between the max option and the $kth$, but as we have already proven this is impossible as long as Axiom 2 holds. Therefore, the policy $\hat \pi_E$ will leave at least 1 option unexplored and $S \neq Z$. 
\end{proof}
\begin{theorem}[State search: depth] \label{theorem:convergence} 
	An exploration policy governed by $\pi^*_E$ will only revisit states for where learning is possible.
\end{theorem}
\begin{proof}
	\textit{Recall}: Axiom 2. Each time $\pi^*_E$ visits a state $s$, so $\Theta \rightarrow \Theta'$, $F(\Theta', a_{t+dt}) < F(\Theta, a_t)$
	
	In Theorem~\ref{theorem:Z} we proved only a deterministic greedy policy will visit each state in $X$ in $T$ trials.
	
	By induction, if $\pi^*E$ will visit all $x \in X$ in $T$ trials, it will revisit them at most $2T$, therefore as $T \rightarrow \infty$, $E \rightarrow \eta$. 
\end{proof}

We assume in the above the action policy $\pi_E$ can visit all possible states in $X$. If for some reason $\pi_E$ can only visit a subset of $X$ then proofs above apply only to that subset. 

These proofs come with some fine print. $E_0$ can be any positive and finite real number, $E_0 > 0$. Different choices for $E_0$ will not change the proofs, espcially their convergence. So in that sense one can choosen it in an arbitrary way. Different choices for $E_0$ can however change individual choices, and their order. This can be quite important in practice, espcially when trying to describe some real data.  This choice will not change the algorithm's long-term behavior \textit{but} different choices for $E_0$ will strongly change the algorithm's short-term behavior. This can be quite important in practice. 

\subsection*{Optimality of $\pi_{\pi}$} \label{sec:opt_pipi} 
Recall that in the main text we introduce the equation below as a candidate with zero regret solution to the exploration-exploitation dilemma.

\begin{equation}
\label{eq:pipi} 
\pi^{\pi} = \ \Big [ \pi_E,\ \pi_R \Big ]
\end{equation}

\begin{equation}
\label{eq:meta_greedy} 
	\argmax_{\pi^{\pi}} \ \Big [ E_{t-1},\ R_{t-1} \Big ]_{\textbf{(2, 1)}}
\end{equation}

In the following section we prove two things about the optimality of $\pi_\pi$. First, if $\pi_R$ had any optimal asymptotic property for value learning before their inclusion into our scheduler, they retain that optimal property under $\pi_\pi$ when $\eta = 0$, or is otherwise sufficiently small. Second, show that if both $\pi_R$ and $\pi_E$ are greedy, and $\pi_\pi$ is greedy in its definition, then Eq~\ref{eq:pipi} is certain to maximize total value. The total value of $R$ and $E$ is the exact quantity to maximize if information seeking and reward seeking are equally important, overall. This is, as the reader may recall, one of our key assumptions. Proving this optimality is analogous to the classic activity selection problem from the job scheduling literature \cite{BellmanBook,Roughgarden2019}.

\begin{theorem}[$\pi_{\pi}$ is unbiased] \label{theorem:meta} 
	 Let any $S$, $A$, $\Theta$, $\pi_R$, $\pi_E$, and $\delta$ be given. Assuming an infinite time horizon, if $\pi_E$ is optimal and $\pi_R$ is optimal, then $\pi_{\pi}$ is also optimal in the same sense as $\pi_E$ and $\pi_R$. 
\end{theorem}
\begin{proof}
	The optimality of $\pi_{\pi}$ can be seen by direct inspection. If $p(R = 0) > 0$ we are given an infinite horizon, then $\pi_E$ will have a unbounded number of trials meaning the optimally of $P^*$ holds. Likewise, $\sum E < \eta$ as $T \rightarrow \infty$, ensuring $pi_R$ will dominate $\pi_{\pi}$ therefore $\pi_R$ will asymptotically converge to optimal behavior. 
\end{proof}

In proving this optimality of $\pi_{\pi}$ we limit the probability of a positive reward to less than one, denoted by $p(R_t = 1) < 1$. Without this constraint the reward policy $\pi_R$ would always dominate $\pi_{\pi}$ when rewards are certain. While this might be useful in some circumstances, from the point of view $\pi_E$ it is extremely suboptimal. The model would never explore. Limiting $p(R_t = 1) < 1$ is a reasonable constraint, as rewards in the real world are rarely certain. A more naturalistic way to handle this edge case is to introduce reward satiety, or a model physiological homeostasis \cite{Keramati2014,Juechems2019}.

\subsubsection*{Optimal scheduling for dual value learning problems}
In classic scheduling problems the value of any job is known ahead of time \cite{Bellmann1954,Roughgarden2019}. In our setting, this is not true. Reward value is generated by the environment, \textit{after} taking an action. In a similar vein, information value can only be calculated \textit{after} observing a new state. Yet Eq.~\ref{eq:pipi} must make decisions \textit{before} taking an action. If we had a perfect model of the environment, then we could predict these future values accurately with model-based control. In the general case though we don't know what environment to expect, let alone having a perfect model of it. As a result, we make a worst-case assumption: the environment can arbitrarily change--bifurcate--at any time. This is a highly nonlinear dynamical system \cite{Strogatz1994}. In such systems, myopic control--using only the most recent value to predict the next value-- is known to be an robust and efficient form of control \cite{Hocker2019}. We therefore assume that last value is the best predictor of the next value, and use this assumption along with Theorem~\ref{theorem:meta} to complete a trivial proof that Eq.~\ref{eq:pipi} maximizes total value.

\subsubsection*{Optimal total value}
If we prove $\pi_{\pi}$ has optimal substructure, then using the same replacement argument \cite{Roughgarden2019} as in Theorem~\ref{theorem:meta}, a greedy policy for $\pi_\pi$ will maximize total value.

\begin{theorem}[Total value maximization of $\pi_{\pi}$] \label{theorem:meta_total} 
    \label{theorem:meta} 
	 Let any $S$, $A$, $\Theta$, $\pi_R$, and $\delta$ be given. If $\pi_R$ is defined on a Markov Decisions, then $\pi_\pi$ is Bellman optimal and will maximize total value. 
\end{theorem}
\begin{proof}
    We assume Reinforcement learning algorithms are embedded in Markov Decisions space, which by definition has the same decomposition properties as that found in optimal substructure.
    
    \textit{Recall}: The memory $\Theta$ has optimal substructure (Theorem~\ref{theorem:opt_sub}.
    
	\textit{Recall}: The asymptotic behavior of $\pi_R$ and $\pi_E$ are independent under $\pi_\pi$ (Theorem~\ref{theorem:meta}
	
	\textit{Recall}: The controller $\pi_\pi$ is deterministic and greedy.
	
	If both $\pi_R$ and $\pi_E$ have optimal substructure and are independent, then $\pi_\pi$ must also have optimal substructure. If $\pi_\pi$ has optimal substructure, and is greedy-deterministic then it is Bellman optimal.
\end{proof}
