\documentclass[9pt,twocolumn,twoside]{pnas-new}
% Use the lineno option to display guide line numbers if required.
% Note that the use of elements such as single-column equations
% may affect the guide line number alignment. 

\templatetype{pnasresearcharticle} % Choose template 
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} = Template for a one-column mathematics article
% {pnasinvited} = Template for a PNAS invited submission

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multicol}


\DeclareMathOperator*{\argmax}{argmax}
\newtheorem{corollary}{Corollary}
\newtheorem{axiom}{Axiom}
\newtheorem{statement}{Statement}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}[theorem]{Lemma}

\title{A way around the exploration-exploitation dilemma.}

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[a,1]{Erik J Peterson}
\author[a,b]{Timothy D Verstynen}
\affil[a]{Department of Psychology}
\affil[b]{Center for the Neural Basis of Cognition, Carnegie Mellon University, Pittsburgh PA}

% Please give the surname of the lead author for the running footer
\leadauthor{Peterson} 

% Please add here a significance statement to explain the relevance of your work
\significancestatement{We show a way around the exploration-exploitation dilemma. In our approach exploration is done only to maximize information value--a quantity we define axiomatically. Maximizing information value forces the animal to learn a general, reward-independent, memory of the world. An important ``side effect'' of this general learning is that reward learning improves--to optimality. We call this view of reinforcement learning dual value learning and show that dual value learning allows for a simple greedy deterministic solution to the exploration-exploitation dilemma.}

% Please include corresponding author, author contribution and author declaration information
% \authorcontributions{EJP?.}
\authordeclaration{The authors have no conflicts of interest to declare.}
% \equalauthors{\textsuperscript{1}A.O.(Author One) and A.T. (Author Two) contributed equally to this work (remove if not applicable).}
\correspondingauthor{\textsuperscript{1}To whom correspondence should be addressed. E-mail: Erik.Exists@gmail.com}

% Keywords are not mandatory, but authors are strongly encouraged to provide them. If provided, please include two to five keywords, separated by the pipe symbol, e.g:
% \keywords{Keyword 1 $|$ Keyword 2 $|$ Keyword 3 $|$ ...} 

\begin{abstract}
    The exploration-exploitation dilemma is considered a fundamental but intractable problem in the learning and decision sciences. Here we show a way around it. We define separate mathematical objectives for exploration and exploitation. To make the objective for exploration independent of reward, we derive a set of general axioms for information value. These let us develop a greedy algorithm which provably and optimally maximizes information value and reward. The cost to this solution is an increase in worst-case sample efficiency when compared to the equivalent reinforcement learning problem. The severity of the cost depends on the size of the environment, and the rate of rewards.
\end{abstract}

\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}
If a mouse is placed in a familiar maze it can face a dilemma. Should it exploit the arm it knows to expect a reward in, or should it explore another unknown arm of the maze to find a more rewarding choice. Given the unknown nature of choice, its hard to form a rational strategy. 

The exploration-exploitation dilemma is not limited to mice in mazes, but is thought to be a hard and fundamental problem in decision science. This dilemma arises when deciding where to eat to dinner, which job to take, and when to marry. It seems that mice and humans share some intrinsic desire to learn about the unknown, and often go exploring. Given this behavior, the theoretical problem for decision science is explaining how and when exploration decisions are made. The most common answer is found in reinforcement learning \cite{Sutton2018a}. 

Reinforcement learning is learning how to map situations (what we'll call states) to actions, with the goal of maximizing reward. 

Theoretical study of the dilemma problem using reinforcement learning has proven it is not just hard problem, but is formally intractable. That is, it's not possible to always find single mathematically optimal solution. It is however possible to approximate an answer--on average. The approximate solutions are typically complex, are require strong but difficult to verify assumptions about animal cognition. 

Formalizing exploration-exploitation is not just essential to understanding natural behavior. Theoretical study of the dilemma is also central to progress in machine learning, artificial intelligence, and casual reasoning. While the dilemma as a reinforcement problem is intractable, there might exist related problems which are tractable. This report offers a viable alternative.

Here we challenge a basic tenant of both the dilemma and of reinforcement learning: exploration is a search for (more) reward. We conjecture that exploration should \textit{only} try to maximize information.

Our challenge rests on a simpleÂ behavioral observation. If you place a naive mouse in maze for the very first time, it will explore the maze even though it can have no expectation of reward. Animals seem to be intrinsically motivated by information. Of course, we're not the first to note this.

Curiosity and exploration have a long history \cite{}. Our contribution is in recognizing:

\textit{Information value should never depend on reward value.}

Why would an animal explore for \textit{only} information if it is trying to maximize reward, to do reinforcement learning? An answer can be found in the environment most animals live in. Most rewards in the natural world repeat themselves in the short-term \cite{}. Different seasons bring different hardships and opportunities, on a daily basis competitors come and go, as do food sources, mates and shelter. Because of this variability it is mistake to motivate exploration, and learn good exploration policies, based on specific rewards, and on traditional reinforcement learning. That is, reinforcement policy must change as the reward distribution does, while a good exploration policy can and should be robust. 

We suppose that information seeking builds knowledge for both the long-term and short-term, while reward seeking often only satisfies short-term needs. 

% TODO: 
Figure 1 shows theoretical frameworks for reinforcement learning (Fig.~\ref{}) and information theory (Fig.~\ref{}). During reinforcement learning an agent observes states $s_t$ from a finite environment $S^N$, using an action policy function $\pi$ to map $s_t$ to an action $a_t$ which is drawn from a finite of number of possibilities $A^K$. Each action on the environment returns a reward $R_t$ and a new state $s_{t+1}$, by its transition function $s_{t+1} = \delta(a_t)$.  

During information transmission a symbol $x$ from a finite alphabet $X$ is transmitted on a noisy channel $C$ to receiver $R$. If we interpret states $S$ a symbols in $X$, we can model state observation in information theory combining both approaches into a single framework and allowing information to effect reward learning. In some forms the information theoretic properties are of the channel are used, in others the channel is said to have a memory $M$. Often, the $M$ is a probability distribution that learned via Bayes rule (Eq.\ref{}).  

% TODO: additive form.
% TODO: dual form

We conjecture the dilemma is really two separate problems. One problem is exploration, and the other is exploitation (Eq.~\ref{} and Eq.~\ref{}).

It's possible to solution to our dual value optimization problem using a Bayesian approach. Doing this creates an equivilance between our axiomatic notion of value and prior work on information gain / Bayesian surprise. We'll show this with some examples in Fig.~\ref{}. Taking a Bayesian approach implies the validity of any solution to the dilemma we find depends on a strict Bayesian assumption. This is could prove a burdensome requirement if it turns out that animals are not actually pure Bayesian reasoning systems. 

Instead, we borrow essential properties from prior work on Bayesian learn and information theoretic analysis in order to generalize them. As a result of our borrowing the value axioms we develop below produce a value measure identical to previous work when it is in fact possible to safely assume Bayesian learning. However, the axioms are general enough to allow for a very large range of alternative definitions of memory. 

Our value axioms require only three weak assumptions: \textit{1.} An animal has a memory $M$ which is large enough to remember its environment. \textit{2.} What can be remembered can be in $M$ can also be forgotten. \textit{3.} There is a way to measure how memory changes with experience. 

\begin{definition}
    \label{def:memory}
    Let a memory $M$ be finite set whose maximum size is $N$--defined over a finite state space $s \in S^N$--whose elements are added by an invertible encoder $f$, such that $M_{t+1} = f(M_{t}, s)$ and $M_{t} = f^{-1}(M_{t+1}, s)$ and whose elements $z$ from $M$ are recovered by a decoder $g$, such that $z = g(M, s)$. The initial memory $M_{0}$ is said to be the empty set, $M_{0} = \emptyset$.
\end{definition}

\begin{center}{\textit{The Value Axioms}.}\end{center}
\begin{axiom} 
    Information value $E$ is the distance between on the two most recent memories, $M_{t+1}$ and $M_{t}$.
    \label{ax:1}
\end{axiom} 

\begin{axiom}
    $E \geq 0$. New information about the world is always \textit{in principle} valuable.
    \label{ax:2}
\end{axiom}

\begin{axiom}
    $E = 0$ if and only if $M_{t+1} = M_{t}$. If you remember something perfectly there is no value in learning it again. 
    \label{ax:3}
\end{axiom}

\noindent
The contents of memory are potentially arbitrary. As a result there is no way to ahead of time say what comparison to make when valuing information. The simplest and most robust solution is to simply value each state against the memory as a whole (Axiom~\ref{ax:4}. 
\begin{axiom}
    $E$ is monotonic with the average change in $M$.
    \label{ax:4}
\end{axiom}

\noindent
In general, specific information is more valuable than less specific information. As surrogate for specificity, we formalize this with the geometric notion of compactness.
\begin{axiom}
    $E$ is monotonic with the compactness $C$ of the change $M$ (Eq.~\ref{eq:compactcude}).
    \label{ax:5}
\end{axiom}
\noindent

% TODO Ax 6: fisher eq?
Figure~\ref{} shows how our definition of value fits into reinforcement learning and information theory. 

Our definition of memory is extremely general. For example, if $f$ and $g$ are identity functions (functions that return what they are given), then $M$ becomes a simple episodic memory, an ongoing record of which states have been visited. If you can add a state to $M$ you can take it out which meets the requirement for invertibility / forgettability (Def~\ref{def:ineq}). One way to measure the distance between two memories of this kind is to simply compare their size (i.e., cardinality). If instead $f$ is temporal encoder, which adds both the state $s$ and the time $t$ to $M$, then $M$ functions like a temporal episodic memory.  

Both kinds of episodic memory are really just data stores to which we can apply a range of $g$ decoders, including dimensionality reduction techniques (\textit{e.g.} PCA, NMF, tSNE), clustering methods (k-NN), and mixture models (GMM, kernel), as well as modern methods for learning in artificial networks (GANs, VAEs). By working a the real number space, each of these is associated with one or more was to measure distance between memories (e.g., the euclidian distance). With a way to measure distance, it is should be universally possible to satisfy the value axioms.

As a second example we can let $f$ and $g$ be probability functions which means $M$ becomes a probability distribution. If you can increases the probability estimate of a state you can equally decrease it, satisfying invertibility. Likewise, there are a number of ways to measure the distance between two distributions (e.g., the KL divergence).

The basic properties guaranteed by the axioms and memory definition are sufficient to derive an optimal action policy to maximize total $E$. 

The Bellman equation provides the optimal policy to maximize $E$ over $S$ for any memory $M$. This is proved in Theorems~\ref{theorem:opt_sub}. Critically, the proof depends only on the idea that $M$ is invertible--that items in $M$ can be forgotten. 

% TODO: Bellman

Having a policy that maximizes $E$ does not ensure the memory that is learned is accurate (has high mutual information between its contents and the world, Fig~\ref{}). The encoding and decoding functions are responsible for accuracy. We leave open the possibility that either function is noisy, forgetful or otherwise imperfect. We only intend that information value is purely subjective measure, concerned only with self-consistency not accuracy. As scientists we'd hope maximizing value would also lead to accuracy. As experimentalists its clear this is not always the case. An animal, especially perhaps a human animal, might learn an very inaccurate model of the world, but as long as it learns it consistently total $E$ can be reach its optimal value.

Having a policy that maximizes $E$ does not necessarily ensure exploration is of good quality in terms of also maximizing reward. Relying on Axiom 6, in Theorems~\ref{theorem:Z} and~\ref{theorem:convergence} we prove our optimal policy for information value, $\pi^*_E$, leads to a complete and exhaustive exploration of any \textit{finite} space $S$. That is,

\begin{enumerate}[noitemsep,wide=0pt,leftmargin=\dimexpr\labelwidth+2\labelsep\relax]
    \item The optimal policy $\pi^*_E$ must visit each state in $s \in S$ at least once (Theorem~\ref{theorem:Z}).
    \item The optimal policy $\pi^*_E$ must revisit each $s \in S$ until learning about each state $s$ plateaus when $g(M_{t+1}, s) = g(M_{t},s)$. (Theorem~\ref{theorem:convergence}).
\end{enumerate}

There exists a number of reinforcement learning algorithms. We shown one example in Eq~\ref{}. 

Equation~\ref{} gives us a strategy to optimize both while ensuring their individual properties and optimality hold. 

In Equation~\ref{} we regard animal behavior as fixed resource--where only one action can be taken at the time--we can imagine the greedy policies in Eq~\ref{} and Eq~\ref{} are simply two possible ``jobs'' competing for ``execution'' (behavioral control). This view turns the dilemma into a problem of optimal scheduling. 

% TODO: meta-greedy

The solution to scheduling problems with 1. non-negative value and 2. identical run times is known to be a greedy algorithm (Theorem~\ref{}). (As each policy only results in a single action $a \sim A$ it's reasonable to say the time it takes to make an action (i.e. the run time) is a constant value).

The scheduling algorithm in Eq~\ref{} will maximize the joint value of $R$ and $E$, but does not ensure each maintains its properties. The constraints we introduce (Eq~\ref{})) ensure $E$ will plateau, and that when it does the reward policy takes control and always chooses the highest value action. 

In stochastic environments $M$ may show small continual fluctuations which get reflected in $E$. To allow Eq~\ref{pipi} to achieve a stable solution we introduce $\epsilon$, which acts as boredom threshold for exploration. To be consistent with the value axioms, $\epsilon > 0$ and $E + \epsilon \geq 0$. 

To ensure the default policy is greedy reward maximization, Eq~\ref{} breaks ties between $R_t$ and $E_t$ in favor of the reward policy.

% In contrast to our approach, a common way to approximate a solution to the dilemma is shown in Eq~\ref{}. In this form the reward $R$ is supplemented by some fictive reward, or information term, $I$. $I$ is typically rescaled by the parameter $\beta$ (if $\beta = 1$ there is no scaling). The true aim here it maximize $R$ but to encourage exploration we add $I$. The term $I$ can be a information theoretic term (e.g., entropy, mutual information, or the KL divergence), a novelty bonus, or a curiosity/error term. Regardless of choice, all these realizations share a common limitation--finding the optimal reward is not certain. Some $I$ will succeed in some cases and $\beta$ choices while others won't. 

% Given a history of action choices, the most valuable option is always chosen in our dual value approach to the dilemma. The cost of this way of doing things is time. It's now a problem of optimal scheduling. In the best schedule w.r.t. value time is taken up by actions to maximize information value that do no \textit{necessarily} provide information about reward. This is especially true if there are actions that are never rewarding but that are complex enough--high entropy enough--to require several observations to learn. 

Trying to maximize reward creates a dilemma between exploration and exploitation. Trying to maximize information avoids the dilemma, but does at a cost to worst case sample efficiency. 

The worst case sample efficiency for $\pi_{\pi}$ is additive in its policies. That is, if in isolation it takes $T_E$ steps to earn $E_{T} = \sum_{T_E} E$, and $T_R$ steps to earn $r_{T} = \sum_{T_R} R$, then the worst case training time for $\pi_{\pi}$ is $T_E + T_R$, if that is neither policy can learn from the other's control. There is however no reason each policy can't observe the transitions $(s_t, a_t, R, s_{t+1})$ caused by the other. If this is allowed,  worst case training time improves to $\max(T_E, T_R)$. 


The Kullback--Leibler ($KL$) divergence satisfies all five value axioms (Eq.~\ref{eq:KL}). In expressing $E$ in terms of KL it also allows us to more concretely demonstrate the mathematical properties implied in our axioms (See Methods).

\begin{equation}
    KL(M', M) = \sum_{s \in S} M'(s) \text{log} \frac{M'(s)}{M(s)} 
    \label{eq:KL}
\end{equation}


% TODO Bandit results


\bibliography{library}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% --------------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\newpage
\section*{Mathmatical Appendix.}
\subsection*{Compactness}
The compactness $C$ of a hyper-cube has a simple formula, $C = \frac{P^2}{A}$ where $P$ is the cube's perimeter and $A$ is its area. Therefore as $M_{t}$ moves to $M_{t+1}$, the we can measure the distances $\{d_i, d_{i+1}, d_{i+2},\ldots d_{N}\}$ for all $O \leq N$ observed states $Z$, such that $Z \subseteq S$ and treat them as if they formed a $O$-dimensional hyper-cube. In measuring this imagined cube we arrive at a geometric estimate for change the compactness of our memory $M$ (Eq.~\ref{eq:compactcude}).

\begin{equation} \label{eq:compactcude}
\begin{split}
    C & = \frac{\Big ( 2 \sum_{O}^{i} d_i \Big )^2}{\prod_{O}^{i} d_i}
\end{split}
\end{equation}

\subsection*{Exploration as a dynamic programming problem}
To use theorems from dynamic programming \cite{Bellman,Sutton2018a} we must prove our memory $M$ (Def.~\ref{def:memory}) has optimal substructure. By optimal substructure we mean that $M$ can be partitioned into to a small number collection or series, each of which is an optimal dynamic programming solution. In general by proving we can decompose some optimization problem in a set of smaller problems whose optimal solution is easy to find or prove, it trivial to prove we can also grow the series optimally which in turns allows for direct proofs by induction.

Here we prove the optimal substructure of $M$ over a series of state observations $P = (s_t, s_{t+1}, s_{t+2}, ... s_{t+T-1})$. For consistency with standard notation for dynamic programming we redefine $E$ as the payoff function, $F(M_{t}, a_t)$ (Eq.~\ref{eq:payout}). 

\begin{equation}
    \begin{split} \label{eq:payout}
    F(M_{t}, a_t) = E(M_{t+1}, M_{t})\\
    \text{subject to the constraints} \\
    a_{t} = \pi(s_t) \\
    s_{t+1} = \delta(s_{t}, a_t),\\ 
    M_{t+1} = f(M_{t}, s_{t})
    \end{split} 
\end{equation}

Using Eq~\ref{eq:payout} we can write down the total information value $V_{\pi_E}$, as the sum of all payouts for some policy $\pi_E$, made over some observation period $T$, where time $t$ is an index $t = (1,2,3,\ldots,T)$. 

\begin{equation} \label{eq:V}
    \begin{split}
        V_{\pi_E} = \sum_{t \in T} F(M_{t}, a_t)\\
    \end{split}
\end{equation}

Our overall goal is to derive a algorithm to maximize $V_{\pi_E}$. Our specific goal is to derive a iterative greedy algorithm to maximize $V_{\pi_E}$, i.e. a dynamic programming solution.

% --------------------------------------------------------------------------
\subsubsection*{A proof of optimal substructure}
A first step in solving a dynamic programming problem is isolating the relevant sub-problem, and proving it has an optimal substructure--that the problem can be decomposed into an iteratively optimal series. In Theorem we prove that any two sequential memories $M_{t+1}$ and $M_{t}$ have optimal substructure.

\begin{theorem}[Optimal substructure] \label{theorem:opt_sub}
    Assuming transition function $\delta$ is deterministic, if $V^*_{\pi_E}$ is the optimal information value given by $\pi_E$, a memory $M_{t+1}$ has optimal substructure if the the last observation $s_t$ can be removed from $M_t$, by $M_{t+1} = f^{-1}(M_{t+1}, s_t$ where the resulting value $V^*_{t-1} = V^*_{t} - F(M_t, a_t)$ is also optimal. 
\end{theorem}
\begin{proof}
    Given a known optimal value $V^*$ given by $\pi_E$ we assume for the sake of contradiction there also exists an alternative policy $\hat \pi_E \neq \pi_E$ that gives a memory $\hat M_{t-1} \neq M_{t-1}$ and for which $\hat V^*_{t-1} > V^*_{t-1}$. 

    To recover the known optimal memory $M_t$ we lift $\hat M_{t-1}$ to $M_t = f(\hat M_{t-1}, s_t)$. This implies $\hat V^* > V^*$ which in turn contradicts the purported original optimality of $V^*$ and therefore $\hat \pi_E$.
\end{proof}

% --------------------------------------------------------------------------
\subsubsection*{A Bellman solution}
Given $M$ has optimal substructure (Theorem~\ref{theorem:opt_sub}), we can use the Bellman equation \cite{Bellman} to write down an optimal greedy policy for maximizing information value $E$ over the time horizon $T$. This is given by Eqs.~\ref{eq:V_star}-\ref{eq:bellman_seq}.

\begin{equation} \label{eq:V_star}
    \begin{split}
        V^*_{\pi_E}(M_0) = \max_{a \in A} \sum_{t \in T} F(M_t, a_t)
    \end{split}
\end{equation}

\begin{equation} \label{eq:bellman_seq}
    \begin{split}
        V^*_{\pi_E}(M_0) &= \max_{a \in A} \Big [\sum_{t \in T} F(M_t, a_t)\Big ]\\
                         &= \max_{a \in A} \Big [F(M_0, a_0) + \sum_{t \in T} F(M_{t+1}, a_{t+1})\Big ]\\
                         &= F(M_0, a_0) + \max_{a \in A} \Big [\sum_{t \in T} F(M_{t+1}, a_{t+1}) \Big ]\\
                         &= F(M_0, a_0) + V^*_{\pi_E}(M_{t+1}) + V^*_{\pi_E}(M_{t+2}),\ \ldots
    \end{split}
\end{equation}

The final line in Eq.~\ref{eq:bellman_seq} lends itself to a recursive reformulation (Eq.~\ref{eq:bellman_iter}).

\begin{equation} \label{eq:bellman_iter}
    V^*_{\pi_E}(M_{t}) = F(M_{t}, a_{t}) + \max_{a \in A} \Big [ [F(M_{t+1}, a_t) \Big ]
\end{equation}
    
% ---------------------------------------------------------------------------
\subsection*{Exploration as a dynamic programming problem}
The dynamic programming solution to maximize information value is also a dynamic programming solution for exploration. By making a weak assumption on the learning of $M$, we can prove that our greedy policy $\pi^*_E$ leads to a complete and exhaustive exploration of any finite space $S$. Specifically, we show:

\begin{enumerate}[noitemsep,wide=0pt,leftmargin=\dimexpr\labelwidth+2\labelsep\relax]
    \item The optimal policy $\pi^*_E$ must visit each state in $s \in S$ at least once.
    \item The optimal policy $\pi^*_E$ must revisit each $s \in S$ until learning about each state $s$ plateaus when $g(M_{t+1}, s) = g(M_{t},s)$.
\end{enumerate}


% HERE: re-write this assumption using f. Weaken it compared to below. Convex doesn't matter.
\subsubsection*{An assumption of learning progress}
The study of information geometry models learning as an approach to information equilibrium. As learning progress plateaus, variance decreases, and therefore so does entropy. The rate at which learning plateaus is governed by the Fisher information metric, which is the hessian of the KL-divergence.

% HERE: re-watch Baez talk...

Exploration depends on learning progress. We assume that every every state observation is accompanied by change in $M$. This change can be arbitrarily small, but must be non-zero. 
% To make our proofs work we assume each observation $s$ is learned--to some small degree perhaps--by the memory $M$. Formally then, and with \textit{a loss of generality}, we assume that $\mathcal{L}_M$ is convex, and that every observation $s$ leads to learning progress on the memory $M$. Unless, that is, $\mathcal{L}_M(s) = 0$. That is, the gradient of $\triangledown \mathcal{L}_M < 0$ for all $s \in P$ when $L(s) \neq 0$. If $\triangledown \mathcal{L}_M = 0$, then for a convex learner we are at the global minimum, and so exploration and learning should cease. Having completed our initial proofs we then show under what conditions these assumptions can be relaxed.

\subsubsection*{Sorting preliminaries}
Our proofs for exploration are really sorting problems. If every state must be visited (or revisited) until learned, then under a greedy policy every state's value must--at one time or another--be the maximum value. 

Sorting requires ranking. Ranking requires that we make a definition for both the greater $>$ and less than inequalities $<$. For three real numbers, ${a,b,c} \in \mathbb{R}$.

\begin{definition} \label{def:ineq}
    \begin{align}
        a \leq b \Leftrightarrow \exists c; b = a + c \\
        a > b \leftrightarrow (a \neq b) \wedge (b \leq a) 
    \end{align}
\end{definition}

\subsubsection*{A greedy policy explores optimally}
% TODO need intro.....

% we let $ F_0 = 0$, talk about f_0 > 0. Somew

\begin{definition}
    Let $Z$ be set of all visited states, where $Z_0$ is the empty set $\{\}$ and $Z$ is built iteratively over a path $P$, such that $Z = \{s | s \in P\ \text{and}\ s \not\in Z\}$.    
\end{definition}

\begin{theorem}[State search -- completeness and uniqueness] \label{theorem:Z}
A greedy policy $\pi$ is the only deterministic policy which ensures all states in $S$ are visited, such that $Z = S$.
\end{theorem}
\begin{proof}    
    Let $\mathbf{E} = (E_1, E_2, ...)$ be ranked series of $E$ values for all states $S$, such that $(E_1 \geq E_2, \geq ...)$. To swap any pair of values ($E_i \geq E_j$) so ($E_i \leq E_j$) by Def.~\ref{def:ineq} $E_i - c = E_j$.  

    Therefore, again by Def.~\ref{def:ineq}, $\exists \int \delta E(s) \rightarrow -c$. 

    \textit{Recall}: $\triangledown \mathcal{L}_M < 0$

    However if we wished to instead swap ($E_i \leq E_j$) so ($E_i \geq E_j$) by definition $\not \exists c; E_i + c = E_j$, as $\not \exists \int \delta \rightarrow c$. 

    To complete the proof, assume that some policy $\hat \pi_E \neq \pi^*_E$. By definition policy $\hat \pi_E$ can any action but the maximum leaving $k-1$ options. Eventually as $t \rightarrow T$ the only possible swap is between the max option and the $kth$ but as we have already proven this is impossible as long as $\triangledown \mathcal{L}_M < 0$. Therefore, the policy $\hat \pi_E$ will leave at least 1 option unexplored and $S \neq Z$.
\end{proof}

\begin{theorem}[State search -- convergence] \label{theorem:convergence}
    Assuming a deterministic transition function $\Lambda$, a greedy policy $\pi_E$ will resample $S$ to convergence as $t \rightarrow T$, $E_t \rightarrow 0$.
\end{theorem}
\begin{proof}
    \textit{Recall}: $\triangledown \mathcal{L}_M < 0$. 

    Each time $\pi^*_E$ visits a state $s$, so $M \rightarrow M'$, $F(M', a_{t+1}) < F(M, a_t)$

    In Theorem~\ref{theorem:Z} we proved only deterministic greedy policy will visit each state in $S$ over $T$ trials.
    
    By induction, if $\pi^*E$ will visit all $s \in S$ in $T$ trials, it will revisit them in $2T$, therefore as $T \rightarrow \infty$, $E \rightarrow 0$. 
\end{proof}

If the transition function $\Lambda$ is stochastic, the noisy state changes will prevent $E$ from fully converging to 0. This might be ideal, as it will force continual re-exploration of the world. However if we redefine the converge of $E$ not to 0 but to some criterion $\epsilon$, we can once again ensure convergence in noisy worlds. That is, in the limit of $t \rightarrow T$, $E_t \rightarrow \epsilon$, where $0 leq \epsilon \ll E_0$ with $E_0$ denoting the initial value of $E$. Too large though, and $\epsilon$ will interfere with potential optimality of $\pi^*_E$ (by Theorem.~\ref{theorem:Z}). 

\subsection*{Optimality of $\pi_{\pi}$}

\begin{theorem}[Optimality of $\pi_{\pi}$] \label{theorem:meta}
    Assuming an infinite time horizon, if $\pi_E$ is optimal and $\pi_R$ is optimal, then $\pi_{\pi}$ is also optimal in the same sense as $\pi_E$ and $\pi_R$.
\end{theorem}
\begin{proof}
    The optimality of $|\pi_{\pi}$ can be seen by direct inspection. If, $p(R = 1) < 1$ and we have an infinite horizon, the $\pi_E$ will have a unbounded number of trials meaning the optimally of $P^*$ holds. Likewise, $\sum E < \epsilon$ as $T \rightarrow \infty$, ensuring $pi_R$ will dominate $\pi_{\pi}$ therefore $\pi_R$ will asymptotically converge to optimal behavior.
\end{proof}

In proving the total optimality of $\pi_{\pi}$ we limit the probability of a positive reward to less than one, denoted by $p(R_t = 1) < 1$. Without this constraint the reward policy $\pi_R$ would always dominate $\pi_{\pi}$ when rewards are certain. While this might be useful in some circumstances, from the point of view $\pi_E$ it is extremely suboptimal as the model would never explore. Limiting $p(R_t = 1) < 1$ is reasonable constraint, as rewards in the real world are rarely certain. A more naturalistic but complex way to handle this edge case would be to introduce reward satiety, and have reward value decay asymptotically with repeated exposure. 

\subsubsection*{An optimally rewarding policy}
If learning during $\pi_{\pi}$ is allowed to continue until $\pi_E$ converges so $E_t - \epsilon = 0$ then, by definition the animal will have completely explored its world. This implies that in turn $\pi_R$ has seen every state, and so can then choose the overall optimal value. Thus is there is a globally optimally reward policy, $\pi_{\pi}$ guarantees it will be found. Classic reinforcement learning views this search as costing potential reward. Dual value learning instead asserts a net gain. Either from information value, from rewards, or both. We suggest that rather than being fundamental, the exploration-exploitation dilemma follows from asking too little from an animal's learning objectives.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% --------------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\newpage
\section*{Supplementaty methods.}

\subsection*{E and KL.}
The Kullback--Leibler divergence (KL) is a widely used information theory metric, which measures the information gained by replacing one distribution with another. It is highly versatile and widely used in machine learning \cite{Goodfellow-et-al-2016}, Bayesian reasoning \cite{Itti2009,Friston2016}, visual neuroscience \cite{Itti2009}, experimental design \cite{Lopez-Fidalgo2007}, compression \cite{Mackay,Still2012} and information geometry \cite{Ay2015}. Using a Bayesian approach, Itti and Baladi \citep{Itti2009} developed an approach similar to ours for visual attention, where our information value is identical to their \textit{Bayesian surprise}. Itti and Baladi (2009) showed that compared to range of other theoretical alternative, information value most strongly correlates with eye movements made when humans look at natural images. Again in a Bayesian context, KL plays a key role in guiding \textit{active inference}, a mode of theory where the dogmatic central aim of neural systems is make decisions which minimize (probabilistic) free energy \cite{Friston2016,Schwartenbeck2019}. 

\subsection*{Intial values.} If both reward and information policies are deterministic, the dilemma mathematically becomes an initial value problem. Generally reward value begins at zero. However the initial set of information values, $E_0$, cannot be 0. If $E_0 = 0$, the meta-policy will never explore. While if $E_0 > 0$, Theorem~\ref{theorem:opt_sub} ensures that any $E_0$ will do. Likewise, theorem~\ref{theorem:Z} ensures that any initial choice will not effect the final optimality of the search. While the magnitude of $E_0$ does not change long term behavior, it will change its short-term dynamics. This might be important in modeling real behavior and in choosing good values for the stopping parameter $\epsilon$ (Eq~\ref{eq:meta_greedy}). 

In our simulations we assume that at the start of learning an animal should have a uniform prior over the possible actions $A \in \mathbb{R}^K$. Thus $p(a_k) = 1/K$ for all $a_k \in A$. We transform this uniform action prior into $E_0$ giving an value well scaled our KL-based realization of $E$, $E_0 = \sum_K p(a_k)\ \text{log}\ p(a_k)$. 

By definition a greedy policy can't handle ties; there is no way to rank equal values. Theorems~\ref{theorem:convergence} and~\ref{theorem:Z} ensure that any tie breaking strategy is valid, making tie breaking in one sense an arbitrary. However like the choice of $E_0$, tie breaking can strongly effect the transient dynamics of $\pi_E$ and so can be quite important in practice. Viable tie breaking strategies taken from experimental work include, ``take the closest option'', ``repeat the last option'', or ``take the option with the highest marginal likelihood''. In our simulations we break ties by ``choosing the next action'' where we placed actions in an arbitrary order.

\subsection*{Hyperparameter tuning.}
Hyper-parameter tuning involved setting the learning rates for both policies in $\pi_{\pi}$ (Eq~\ref{eq:meta_greedy}), and the convergence threshold for exploration, $\epsilon$. 

\end{document}
