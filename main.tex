% 
% Annual CCN conference
% Sample LaTeX Two-Page Summary -- Proceedings Format
% based on the prior cognitive science style file

% Original : Ashwin Ram (ashwin@cc.gatech.edu)       04/01/1994
% Modified : Johanna Moore (jmoore@cs.pitt.edu)      03/17/1995
% Modified : David Noelle (noelle@ucsd.edu)          03/15/1996
% Modified : Pat Langley (langley@cs.stanford.edu)   01/26/1997
% Latex2e corrections by Ramin Charles Nakisa        01/28/1997 
% Modified : Tina Eliassi-Rad (eliassi@cs.wisc.edu)  01/31/1998
% Modified : Trisha Yannuzzi (trisha@ircs.upenn.edu) 12/28/1999 (in process)
% Modified : Mary Ellen Foster (M.E.Foster@ed.ac.uk) 12/11/2000
% Modified : Ken Forbus                              01/23/2004
% Modified : Eli M. Silk (esilk@pitt.edu)            05/24/2005
% Modified : Niels Taatgen (taatgen@cmu.edu)        10/24/2006
% Modified : David Noelle (dnoelle@ucmerced.edu)     11/19/2014
% Modified : Konrad Kording (koerding@gmail.com) 2/15/2017

\documentclass[10pt,letterpaper]{article}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{ccn}
\usepackage{pslatex}
\usepackage{apacite}

\newtheorem{corollary}{Corollary}
\newtheorem{axiom}{Axiom}
\newtheorem{statement}{Statement}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}[theorem]{Lemma}


\title{A way around the exploration-exploitation dilemma}
 
\author{{\large \bf Erik J Peterson (erik.exists@gmail.com)} \\
  Carnegie Mellon, Dept. of Psychology\\
  5000 Forbes Ave. \\
  Pittsburgh, PA 15213
 \AND {\large \bf Timothy Verstynen (timothyv@andrew.cmu.edu)} \\
  Carnegie Mellon, Dept. of Psychology\\
  5000 Forbes Ave. \\
  Pittsburgh, PA 15213
}

\begin{document}
\maketitle

\section{Abstract}
{
\bf
    The exploration-exploitation dilemma is considered a fundamental but intractable problem in the learning and decision sciences. At its crux is the search to maximize reward. Here we challenge this view and show a way around the dilemma by defining separate mathematical objectives for exploration and exploitation. To make the objective for exploration independent of reward, we derive a set of general axioms for information value. Using these axioms we develop a greedy algorithm which provably and optimally maximizes both information and reward. 
    %The cost of this solution to the dilemma is a constant-factor increase in the worst-case algorithmic run-time.
}
\begin{quote}
\small
\textbf{Keywords:} 
reinforcement learning; exploration; theory
\end{quote}

\section{Introduction}
Exploration during behavior can have two very different explanations depending on whether an animal or artificial agent might receive a reward. If there is no reason to expect a reward, exploration is treated as a search for novel or maximum information \cite{Mehlhorn2015}. For example, when a rat is placed in a new environment it will explore even if no tangible rewards, like food and water, are present or expected \cite{Liu2019,Mehlhorn2015}. If however reward is expected an animal, and many artificial agents, will explore and discover it \cite{Mehlhorn2015}. This exploration gets interpreted as a search to maximize reward \cite{Sutton2018}. 

An open problem in the decision sciences it to unified both kinds of exploration. This is difficult in part because exploration for reward leads to the intractable problem of the exploration-exploitation dilemma, which is fundamental \cite{Thrun1992,Dayan1996,Findling2018,Gershman2018a}. 

But is it really fundamental?

\section{Results}
We conjecture \textit{all} exploration can be correctly interpreted as a search for information. Reward value doesn't matter. 

If exploration \textit{is} \textit{just} a search for max information, then what was the ``dilemma'' can be divided into two problems. One problem is exploration (recast as the search for max information) and the other is exploitation, which still means a search for max reward. The focus of this paper is deriving a simple greedy algorithm for solving both problems, simultaneously.

\subsection{Information is not a reward}
Diving the dilemma into independent parts is correct because information and reward have opposing properties.

Rewards are a conserved resource, information is not. For example, if a rat shares a potato chip with a cage-mate, it must break the chip up leaving it less food for itself. While if a student shares an idea with a classmate, that idea is not divided up. 

The value of information must decline with learning, while the value of reward is constant. For example, knowing where one potato chip is generally means going back and eating another. Whereas if a student knows the capital of the United States, there is no value to the student in being taught the capital of the United States is Washington DC.

% These two philosophical differences suggest that reward and information may also have notable mathematical differences. To evenly share a reward $r$ between $n$ others, $r_n = \frac{r}{n}$. In contrast, information value can, in principle, be shared without loss, i.e., $I_n = I$. Likewise, if an animal is learning about some state of world $s$ the value of information about $s$ should decrease with time. Assuming learning can asymptote, then as $t \rightarrow \infty, I_s \rightarrow 0$. For rewards, value never changes. So as $t \rightarrow \infty, r_s \rightarrow r_s$.\footnote{Though including reward satiety effects may somewhat diminish $r_s$ in practice.} In summary, information is fundamentally not a reward. Information about the world and the rewards from the world are very different kinds of things.


\subsection{A definition of information value}
To make our eventual solution general, we first need to separate reward value from information value in a principled and general way. We do this axiomatically. Our axioms \textit{do not} depend on information theory nor on Bayesian reasoning, the standard approaches \cite{Friston2016,Haarnoja2018,Itti2009}. 

We reason that the value of any observation $s$ made by an animal or agent depends entirely on what an animal or agent learns by making that observation--how it changes an animal or agent's memory. 

We let a memory $M$ be finite set whose maximum size is $N$--defined over a finite state space $s \in S^N$--whose elements are added by an invertible encoder $f$, such that $M_{t+1} = f(M_{t}, s)$ and $M_{t} = f^{-1}(M_{t+1}, s)$ and whose elements $z$ from $M$ are recovered by a decoder $g$, such that $z = g(M, s)$. The initial memory $M_{0}$ is said to be the empty set, $M_{0} = \emptyset$ (Definition 1).
 
Our notion of memory is general. We don't say anything about the details of the encoder $f$, the decoder $g$, the numerical properties of the individual states $s$ in the state-space $S$ (other than $S$ is finite). Nor do we require that what goes into $M$ (i.e., $s$) and what comes out ($z$) have any fixed relationship.

\begin{center} \textbf{Axiomatic information value ($E$)} \end{center}

\begin{axiom}[Axiom of Now]
    $E$ depends only on difference $dM$ between $M_{t+1}$ and $M_{t}$.
    \label{ax:1}
\end{axiom} 
\noindent
That is, the value of an observation $s$ depends only on how the current memory changes. Its history doesn't matter.

\begin{axiom}[Axiom of Novelty]
    $E = 0$ if and only if $dM = 0$. 
    \label{ax:3}
\end{axiom}
\noindent
That is, an observation that doesn't change the memory has no value.

\begin{axiom}[Axiom of Scholarship]
    $E \geq 0$.
    \label{ax:3}
\end{axiom}
\noindent
That is, all (new) information is \textit{in principle} valuable even its consequences are later found to be negative.

\begin{axiom}[Axiom of Specificity]
    $E$ is monotonic with the compactness $C$ of $dM$ (Eq.~\ref{eq:compactcude}) .
    \label{ax:5}
\end{axiom}
\noindent
That is, more specific information is more valuable than less specific information. (We use the mathematical idea of compactness to formalize specificity.)

\begin{axiom}[Axiom of Equilibrium]
    $\frac{dM^2}{d\theta^s} < 0$
    \label{ax:5}
\end{axiom}
\noindent
That is, using the parameters $\theta$ learning in $M$ makes continual progress toward equilibrium (i.e. self-consistency) with each observation and will approach a steady-state value of $dM \rightarrow 0$ and $E \rightarrow 0$.

Though they are formally independent, the axioms do borrow properties from Bayesian learning and information geometry \cite{Friston2016,Harper2009,Ly2017,Itti2009}. As a result, our definition of axiomatic value reduces to the KL divergence/information gain \cite{MacKay2003,Ly2017}. If that is, if one can assume the animals or agents memory is Bayesian or otherwise a probability distribution.

\subsection{Exploration as a dynamic programming problem}
A nice solution to maximizing $E$ would be a dynamic programming solution. It would guarantee that total value (Eq.~\ref{eq:V_E}) is maximized by a simple deterministic, locally greedy, algorithm. 

In Theorem~\ref{theorem:opt_sub} we prove our definition of memory (Def. 1) has one critical property (optimal substructure) needed for a dynamic programming solution \cite{Roughgarden2019}. The other two--$E\ge0$ and the Markov property \cite{Sutton2018}--are fulfilled by the Axioms 3 and 1 respectively. 

To write down the Bellman solution for $E$ as dynamic programming problem we need some new notation. 

Let $a$ be an action drawn from a finite action space $A^K$. Let time $t$ denote an index $t = (1,2,3,\ldots,\infty)$. Let $\pi$ denote any policy function that maps a state $s$ to an action $a$, $\pi : s \rightarrow a$, such that $\forall s_t \in S, \forall a_t \in A$. Let $\delta$ be a transition function which maps $(s_{t},a_t)$ to a new state $s_{t+1}$, $\delta : (s_{t}, a_t) \rightarrow s_{t+1}$ where once again $\forall s_t \in S, \forall a_t \in A$.

For notational simplicity we let $E$ be redefined as $F(M_{t}, a_t)$, a ``payoff function'' in dynamic programming jargon (Eq~\ref{eq:payout}). 

\begin{equation}
    \begin{split} \label{eq:payout}
    F(M_{t}, a_t) = E(M_{t+1}, M_{t})\\
    \text{subject to the constraints} \\
    a_{t} \sim \pi(s_t) \\
    s_{t+1} = \delta(s_{t}, a_t),\\ 
    M_{t+1} = f(M_{t}, s_{t})
    \end{split} 
\end{equation}

\noindent
The value function for $F$ is Eq.~\ref{eq:V_E}. The Bellman solution to recursively learn this function is found in Eq.~\ref{eq:bellman_iter}. 

\begin{equation} \label{eq:V_E}
    \begin{split}
        V_{\pi_E}(M_0) = \Big [ \max_{a \in A} \sum_{t=0}^{\infty} F(M_t, a_t) \ \Big | \ \pi_E, \ M \Big ]
    \end{split}
\end{equation}

To find recursive Bellman solution we decompose Eq.~\ref{eq:V_E} into an initial value $F_0$ and the remaining series. When this decomposition is applied recursively we eventually find an iterative optimal and greedy solution. The steps for our decomposition in terms of $F$ and $M$ are shown in Eq~\ref{eq:bellman_iter} and the result in Eq.~\ref{eq:bellman_seq}.

\begin{equation} \label{eq:bellman_seq}
    \begin{split}
        V^*_{\pi_E}(M_0) &= \max_{a \in A} \Big [\sum_{t=0}^{\infty} F(M_t, a_t)\Big ]\\
                         &= \max_{a \in A} \Big [F(M_0, a_0) + \sum^{\infty}_{t=1} F(M_{t+1}, a_{t+1})\Big ]\\
                         &= F(M_0, a_0) + \max_{a \in A} \Big [\sum_{t=1}^{\infty} F(M_{t+1}, a_{t+1}) \Big ]\\
                         &= F(M_0, a_0) + V^*_{\pi_E}(M_{t+1}) + V^*_{\pi_E}(M_{t+2}),\ \ldots
    \end{split}
\end{equation}

\begin{equation} 
\label{eq:bellman_iter}
    V^*_{\pi_E}(M_{t}) = F(M_{t}, a_{t}) + \max_{a \in A} \Big [ F(M_{t+1}, a_t) \Big ]
\end{equation}

\subsection{A note on exploration quality}
Having a policy $\pi^*_E$ that maximizes $E$ also does not necessarily ensure that exploration is of good quality. Relying on Axiom 6, Theorems~\ref{theorem:Z} and~\ref{theorem:convergence} prove $\pi^*_E$ also leads to a complete and exhaustive exploration of any \textit{finite} space $S$.

\begin{itemize}
    \item \textit{Theorem}~\ref{theorem:Z}. The optimal policy $\pi^*_E$ must visit each state in $s \in S$ at least once.
    \item \textit{Theorem}~\ref{theorem:convergence}. The optimal policy $\pi^*_E$ must revisit each $s \in S$ until learning about each state $s$ until the memory reaches equilibrium $dM = 0$.
\end{itemize}

% Having a policy that maximizes $E$ does not ensure the memory that is learned is accurate (has high mutual information between its contents and the world, Fig~\ref{}). The encoding and decoding functions are responsible for accuracy. We leave open the possibility that either function is noisy, forgetful or otherwise imperfect. We only intend that information value is purely subjective measure, concerned only with self-consistency not accuracy. As scientists we'd hope maximizing value would also lead to accuracy. As experimentalists its clear this is not always the case. An animal, especially perhaps a human animal, might learn an very inaccurate model of the world, but as long as it learns it consistently total $E$ can be reach its optimal value.

\subsection{Scheduling a way around the dilemma}
To solve the dual problems we've posed--information and reward maximization--we need an algorithm that can maximize the total value of both objectives. We found a simple (if surprising solution) in the computer science sub-field of optimal scheduling. 

There are range of optimal and useful reinforcement learning algorithms algorithm that operate on a discrete Markov decision spaces
\cite{Sutton2018}. In principle, any of these are compatible with our approach. As a result we use $\pi_R$ to denote any suitable reinforcement learning algorithm.

Animal or agent behavior is generally viewed as a fixed resource as one can only take one action at a time. With this in mind, we imagine the policies for exploration and exploitation act as two possible ``jobs'' competing for control of this fixed (behavioral) resource. By definition each of these ``jobs'' naturally produces non-negative values an optimal job scheduler could use. $E$ for information or $R$ for reward/reinforcement learning. Each of the ``jobs'' takes a constant amount of time, each policy only results in a single action $a \sim A$.

The solution to scheduling problems that 1. produce non-negative value and 2. have fixed run times is known to be a simple greedy algorithm \cite{Roughgarden2019}. We restate this solution as a pair of inequalities in Eq.~\ref{eq:pipi}. 

\begin{equation} 
    \label{eq:pipi}
    \begin{split}
        \pi_{\pi}(s) = 
        \begin{cases}
            \pi^*_E(s) & : E_s - \epsilon > R_s \\
            \pi_R(s) & : E_s - \epsilon \le R_s \\
        \end{cases}\\
        \text{subject to the constraints}\\
        R_s \ \in \{0, 1 \}\\
        p(R_s) < 1
    \end{split}
\end{equation}

\subsubsection{The fine print}
The inequality in Eq~\ref{eq:pipi} ensures total value summed over $E_t$ and $R_t$ is maximized \cite{Roughgarden2019}. It does not ensure each policy also maintains its own optimality. Specifically we're interested in  Theorems~\ref{theorem:Z} and~\ref{theorem:convergence}. We ensure these hold by introducing two constraints the reward distribution shown in Eq~\ref{eq:pipi} (and which we prove are sufficient in Theorem~\ref{theorem:meta}).

In stochastic environments learning in $M$ may show small continual fluctuations, mirroring the environmental dynamics. These will of course get reflected in $E$ fluctuations. To allow Eq.~\ref{eq:pipi} to achieve a stable solution we introduce $\epsilon$, a boredom threshold for exploration. When $E_t < \epsilon$ we say the policy is ``bored'' with exploration. To be consistent with the value axioms, $\epsilon > 0$ and $E + \epsilon \geq 0$. 

To ensure the default policy is reward maximization, Eq.~\ref{eq:pipi} breaks ties between $R_t$ and $E_t$ in favor of $\pi_R$.

The initial value $E_0$ for $\pi^*_E$ can be arbitrary with the limit $E_0 > 0$. In theory $E_0$ does not change $\pi^*_E$'s long term behavior, but different values will change the algorithms short-term dynamics and so might be quite important in practice. 

By definition a pure greedy policy, like $\pi^*_E$, can't handle ties. There is simply no mathematical way to rank equal values. Theorems~\ref{theorem:convergence} and~\ref{theorem:Z} ensure that any tie breaking strategy is valid. However like the choice of $E_0$, tie breaking can strongly effect the transient dynamics.

\subsection{Algorithmic complexity}
The worst case run time for $\pi_{\pi}$ is additive in its policies. That is, if in isolation it takes $T_E$ steps to earn $E_{T} = \sum_{T_E} E$, and $T_R$ steps to earn $r_{T} = \sum_{T_R} R$, then the worst case training time for $\pi_{\pi}$ is $T_E + T_R$, if that is neither policy can learn from the other's control. There is however no reason each policy can't observe the transitions $(s_t, a_t, R, s_{t+1})$ caused by the other. If this is allowed, worst case training time improves to $\max(T_E, T_R)$. 

\section{Summary}
We show a way around the exploration-exploitation dilemma. In our approach exploration is done only to maximize information value, a quantity we define axiomatically. Maximizing information value forces the animal to learn a general, reward-independent, memory of the world. An important ``side effect'' of this general learning is that reward learning improves to optimality. 

\bibliographystyle{apacite}
\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}
\bibliography{library}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% --------------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
% \newpage
\section{Mathematical Appendix.}
\subsection*{Compactness}
The compactness $C$ of a hyper-cube has a simple formula, $C = \frac{P^2}{A}$ where $P$ is the cube's perimeter and $A$ is its area. Therefore as $M_{t}$ moves to $M_{t+1}$, the we can measure the distances $\{d_i, d_{i+1}, d_{i+2},\ldots d_{N}\}$ for all $O \leq N$ observed states $Z$, such that $Z \subseteq S$ and treat them as if they formed a $O$-dimensional hyper-cube. In measuring this imagined cube we arrive at a geometric estimate for the compactness of changes to memory (Eq.~\ref{eq:compactcude}).

\begin{equation} \label{eq:compactcude}
\begin{split}
    C & = \frac{\Big ( 2 \sum^{O}_{i} d_i \Big )^2}{\prod^{O}_{i} d_i}
\end{split}
\end{equation}

\subsection*{Information value as a dynamic programming problem}
To use theorems from dynamic programming \cite{Roughgarden2019,Sutton2018} we must prove our memory $M$ has optimal substructure. By optimal substructure we mean that $M$ can be partitioned into to a small number collection or series, each of which is an optimal dynamic programming solution. In general by proving we can decompose some optimization problem in a set of smaller problems whose optimal solution is easy to find or prove, it trivial to prove we can also grow the series optimally which in turns allows for direct proofs by induction.

\begin{theorem}[Optimal substructure] \label{theorem:opt_sub}
    Assuming transition function $\delta$ is deterministic, if $V^*_{\pi_E}$ is the optimal information value given by $\pi_E$, a memory $M_{t+1}$ has optimal substructure if the the last observation $s_t$ can be removed from $M_t$, by $M_{t+1} = f^{-1}(M_{t+1}, s_t)$ where the resulting value $V^*_{t-1} = V^*_{t} - F(M_t, a_t)$ is also optimal. 
\end{theorem}
\begin{proof}
    Given a known optimal value $V^*$ given by $\pi_E$ we assume for the sake of contradiction there also exists an alternative policy $\hat \pi_E \neq \pi_E$ that gives a memory $\hat M_{t-1} \neq M_{t-1}$ and for which $\hat V^*_{t-1} > V^*_{t-1}$. 

    To recover the known optimal memory $M_t$ we lift $\hat M_{t-1}$ to $M_t = f(\hat M_{t-1}, s_t)$. This implies $\hat V^* > V^*$ which in turn contradicts the purported original optimality of $V^*$ and therefore $\hat \pi_E$.
\end{proof}

\subsubsection{Bellman solution}
To find the recursive Bellman solution we decompose Eq.~\ref{eq:V_E} into an initial value $F_0$ and the remaining series in the summation. When this decomposition is applied recursively (Eq~\ref{eq:bellman_iter}) we arrive at an iterative optimal and greedy solution (Eq.~\ref{eq:bellman_seq}). 

\begin{equation} \label{eq:bellman_seq}
    \begin{split}
        V^*_{\pi_E}(M_0) &= \max_{a \in A} \Big [\sum_{t=0}^{\infty} F(M_t, a_t)\Big ]\\
                         &= \max_{a \in A} \Big [F(M_0, a_0) + \sum^{\infty}_{t=1} F(M_{t+1}, a_{t+1})\Big ]\\
                         &= F(M_0, a_0) + \max_{a \in A} \Big [\sum_{t=1}^{\infty} F(M_{t+1}, a_{t+1}) \Big ]\\
                         &= F(M_0, a_0) + V^*_{\pi_E}(M_{t+1}) + V^*_{\pi_E}(M_{t+2}),\ \ldots
    \end{split}
\end{equation}

\subsection*{A greedy policy explores exhaustively}
Our proofs for exploration breadth are really sorting problems. If every state must be visited (or revisited) until learned, then under a greedy policy every state's value must--at one time or another--be the maximum value. 

Let $Z$ be set of all visited states, where $Z_0$ is the empty set $\{\}$ and $Z$ is built iteratively over a path $P$, such that $Z = \{s | s \in P\ \text{and}\ s \not\in Z\}$.    

Sorting requires ranking, so we define an algebraic notion of inequality for any three numbers ${a,b,c} \in \mathbb{R}$ are defined in Eq.~\ref{eq:ineq}.

\begin{align}
    \label{eq:ineq}
    a \leq b \Leftrightarrow \exists \ c;\ b = a + c \\
    a > b \Leftrightarrow (a \neq b) \wedge (b \leq a) 
\end{align}

\begin{theorem}[State search -- completeness and uniqueness] \label{theorem:Z}
A greedy policy $\pi$ is the only deterministic policy which ensures all states in $S$ are visited, such that $Z = S$.
\end{theorem}
\begin{proof}    
    Let $\mathbf{E} = (E_1, E_2, ...)$ be ranked series of $E$ values for all states $S$, such that $(E_1 \geq E_2, \geq ...)$. To swap any pair of values ($E_i \geq E_j$) so ($E_i \leq E_j$) by Eq.~\ref{eq:ineq} $E_i - c = E_j$.  

    Therefore, again by Eq.~\ref{eq:ineq}, $\exists \int \delta E(s) \rightarrow -c$. 

    \textit{Recall}: $\triangledown \mathcal{L}_M < 0$

    However if we wished to instead swap ($E_i \leq E_j$) so ($E_i \geq E_j$) by definition $\not \exists c; E_i + c = E_j$, as $\not \exists \int \delta \rightarrow c$. 

    To complete the proof, assume that some policy $\hat \pi_E \neq \pi^*_E$. By definition policy $\hat \pi_E$ can any action but the maximum leaving $k-1$ options. Eventually as $t \rightarrow T$ the only possible swap is between the max option and the $kth$ but as we have already proven this is impossible as long as $\triangledown \mathcal{L}_M < 0$. Therefore, the policy $\hat \pi_E$ will leave at least 1 option unexplored and $S \neq Z$.
\end{proof}

\begin{theorem}[State search -- convergence] \label{theorem:convergence}
    Assuming a deterministic transition function $\Lambda$, a greedy policy $\pi_E$ will resample $S$ to convergence as $t \rightarrow T$, $E_t \rightarrow 0$.
\end{theorem}
\begin{proof}
    \textit{Recall}: $\triangledown \mathcal{L}_M < 0$. 

    Each time $\pi^*_E$ visits a state $s$, so $M \rightarrow M'$, $F(M', a_{t+1}) < F(M, a_t)$

    In Theorem~\ref{theorem:Z} we proved only deterministic greedy policy will visit each state in $S$ over $T$ trials.
    
    By induction, if $\pi^*E$ will visit all $s \in S$ in $T$ trials, it will revisit them in $2T$, therefore as $T \rightarrow \infty$, $E \rightarrow 0$. 
\end{proof}

\subsection*{Optimality of $\pi_{\pi}$} \label{sec:opt_pipi}
\begin{theorem}[Optimality of $\pi_{\pi}$] \label{theorem:meta}
    Assuming an infinite time horizon, if $\pi_E$ is optimal and $\pi_R$ is optimal, then $\pi_{\pi}$ is also optimal in the same sense as $\pi_E$ and $\pi_R$.
\end{theorem}
\begin{proof}
    The optimality of $|\pi_{\pi}$ can be seen by direct inspection. If, $p(R = 1) < 1$ and we have an infinite horizon, the $\pi_E$ will have a unbounded number of trials meaning the optimally of $P^*$ holds. Likewise, $\sum E < \epsilon$ as $T \rightarrow \infty$, ensuring $pi_R$ will dominate $\pi_{\pi}$ therefore $\pi_R$ will asymptotically converge to optimal behavior.
\end{proof}

In proving the total optimality of $\pi_{\pi}$ we limit the probability of a positive reward to less than one, denoted by $p(R_t = 1) < 1$. Without this constraint the reward policy $\pi_R$ would always dominate $\pi_{\pi}$ when rewards are certain. While this might be useful in some circumstances, from the point of view $\pi_E$ it is extremely suboptimal as the model would never explore. Limiting $p(R_t = 1) < 1$ is reasonable constraint, as rewards in the real world are rarely certain. A more naturalistic but complex way to handle this edge case might be to introduce reward satiety, and have reward value decay with repeated exposure. 
\end{document}
