\documentclass[fleqn,10pt]{wlscirep}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsthm} 
\usepackage{amsmath} 
\usepackage{amssymb} 
\usepackage{multicol} 
\usepackage{array}
\usepackage{hyperref}
\usepackage{lineno}

\DeclareMathOperator*{\argmax}{argmax} 
\newtheorem{axiom}{Axiom} 
\newtheorem{theorem}{Theorem}


\title{A way around the exploration-exploitation dilemma}

\author[1,2*]{Erik J Peterson}
\author[2,3,4]{Timothy V Verstynen}
\affil[1]{Department of Psychology} 
\affil[2]{Center for the Neural Basis of Cognition} 
\affil[3]{Carnegie Mellon Neuroscience Institute}
\affil[4]{Biomedical Engineering, Carnegie Mellon University, Pittsburgh PA}
\affil[*]{Erik.Exists@gmail.com}

\begin{abstract}
The optimal decision to exploit existing rewards, or explore looking for larger rewards, is known to be a mathematically intractable problem. Here we challenge this fundamental result in the learning and decision sciences by showing that there is an optimal solution if exploitation and exploration are treated as independent, but competing, objectives. To make it independent we re-imagine exploration as an open-ended search for any new information, whose value we define with a new set of universal axioms. We prove a reward-information competition can be solved by a deterministic winner-take-all algorithm. This algorithm has the properties expected of an ideal solution to the original dilemma, maximizing total value with no regret. Our work suggests an answer to all exploration-exploitation questions can found by exploring simply to learn.
\end{abstract}
\begin{document}
\flushbottom
\maketitle

\thispagestyle{empty}
\linenumbers
\section*{Introduction}
Decision making in the natural world often leads to a dilemma. For example, let's imagine a bee foraging in a meadow. The bee could go to the location of a flower it's been to before (exploitation) or it could go somewhere else, to exploration. In most accounts of this situation, decisions to explore and exploit both try to maximize tangible rewards, like nectar \cite{Sutton2018} (Figure~\ref{fig:f1}\textbf{A}). This account however contains in it a mathematically intractable problem. There is no way prove, for any given moment, whether it is optimal to explore or to exploit \cite{Thrun1992a,Dayan1996,Findling2018,Gershman2018b}.    

But resource gathering is not the only reason animals explore. Many animals, like our bee, explore to learn about their environment, developing an often simplified model that helps them in planning actions and making future decisions \cite{Ahilan2019,Poucet1993}. Borrowing from the field of artificial intelligence we refer to these models as "world models" \cite{Schmidhuber2019,Sutton2018,Schmidhuber1991}. World models offer a principled explanation for why animals are intrinsically curious \cite{Mehlhorn2015,Gupta2006,Berger-Tal2014,Gottlieb2018,Schwartenbeck2019,Pathak2017}, and prone to explore even when no rewards are present or expected \cite{Hughes1997}. %In some cases information seeking is known to happen even if it explicitly leads to a loss of reward \cite{Wang2019}. 

Given animals explore just to learn, we wondered if exploration exclusively for reward is necessary. This lead us to make a bold conjecture that's the focus of this paper. The only exploratory behavior an animal needs is that which builds its world model (Figure~\ref{fig:f1}\textbf{B}); exploration for reward is redundant.
% We go on to prove here this mode of exploration is sufficient to also optimize for rewards and, most importantly, has a tractable and optimal solution. Optimal here means maximising value while minimizing regret.

\begin{figure}
	[tbhp] \centering 
	\includegraphics[width=.7\linewidth]{figures/fig1.png} 
	\caption{\label{fig:f1} Two views of exploration and exploitation. \textbf{A}. The classic dilemma: either exploit an action with a known reward (e.g., return to the previous plant) or explore other actions on the chance they will return a better outcome (e.g., find a plant with more flowers). \textbf{B.} Here we offer an alternative view of the dilemma, with two different competitive goals: maximize rewards (e.g., keep returning to known flower locations) \texti{or} build a world model by learning new information (e.g., layout of the environment). Exploration here focused on learning in general, not on reward learning specifically. \textit{Artist credit}: Richard Grant.}
\end{figure}

Our contribution is then threefold. We offer two axioms that serve as a general basis to value of any observation, given a world model. This lets us define a separate objective for exploration. In fact, the axioms lets us formally disconnect Shannon's Information Theory from any notion of information value, which leads to a new universal theory. Next we prove that the computer science method of dynamic programming \cite{Bellmann1954,Sutton2018} provides an optimal way to maximize this kind of information value. Finally, we describe a simple winner-take-all algorithm that optimally maximizes both information value and reward. 

\section*{Results}
\subsection*{A definition of information value}
Tangible rewards are a conserved resource, while learned information is not. This makes them distinct concepts or ideas. For example, if a rat shares potato chip with a cage-mate, she must necessarily split up the chip leaving less food for herself. Whereas if student shares the latest result from a scientific paper with a lab-mate, they do not necessarily forget a portion of that result.

In an optimization problem, like the dilemma, we assume distinct concepts should have separate objective functions. To do this we first looked to the field of information theory \cite{Shannon1948}, but found it wanting. The problem of information value is not based in the statistical problem of transmitting symbols, as was Shannon’s goal. It is based on the problem of learning and remembering them--which is a very different problem. 

\subsubsection*{A model of memory}
World models are memories with some amount of simplification \cite{Schmidhuber1991,Ha2018}. They can range from simple novelty signals \cite{Kakade2002}, to location or state counts \cite{Bellemare2016,Dayan1993}, state and action prediction \cite{Schmidhuber1991,Pathak2017,Friston2016}, flow \cite{Yang2019}, learning progress \cite{Lopes2012}, classic working or episodic memories \cite{Miller1956,Tulving2002}, Bayesian and hierarchical Bayesian models \cite{Park2017,Itti2009,Friston2016,Tenenbaum2006}, latent spaces \cite{Kingma2013} and recurrent neural networks \cite{Ganguli2008,Ha2018,Schmidhuber2015a,Mante2013}. 

We have no reason to prefer any one kind of world model over any other. So we adopt an abstract definition designed to overlap with nearly any world model. We assume that time is a continuous value and denote increases in time using the differential quantity $dt$. We then express changes in $M$ (our world model, defined below) as a gradient, $\nabla M$. We also assume that observations about the environment $s$ are real numbers sampled from a finite state space $s \in S$, whose size is $N$ (denoted $S^N$). Actions are also real numbers $a$, drawn from a finite space $A^K$. Rewards $R_t$, when they appear, are binary $(0,1)$ and are provided by the external environment. 

\textit{Definition 1.} We can now formally define a world model $M$ as a finite set of real numbers, whose maximum size is also $N$ ($M^N$). We say that every world model has a pair of functions $f$ and $g$ which function as encoder and decoder. Learning of $s$ at time $t$ (i.e. $s_t$) by $M$ is done by the invertible encoder function $f$, $M_{t+dt} = f(M_{t}, s_{t})$ and $M_{t} = f^{-1}(M_{t+dt}, s_{t})$. Memories $\hat s_t$ about $s_t$ are recalled by the decoder function $g$, $\hat s_t = g(M_t, s_t)$. 

The invertibility of $f$, denoted as $f^{-1}$, is a mathematical way to ensure that any observations that be encoded in the world model can also be forgotten. This is both an important aspect of real memory, and a critical point for our analysis.

The details of $f$ and $g$ define what kind of world model $M$ is. Let's consider some examples. If $f$ adds states $s_t$ to the memory, and $g$ tests whether $s_t$ is in $M$, then $M$ is a model of novelty \cite{Kakade2002}. If $f$ counts states and $g$ returns those counts, then $M$ is a count-based heuristic \cite{Bellemare2016,Dayan1993}. If $f$ follows Bayes rule and $g$ decodes the probability of $s_t$, then $M$ is a Bayesian memory \cite{Itti2009,Friston2016,Tenenbaum2006,Schmidhuber1991,Pathak2017,Friston2016}. If the size of $M$ is much smaller than the size of the state space $S^N$, then $f$ can be seen as learning a latent or compressed representation im $M$ \cite{Kingma2013,Schmidhuber2008,Levi-Aharoni2019,Ganguli2010,Ha2018,Schmidhuber2015a,Mante2013,Park2017}, and $g$ decodes a reconstruction of $s$ ($\hat s_t$) or future states ($\hat s_{t+dt}$).
 
% \subsubsection*{Axioms for information value}
% In all of these examples, the value of any observation made by an agent who is learning a world model depends entirely on what the agent learns by making that observation.
% \begin{axiom}
% 	[Axiom of Memory] The value of $E(s_t)$ depends \emph{only} on how the memory $M$ changes.
% \label{ax:1} \end{axiom}
% \noindent This axiom does two things. It ensures value depends only on the memory, and how it changes. And in particular it ensures value depends only on the most recent change in $M$. That driven by observing $s_t$. Readers familiar with reinforcement learning can see this as a natural way to induce the Markov property on the memory.
% \begin{axiom}
% 	[Axiom of Novelty] The value of $E(s_t)$ is zero only when $\nabla M$ is zero. 
% \label{ax:2} \end{axiom}
% \begin{axiom}
% 	[Axiom of Scholarship] Learning can't lead to negative value, so the value of $E(s_t) \geq 0$.
% \label{ax:3} \end{axiom}
% \begin{axiom}
%     [Axiom of Maximum Difference] The value of $E(s_t)$ should be the maximum value which depends on the total change in $M$.
% \label{ax:4} \end{axiom}

% \begin{axiom}
%     [Axiom of Specificity] If the total change in memory $M$ due to observation $s_t$ is held constant ($\frac{dM}{dt} = h$), the more compact (Eq.~\ref{eq:compactcude}) the change in memory the more valuable the observation. 
% \label{ax:4} \end{axiom}
% \noindent Axiom~\ref{ax:4} adds two critical and intertwined properties. It ensures that if all else is held equal, more specific observations are more valuable that less specific observations \cite{Berlyne1950,Kidd2015}. It also ensures that an observation that leads to a simplifying or parsimonious insight (is equivalent to a compression of the memory, \cite{Schmidhuber2008}) is more valuable than one that changes memory the same total amount but does not lead to compression.

% TODO: this can be violated easily. Visit the full def of PAC learning
% and just use that... skip NP?
% \begin{axiom}
% 	[Axiom of Equilibrium] An observation $s_t$ must be learnable by $M$. $\frac{d^2M(s_t)}{dt^2} \leq 0$. 
% \label{ax:5} \end{axiom}

\subsubsection*{Axioms for information value}
To formalize information value we use two axioms that define a real valued function, $E(s)$, that measures the value of any observation $s_t$ given a world model $M$.

\begin{axiom}
    [Axiom of Change] The value of $E(s_t)$ depends only on the total distance $M$ moves by making observation $s_t$.
\label{ax:4} \end{axiom}

\noindent This simple axiom does three important things. It ensures that value depends only on the world model, that value is a distance in memory, and that value learning has the Markov property \cite{Sutton2018}. 

Distance in memory is the natural choice. When value is distance, learning is always a valuable thing that cannot lead to negative value (\textit{i.e.}, $E(s_t) \ge 0$). When value is distance, value is zero only when $\nabla M$ is zero. That is, only no learning means there is no value. 

Different $f$ and $g$ pairs will naturally need different ways to measure distances in $M$. For example, in a novelty world model \cite{Kakade2002} either the hamming or Manhattan distance are applicable and would produce binary distance values, as would a count model \cite{Bellemare2016,Dayan1993}. A latent memory \cite{Schmidhuber1991,Pathak2017} might instead use the euclidean norm of its own error gradient \cite{Pascanu2013}. While a probabilistic or Bayesian memory would likely use the Kullback–Leibler (KL) divergence \cite{Park2017,Friston2016}.  From the axiomatic point of view, all of these are equally good choices.
 
Axiom~\ref{ax:4} does not require that the distance in memories from $M$ to $M'$ be the same as from $M'$ to $M$. For the technically inclined this makes the distance metric $d$ underlying $E$ a formally only a pre-metric. For many choices of $M$ and $d$ this symmetry will be present, e.g., Euclidean norm). But such symmetry is not necessary and in some cases not desirable (e.g., the KL divergence on a Bayesian memory).

\begin{axiom}
	[Axiom of Equilibrium] To be valuable an observation $s_t$ must be learnable by $M$; $\mathbb{E}\big [\nabla^2 M \big ] \leq 0$ for all $s \in S$. 
\label{ax:5} 
\end{axiom}
\noindent By learnable we mean two things. First, with every (re)observation of $s$, $M$ should change. Second, the change in $M$ must eventually reach a learned equilibrium. Learnability is necessary for information value to have certain meaning. 

Most attempts to value information rest their definition on information theory. Value might rest on the intrinsic complexity of an observation (i.e., its entropy) \cite{Haarnoja2018} or on its similarity to the environment (i.e., mutual information) \cite{Kolchinsky2018}, or on some other salience signal \cite{Tishby2000}. In our analysis though learning alone drives value. This is because learning might happen on a true world model or with a faulty world model, or be about a fictional narrative. The observation might be simple, or complex. From the subjective point of view, which is the right point of view, the value for all these can be the same. Value depends only on the total knowledge gained.

\subsection*{Exploration as a dynamic programming problem} Dynamic programming is a popular optimization method because it guarantees value is maximized using a simple algorithm that always chooses the largest option. In Theorem~\ref{theorem:opt_sub} (see \textit{Mathematical Appendix}) we prove that our definition of memory has one critical property, optimal substructure, that is needed for an optimal dynamic programming solution \cite{Bellmann1954,Roughgarden2019}. The other two required properties, $E \ge 0$ and the Markov property \cite{Bellmann1954,Roughgarden2019}, are fulfilled by the \textit{Axiom 1}. 

To write down our dynamic programming solution we introduce a little more notation. We let $\pi$ denote an action policy, a function that takes a state $s$ and returns an action $a$. We let $\delta$ denote the transition function, which takes a state-action pair $(s_{t},a_t)$ and returns a new state, $s_{t+dt}$. This function acts as an abstraction for the actual world. For notational consistency with the standard Bellman approach we also redefine $E(s)$ as a \textit{payoff function}, $F(M_{t}, a_t)$ \cite{Bellmann1954}.
 
\begin{equation}
	\begin{split}\label{eq:payout} 
		F(M_{t}, a_t) = E(s)\\
		\text{subject to the constraints} \\
		a_{t} = \pi(s_t) \\
		s_{t+dt} = \delta(s_{t}, a_t),\\
		M_{t+dt} = f(M_{t}, s_{t}) 
	\end{split}
\end{equation}

\noindent The value function for $F$ is,

\begin{equation}\label{eq:V_E} 
	\begin{split}
		V_{\pi_E}(M_0) = \Big [ \max_{a \in A} \sum_{t=0}^{\infty} F(M_t, a_t) \ \Big | \ M,\ S,\ A \Big ]. 
	\end{split}
\end{equation}

\noindent And the recursive Bellman solution to learn this value function is,

\begin{equation}\label{eq:bellman_iter} 
	V^*_{\pi_E}(M_{t}) = F(M_{t}, a_{t}) + \max_{a \in A} \Big [ F(M_{t+dt}, a_t) \Big ].
\end{equation}

For the full derivation of Eq~\ref{eq:bellman_iter} see the \textit{Mathematical Appendix}, where we also prove that Eq~\ref{eq:bellman_iter} leads to exhaustive exploration of any finite space $S$ (Theorems~\ref{theorem:Z} and~\ref{theorem:convergence}).

% Axiom~\ref{ax:5} requires that learning in $M$ converge. Axiom~\ref{ax:4} requires information value increases with surprise, re-scaled by specificity. When combined with a greedy action policy like $\pi^*_E$, these axioms naturally lead to active learning \cite{Shyam2018,Pathak2019,Schwartenbeck2019} and to adversarial curiosity \cite{Schmidhuber2019a}.

\subsection*{Scheduling a way around the dilemma} Remember that the goal of reinforcement learning is to maximize reward, an objective approximated by the value function $V_R(s)$ and an action policy $\pi_R$. 

\begin{equation}\label{eq:V_R} 
	V^{\pi_R}_R(s) = \mathbb{E} \Big [ \sum_{k=0}^{\infty} R_{t+k+1} \big | s = s_t \Big ]
\end{equation}

Remember too that our overall goal is to find an algorithm that maximizes both information and reward value. To do that we imagine the policies for exploration and exploitation are possible ``jobs'' competing to control behavior. We know that, by definition, each of these jobs produces non-negative values: $E$ for information or $R$ for reinforcement learning. So our goal is to find an optimal scheduler for these two jobs. 

To do this we further simplify our assumptions. We assume each action takes a constant amount of time, and has no energetic cost. We assume the policy can only take one action at a time, and that those actions are exclusive. Most scheduling solutions also assume that the value of a job is fixed, while in our problem information value changes as the world model improves. In a general setting however, where one has no prior information about the environment, the best predictor of the next value is the last or most recent value \cite{Hocker2019,Roughgarden2019}. We assume this precept holds in all of our analysis.

With these assumptions in place, the optimal solution to this kind of scheduling problem is known to be a purely local, winner-take-all, algorithm \cite{Bellmann1954,Roughgarden2019}. We state this winner-take-all solution here as a set of inequalities where $R_t$ and $E_t$ represent the value of reward and information at the last time-point.

\begin{equation}
\label{eq:pipi} 
	\begin{split}
		\pi_{\pi}(s_t) = 
		\begin{cases}
			\pi^*_E(s_t) & : E_t - \eta > R_t \\
			\pi_R(s_t) & : E_t - \eta \le R_t \\
		\end{cases}
		\\
		\text{subject to the constraints}\\
		p(\mathbb E[R]) < 1 \\
		E - \eta \geq 0
	\end{split}
\end{equation}

To ensure that the default policy is reward maximization, Eq.~\ref{eq:pipi} breaks ties between $R_t$ and $E_t$ in favor of $\pi_R$. In stochastic environments, $M$ can show small continual fluctuations. To allow Eq.~\ref{eq:pipi} to achieve a stable solution we introduce $\eta$, a boredom threshold for exploration. Larger values of $\eta$ devalue information exploration and favor exploitation of reward.

 The worst case algorithmic run time for Eq~\ref{eq:pipi} is linear and additive in its policies. So if in isolation it takes $T_E$ steps to earn $E_{T} = \sum_{T_E} E$, and $T_R$ steps to earn $r_{T} = \sum_{T_R} R$, then the worst case training time for $\pi_{\pi}$ is $T_E + T_R$. It is worth noting that this is only true if neither policy can learn from the other's actions. There is, however, no reason that each policy cannot observe the transitions $(s_t, a_t, R, s_{t+dt})$ caused by the other. If this is allowed, worst case training time improves to $\max(T_E, T_R)$.

\subsection*{Exploration without regret} Suboptimal exploration strategies will lead to a loss of potential rewards by wasting time on actions that have a lower expected value. Regret $G$ measures the value loss caused by such exploration. $G = \hat V - V_a$, where $\hat V$ represents the maximum value and $V_a$ represents the value found by taking an exploratory action rather than an exploitative one \cite{Sutton2018}. 

Optimal strategies for a solution to the exploration-exploitation dilemma should maximize total value with zero total regret. 

\begin{figure}
	[tbhp] \centering 
	\includegraphics[width=.6\linewidth]{figures/fig2.png} 
	\caption{ \label{fig:f2} Bandits. Reward probabilities for each arm in bandit tasks I-IV. Grey dots highlight the optimal (i.e., highest reward probability) arm. See main text for a complete description.} 
\end{figure}

\begin{figure}
	[tbhp] \centering 
	\includegraphics[width=.5\linewidth]{figures/fig3.png} 
	\caption{ \label{fig:f3} Regret and total accumulated reward across models and bandit task. Median total regret (left column) and median total reward (right column) for simulations of each model type ($N=100$ experiments per model). See main text and Table~\ref{tab:agents} for description of each model. Error bars in all plots represent median absolute deviation.} 
\end{figure}

To evaluate dual value learning (Eq.~\ref{eq:pipi}) we compared total reward and regret across a range of both simple, and challenging multi-armed bandit tasks. Despite its apparent simplicity, the essential aspects of the exploration-exploitation dilemma exist in the multi-armed bandit task \cite{Sutton2018}. Here the problem to be learned is the distribution of reward probabilities across arms (Figure ~\ref{fig:f2}).  To estimate the value of any observation $s_t$, we compare sequential changes in this probabilistic memory, $M_{t+dt}$ and $M_t$ using the KL divergence (i.e. relative entropy; Figure \ref{fig:supf1}\textbf{A}-\textbf{B}). The KL divergence is a standard way to measure the distance between two distributions \cite{MacKay2003} and is, by design, consistent with our axioms (see the \textit{Supplementary Materials} for a more thorough discussion). 

We start with a simple experiment involving a single high value arm. The rest of the arms have a uniform reward probability (Bandit \textbf{I}). This represents a trivial problem. Next we tried a basic exploration test (Bandit \textbf{II}), with one winning arm and one distractor arm whose value is close to but less than the optimal choice. We then move on to a more difficult sparse exploration problem (Bandit \textbf{III}), where the world has a single winning arm, but the overall probability of receiving any reward is very low ($p(R) = 0.02$ for the winning arm, $p(R) = 0.01$ for all others). Sparse reward problems are notoriously difficult to solve, and are a common feature of both the real world and artificial environments like Go, chess, and class Atari video games \cite{Mniha,Silver2016b,Silver2018}. Finally, we tested a complex, large world exploration problem (Bandit (\textbf{IV}) with 121 arms, and a complex, randomly generated reward structure. Bandits of this type and size are near the limit of human performance \cite{Wu2018}. 

We compared the reward and regret performance of 6 artificial agents. All agents used the same temporal difference learning algorithm (TD(0), \cite{Sutton2018}); see \textit{Supplementary materials}). The only difference between the agents was their exploration mechanism (Table~\ref{tab:agents}).  The e-greedy algorithm is a classic exploration mechanism \cite{Sutton2018}. Its annealed variant is common in state-of-the-art reinforcement learning papers, like Mnih \emph{et al} (\cite{Mniha}). Other state-of-the-art exploration methods are models that treat Bayesian information gain as an intrinsic reward and the goal of all exploration is to maximize total reward (extrinsic plus intrinsic) \cite{Jaegle2019,Schmidhuber1991}. To provide a lower bound benchmark of performance we included an agent with a purely random exploration policy.

\newcolumntype{L}{>{\arraybackslash}m{4cm}} 
\begin{table}[] 
    \centering 
	\caption{Artificial agents.} \label{tab:agents} 
	\begin{tabular}
		{|l|L|} \hline \textbf{Agent} & \textbf{Exploration mechanism} \\
		\hline Dual value & Our algorithm (Eq~\ref{eq:pipi}). \\
		\hline E-greedy & With probability $1-\epsilon$ follow a greedy policy. With probability $\epsilon$ follow a random policy. \\
		\hline Annealed e-greedy & Identical to E-greedy, but $\epsilon$ is decayed at fixed rate. \\
		\hline Bayesian reward & Use the KL divergence as a weighted intrinsic reward, sampling actions by a soft-max policy. $\sum_T R_t + \beta E_t$ \\
		\hline Random & Action are selected with a random policy (no learning) \\
		\hline 
	\end{tabular}
\end{table}

All of the classic and state-of-the-art algorithms performed well at the different tasks in terms of accumulation of rewards (right column, Figure \ref{fig:f3}). The one exception to this being the sparse low reward probability condition (Bandit \textbf{III}), where the dual value algorithm consistently returned more rewards than the other models. In contrast, most of the traditional models still had substantial amounts of regret in most of the tasks, with the exception of the annealed variant of the e-greedy algorithm during the sparse, low reward probability task (left column, Figure~\ref{fig:f3}). In contrast, the dual value learning algorithm consistently was able to maximize total reward with zero or near zero (Bandit \textbf{III}) regret, as would be expected by an optimal exploration policy.


\section*{Discussion}
\subsection*{Past work}
We are certainly not the first to quantify information value \cite{Kolchinsky2018,CogliatiDezza2017}, or use that value to optimize reward learning \cite{Kelly1956,Schmidhuber1991,Dayan1996,deAbril2018,Itti2009}. Information value though is typically framed as a means to maximize the amount of tangible rewards (e.g., food, water, money) accrued over time \cite{Sutton2018}. This means that information is treated as an analog of these tangible or external rewards (i.e., an \textit{intrinsic reward}) \cite{Schmidhuber1991,Berger-Tal2014,Itti2009,Friston2016}. This approximation does drive exploration in a practical and useful way, but doesn't change the intractability of the dilemma \cite{Thrun1992a,Dayan1996,Findling2018,Gershman2018b}. 

% Many accounts of information value rely on both Bayesian reasoning, and information theory \cite{Kelly1956,Itti2009,Friston2016}. In a formal, descriptive, or mathematical world, using these makes sense. However for a naturalistic theory of learning these are strong assumptions, that may not hold up. For example, if it turns out that animals are not in fact general Bayesian reasoning systems. 

At the other extreme from reinforcement learning are pure exploration methods, like curiosity \cite{Berlyne1950,Jaegle2019,Pathak2017} or PAC approaches \cite{Valiant1984}. Curiosity learning is not generally known to converge on rewarding actions with certainty, but never-the-less can be an effective heuristic \cite{Pathak2017,Burda2018,Colas2019}. Within some bounded error, PAC learning is certain to converge \cite{Valiant1984}. For example, it will find the most rewarding arm in a bandit, and do so with a bounded number of samples \cite{Even-Dar2002}. However, the number of samples is fixed and based on the size of the environment (but see \cite{Even-Dar2006,Strehl2009}). So while PAC will give the right answer, eventually, its exploration strategy also guarantees high regret.

% Cisek (2019) traced the evolution of perception, cognition, and action circuits from the Metazoan to the modern age \cite{Cisek2019}. The circuits for reward exploitation and observation-driven exploration appear to have evolved separately, and act competitively--exactly the model we suggest. In particular he notes that exploration circuits in early animals were closely tied to the primary sense organs (i.e. information) and, historically anyway, had no input from the homeostatic circuits needed for reward valuation \cite{Keramati2014,Cisek2019,Juechems2019}. 

\subsection*{Animal behavior}
In psychology and neuroscience, curiosity and reinforcement learning have developed as separate disciplines \cite{Berlyne1950,Kidd2015,Sutton2018}. And they are separate problems, with links to different basic needs: gathering resources to maintain physiological homeostasis \cite{Keramati2014,Juechems2019} and gathering information to plan for the future \cite{Valiant1984,Sutton2018}. Here we suggest that though they are separate problems, they are problems that can, in large part, solve one another.

The theoretical description of exploration in scientific settings is probabilistic \cite{Calhoun2014,Song2019a,Gershman2018b,Schulz2018a}. By definition probabilistic models can't make exact predictions of behavior, only statistical ones. Our approach is deterministic, and so does make exact predictions. Our theory predicts that it should be possible to guide exploration in real-time using, for example, optogenetic methods in neuroscience, or well timed stimulus manipulations in economics or other behavioral sciences. 

% In species ranging from human to \textit{Caenorhabditis elegans}, there are hundreds perhaps thousands of exploration-exploitation experiments. Analysis of their behavior has been generally been limited to aggregate distributions. A deterministic theory can, in principle, open up entirely new avenues for reanalysis--using our model to make exact temporal predictions.

\subsection*{Artificial intelligence}
Progress in reinforcement learning and artificial intelligence research is limited by three factors: data efficiency, exploration efficiency, and transfer learning \cite{Ha2018}. Our algorithm speaks directly to all three of these limits. By treating exploration as a problem in building a world model, our algorithm always ensures high quality exploration. The focus on the world model also means it can be naturally integrated with data efficient model-based reinforcement learning \cite{Sutton2018,Shyam2018}. Finally, as it builds a world model that is free of any task specific bias--the model is ideal for later transfer or fine-tuning \cite{Yosinski2014,Barreto2018}. 

We describe here a simple and optimal algorithm to combine nearly any world model with any reinforcement learning algorithm. This effectively joins the two approaches to reinforcement learning--model-free and model-based--into an advantageous holistic whole where exploration is model-based but exploitation is model-free.

% Data efficiency refers to how many samples or observations it takes to make a set amount of learning progress. The most successful reinforcement learning algorithms are highly data inefficient (e.g., Q-learning \cite{Mnih2015}). To make reinforcement learning data inefficient generally requires that one include a (world) model in the reinforcement algorithm itself. 

% Exploration, by design, learns a world model. As this model improves it can be used directly by the reinforcement learning policy, potentially leading to substantial improvement in data efficiency. The specialist could think of this as a loose generalization of the successor model \cite{Dayan1993,Kulkarni2016a,Ahilan2019}.

% In the large and complex environments that modern machine learning operates in random exploration takes too long to be practical. So there is a critical need for more efficient exploration strategies \cite{Ha2018}, often known as active sampling or directed exploration. A range of heuristics for exploration have been explored \cite{Gottlieb2018,Epshteyn2008,Thrun1992b,Ishii2002a,Bellemare2016,Haarnoja2018}. Dual value algorithms offer a new and principled approach. Designing efficient exploration reduces to two questions: what should our agent remember? How should we measure change change in that memory? Subject to the axioms and Eq.~\ref{eq:pipi}, of course.

% Deep reinforcement learning can match or exceed human performance in games like chess \cite{Silver2018}, Go \cite{Silver2016b}, as well as less structured games like classic Atari video games \cite{Mnih2015}. It would be ideal to transfer performance from one task to another without much retraining \cite{Sutton2018}. But with even minor changes to the environment, most artificial networks often cannot adapt \cite{Zhang2018,Zhang2018a}. This is known as the transfer problem. One thing that limits transfer is that many networks are trained end-to-end, which simultaneously learns a world model and a strategy for maximizing value. Disentangling these can improve transfer. We are certaintly not the first to suggest using a world model for transfer \cite{Barreto2018,Ha2018}. What we do offer is a simple and optimal algorithm to combine nearly any world model with any reinforcement learning scheme.

% \subsection*{Cost}
% It is not fair to talk about benefits without talking about costs. The worst-case run-time of a dual value algorithm is $\max(T_E, T_R)$, where $T_E$ and $T_R$ represent the time to learn to some criterion (see \textit{Results}). In the unique setting where minimizing regret, maximizing data efficiency, exploration efficiency, and transfer do not matter, dual value learning can be a suboptimal choice. 

\subsection*{Acknowledgments.}
 EP and TV wish to thank Jack Burgess, Matt Clapp, Kyle ``Donovank'' Dunovan, Richard Gao, Roberta Klatzky, Jayanth Koushik, Alp Muyesser, Jonathan Rubin, and Rachel Storer for their comments on earlier drafts. EP also wishes to thank Richard Grant for his illustration work in Figure 1.

The research was sponsored by the Air Force Research Laboratory (AFRL/AFOSR) award FA9550-18-1-0251. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. government. TV was supported by the Pennsylvania Department of Health Formula Award SAP4100062201, and National Science Foundation CAREER Award 1351748.


\bibliography{library}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% --------------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\include{supplement}
\include{appendix} 

\end{document} 
\bibliography{sample}
