\documentclass[9pt,twocolumn,twoside]{pnas-new}

% Use the lineno option to display guide line numbers if required.
% Note that the use of elements such as single-column equations
% may affect the guide line number alignment. 
\templatetype{pnasresearcharticle} 

% Choose template 
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} = Template for a one-column mathematics article
% {pnasinvited} = Template for a PNAS invited submission
\usepackage{amsthm} 
\usepackage{amsmath} 
\usepackage{amssymb} 
\usepackage{multicol} 
\usepackage{array}
\usepackage{hyperref}

\DeclareMathOperator*{\argmax}{argmax} 
\newtheorem{axiom}{Axiom} 
\newtheorem{theorem}{Theorem}

% \title{Curiosity is an optimal solution to the exploration-exploitation dilemma}
\title{A way around the exploration-exploitation dilemma.}

\author[a,b,1]{Erik J Peterson} 
\author[a,b,c,d]{Timothy D Verstynen} 
\affil[a]{Department of Psychology} 
\affil[b]{Center for the Neural Basis of Cognition} 
\affil[c]{Carnegie Mellon Neuroscience Institute}
\affil[d]{Biomedical Engineering, Carnegie Mellon University, Pittsburgh PA}
\leadauthor{Peterson} 
\authordeclaration{The authors have no conflicts of interest to declare.}

% \equalauthors{\textsuperscript{1}A.O.(Author One) and A.T. (Author Two) contributed equally to this work (remove if not applicable).}
\correspondingauthor{\textsuperscript{1}To whom correspondence should be addressed. E-mail: Erik.Exists@gmail.com}

\begin{abstract}
For all animals the decision to explore comes with a risk of getting less. For example, a foraging bee might find less nectar, or hunting hawk less prey. This loss is often formalized as regret. It's been mathematically proven that exploring an uncertain world with a specific goal always has some regret. This is why exploration-exploitation can be a dilemma. Given this proof we wondered if the common advice to ``focus on learning and not the goal'' might have mathematical merit. So we re-imagined exploration in the dilemma as an open ended search for any new information. We then developed a new minimal description of information value, which generalizes existing ideas like curiosity, novelty and information gain. We use this description to model the dilemma as a competition between strategies that maximize reward and information independently. Here we prove this competition has a no regret solution. When we study this solution in simulation -- using classic bandit tasks -- it outperforms standard approaches, especially when rewards are sparse.
\end{abstract}

% SIGNIFICANCE
\significancestatement{
The uncertainty of the unknown can always be recast as an opportunity to learn. But learning doesn't look like enough compensation when, for example, you're a bee foraging for nectar or a hawk hunting for prey. An animal exploring though quickly finds itself with a difficult question. Is it better to keep exploring, or stick with the known? This problem, the exploration-exploitation dilemma, is mathematically intractable. 

When one mathematical problem can’t be solved perfectly, it’s often good to find another related problem that can be. In this work we show that when exploration is re-imagined as an open-ended search to learn as much as possible, it actually becomes possible to solve exploration-exploitation optimally. Perhaps learning is enough, after all.
}
% \dates{This manuscript was compiled on \today}
% \doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}
\begin{document} 
\verticaladjustment{-2pt} 
\maketitle

\thispagestyle{firststyle} \ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}

% TODO: the search for reward depends on having a good world model. This is not made clear. Such limits are not discussed... enough? Fix this during revision!
\section*{Introduction}
Decision making in the natural world often leads to a dilemma. As an example let's imagine a bee foraging in a meadow (Figure~\ref{fig:f1}\textbf{A}). The bee could go to the location of a flower it's been to before to gather nectar. Or the bee go somewhere new, and explore. Exploration comes though with the risk of getting less nectar. Perfectly optimizing away this risk is a mathematically intractable problem; there is no way to explore without enduring some regret \cite{Thrun1992a,Dayan1996,Findling2018,Gershman2018b}, and so the decision can become a dilemma.

Resource gathering is not the only reason animals explore. Many animals, like our bee, explore out of curiosity (Figure~\ref{fig:f1}\textbf{B}). This exploration lets them learn about their environment, developing an often simplified model that helps them in planning actions and making future decisions \cite{Ahilan2019,Poucet1993}. Borrowing from the field of artificial intelligence we refer to these models as \textit{world models} \cite{`Schmidhuber2019,Sutton2018,Schmidhuber1991'}. World models offer a principled explanation for why animals are intrinsically curious \cite{Mehlhorn2015,Gupta2006,Berger-Tal2014,Gottlieb2018,Schwartenbeck2019,Pathak2017}, and prone to explore even when no rewards are present or expected \cite{Hughes1997}. 

Curiosity raises the question of whether animals need to explore looking for specific goals or rewards are all. Perhaps we've misinterpreted their actions, and so misconceived of a fundamental problem in the learning and decision sciences. Here we explore a bold conjecture:

\begin{quote}
`Exploration for reward is never needed. The only exploratory behavior an animal needs is that which builds its world model. '   
\end{quote}

\begin{figure}
	[tbhp] \centering 
	\includegraphics[width=.95\linewidth]{figures/fig1.png} 
	\caption{Two views of exploration and exploitation. \textbf{A}. The classic dilemma: either exploit an action with a known reward (e.g., return to the previous plant) or explore other actions on the chance they will return a better outcome (e.g., find a plant with more flowers). \textbf{B.} Here we offer an alternative view of the dilemma, with two different competitive goals: maximize rewards (e.g., keep returning to known flower locations) \texti{or} build a world model by learning new information (e.g., layout of the environment). Exploration here focused on learning in general, not on reward learning specifically. \textit{Artist credit}: Richard Grant.}
	\label{fig:f1} 
\end{figure}

Our contribution is threefold. We define a new minimal (axiomatic) description for information value, which generalizes existing ideas like curiosity, novelty and information gain. In fact, the axioms let us formally disconnect information theory \cite{Shannon1948} from information value, suggesting we may have uncovered a new universal theory. Next we prove that the computer science method of dynamic programming \cite{Bellmann1954,Sutton2018} provides an optimal way to maximize this kind of information value. Finally, we describe a simple winner-take-all scheduling algorithm that can optimally solve a competition between strategies which independently maximize information value and reward. 

\section*{Results}
Tangible rewards are a conserved resource, but learned information isn't. For example, if a rat shares potato chip with a cage-mate, she must necessarily split up the chip leaving less food for herself. Whereas if student shares the latest result from a scientific paper with a lab-mate, they do not necessarily forget a portion of that result. These differences  make reward and information different concepts, and so considering information as a kind of reward isn't consistent.  

If information value isn't a reward, we need another way to study and value it. To do this we first looked to the field of information theory \cite{Shannon1948}, but the problem of information value is not based in the statistical problem of transmitting symbols, as was Shannon’s goal. It is based on the problem of learning and remembering them.

\subsection*{A minimal model of memory}
World models are memories with some amount of simplification \cite{Schmidhuber1991,Ha2018}. They can range from simple novelty signals \cite{Kakade2002}, to location or state counts \cite{Bellemare2016,Dayan1993}, state and action prediction \cite{Schmidhuber1991,Pathak2017,Friston2016}, flow \cite{Yang2019}, learning progress \cite{Lopes2012}, classic working or episodic memories \cite{Miller1956,Tulving2002}, Bayesian and hierarchical Bayesian models \cite{Park2017,Itti2009,Friston2016,Tenenbaum2006}, latent spaces \cite{Kingma2013} and recurrent neural networks \cite{Ganguli2008,Ha2018,Schmidhuber2015a,Mante2013}. 

We have no mathematical reason to prefer any one kind of world model over any other. So we designed a new minimal definition, designed to overlap with all of them.

We must introduce some intial notation. We assume that time is a continuous value and denote increases in time using the differential quantity $dt$. We can then express changes in $M$ (our world model, defined below) as a gradient, $\nabla M$. We also assume that observations about the environment $s$ are real numbers sampled from a finite state space $s \in S$, whose size is $N$ (denoted $S^N$). Actions are also real numbers $a$, drawn from a finite space $A^K$. Rewards $R_t$ -- when they appear -- are binary $(0,1)$ and are provided \textit{only} by the external environment. 

\textit{Definition 1.} We can now formally define a world model $M$ as a finite set of real numbers, whose maximum size is $L$ ($M^L$). We say that every world model has a pair of functions $f$ and $g$. Learning of $s$ at time $t$ (i.e. $s_t$) by $M$ is done by the invertible encoder function $f$, $M_{t+dt} = f(M_{t}, s_{t})$ and $M_{t} = f^{-1}(M_{t+dt}, s_{t})$. Memories $\hat s_t$ about $s_t$ are recalled by the decoder function $g$, $\hat s_t = g(M_t, s_t)$. 

The invertibility of $f$, denoted as $f^{-1}$, is a mathematical way to ensure that any observations encoded in the world model can also be forgotten. This is both an important aspect of real memory, and a critical point for our mathematical analysis.

The details of $f$ and $g$ define what kind of world model or memory $M$ is. Let's consider some examples. If $f$ adds states $s_t$ to the memory, and $g$ tests whether $s_t$ is in $M$, then $M$ is a model of novelty \cite{Kakade2002}. If $f$ counts states and $g$ returns those counts, then $M$ is a count-based heuristic \cite{Bellemare2016,Dayan1993}. If $f$ follows Bayes rule and $g$ decodes the probability of $s_t$, then $M$ is a Bayesian memory \cite{Itti2009,Friston2016,Tenenbaum2006,Schmidhuber1991,Pathak2017,Friston2016}. If the size of $M$ is much smaller than the size of the state space $S^N$, then $f$ can be seen as learning a latent or compressed representation im $M$ \cite{Kingma2013,Schmidhuber2008,Levi-Aharoni2019,Ganguli2010,Ha2018,Schmidhuber2015a,Mante2013,Park2017}, and $g$ decodes a reconstruction of $s$ ($\hat s_t$) or future states ($\hat s_{t+dt}$).
 
% \subsubsection*{Axioms for information value}
% In all of these examples, the value of any observation made by an agent who is learning a world model depends entirely on what the agent learns by making that observation.
% \begin{axiom}
% 	[Axiom of Memory] The value of $E(s_t)$ depends \emph{only} on how the memory $M$ changes.
% \label{ax:1} \end{axiom}
% \noindent This axiom does two things. It ensures value depends only on the memory, and how it changes. And in particular it ensures value depends only on the most recent change in $M$. That driven by observing $s_t$. Readers familiar with reinforcement learning can see this as a natural way to induce the Markov property on the memory.
% \begin{axiom}
% 	[Axiom of Novelty] The value of $E(s_t)$ is zero only when $\nabla M$ is zero. 
% \label{ax:2} \end{axiom}
% \begin{axiom}
% 	[Axiom of Scholarship] Learning can't lead to negative value, so the value of $E(s_t) \geq 0$.
% \label{ax:3} \end{axiom}
% \begin{axiom}
%     [Axiom of Maximum Difference] The value of $E(s_t)$ should be the maximum value which depends on the total change in $M$.
% \label{ax:4} \end{axiom}

% \begin{axiom}
%     [Axiom of Specificity] If the total change in memory $M$ due to observation $s_t$ is held constant ($\frac{dM}{dt} = h$), the more compact (Eq.~\ref{eq:compactcude}) the change in memory the more valuable the observation. 
% \label{ax:4} \end{axiom}
% \noindent Axiom~\ref{ax:4} adds two critical and intertwined properties. It ensures that if all else is held equal, more specific observations are more valuable that less specific observations \cite{Berlyne1950,Kidd2015}. It also ensures that an observation that leads to a simplifying or parsimonious insight (is equivalent to a compression of the memory, \cite{Schmidhuber2008}) is more valuable than one that changes memory the same total amount but does not lead to compression.

% TODO: this can be violated easily. Visit the full def of PAC learning
% and just use that... skip NP?
% \begin{axiom}
% 	[Axiom of Equilibrium] An observation $s_t$ must be learnable by $M$. $\frac{d^2M(s_t)}{dt^2} \leq 0$. 
% \label{ax:5} \end{axiom}

% TODO: this whole section needs a re-write based on the slides
\subsection*{A minimal description of information value.}
To formalize information value we use two axioms that define a real valued function, $E(s)$, that measures the value of any observation $s_t$ given a world model $M$ and a distance metric $d$.

\begin{axiom}
    [Axiom of Change] The value of information $E(s_t)$ depends only on the total distance $M$ moves by making observation $s_t$.
\label{ax:4} \end{axiom}

\noindent This axiom does three important things. It ensures information value depends only on the world model, that value is a distance in memory, and that value learning has the Markov property \cite{Sutton2018}. Now, let's unpack it.

By distance we mean a function $\delta = d(m,m')$, where $m \in M$ and $m' \in M'$ are discrete memories drawn from two memories $M$ and $M'$. We define $d$ so $d \ge 0$ for all $s \in S$, and let $d = 0$ only if $M = M'$. Our definition of $d$ \textit{does not} require the distance in memories from $M$ to $M'$ be the same as from $M'$ to $M$. Nor for the triangle inequality to hold. For the technically inclined, this definition makes $d$ a pre-metric. 

By total distance we mean the norm $||\Delta||$, where $\Delta = \{\delta_1, \delta_2,...,\delta_L\}$. 

In summary, Let $E \equiv ||\Delta||$.

Different $f$ and $g$ pairs will naturally need different ways to measure distances in $M$. For example, in a novelty world model \cite{Kakade2002} either the hamming or Manhattan distance are applicable and would produce binary distance values, as would a count model \cite{Bellemare2016,Dayan1993}. A latent memory \cite{Schmidhuber1991,Pathak2017} might instead use the euclidean norm of its own error gradient \cite{Pascanu2013}. While a probabilistic or Bayesian memory would likely use the Kullback–Leibler (KL) divergence \cite{Park2017,Friston2016}.
 
\begin{axiom}
	[Axiom of Equilibrium] To be valuable an observation $s_t$ must be learnable by $M$ 
\label{ax:5} 
\end{axiom}

\noindent By learnable we mean two things. First, with every (re)observation of $s$, $M$ should change. Second, the change in $M$ must eventually reach a learned equilibrium. To formalize these we constrain the average gradient of $M$, so $\mathbb{E}\big [\nabla^2 M \big ] \leq 0$. 

Most attempts to value information rest their definition on information theory. Value might rest on the intrinsic complexity of an observation (i.e., its entropy) \cite{Haarnoja2018} or on its similarity to the environment (i.e., mutual information) \cite{Kolchinsky2018}, or on some other salience signal \cite{Tishby2000}. In our analysis, learning alone drives value. This is because learning might happen on a true world model or with a faulty world model, or be about a fictional narrative. The observation might be simple, or complex. From a subjective point of view, which is the right point of view for value, all of these are the same; value depends only on the total knowledge gained.

\subsection*{Exploration as a dynamic programming problem} Dynamic programming is a popular optimization method because it guarantees value is maximized using a simple algorithm that always chooses the largest option. In Theorem~\ref{theorem:opt_sub} (see \textit{Mathematical Appendix}) we prove that our definition of memory has one critical property, optimal substructure, that is needed for an optimal dynamic programming solution \cite{Bellmann1954,Roughgarden2019}. The other two required properties, $E \ge 0$ and the Markov property \cite{Bellmann1954,Roughgarden2019}, are fulfilled by the \textit{Axiom 1}. 
To write down our dynamic programming solution we introduce a little more notation. We let $\pi$ denote an action policy, a function that takes a state $s$ and returns an action $a$. We let $\delta$ denote the transition function, which takes a state-action pair $(s_{t},a_t)$ and returns a new state, $s_{t+dt}$. This function acts as an abstraction for the actual world. For notational consistency with the standard Bellman approach we also redefine $E(s)$ as a \textit{payoff function}, $F(M_{t}, a_t)$ \cite{Bellmann1954}.
 
\begin{equation}
	\begin{split}\label{eq:payout} 
		F(M_{t}, a_t) = E(s)\\
		\text{subject to the constraints} \\
		a_{t} = \pi(s_t) \\
		s_{t+dt} = \delta(s_{t}, a_t),\\
		M_{t+dt} = f(M_{t}, s_{t}) 
	\end{split}
\end{equation}

\noindent The value function for $F$ is,

\begin{equation}\label{eq:V_E} 
	\begin{split}
		V_{\pi_E}(M_0) = \Big [ \max_{a \in A} \sum_{t=0}^{\infty} F(M_t, a_t) \ \Big | \ M, \ d, \ S \ \Big ]. 
	\end{split}
\end{equation}

\noindent And the recursive Bellman solution to learn this value function is,

\begin{equation}\label{eq:bellman_iter} 
	V^*_{\pi_E}(M_{t}) = F(M_{t}, a_{t}) + \max_{a \in A} \Big [ F(M_{t+dt}, a_t) \Big ].
\end{equation}

For the full derivation of Eq~\ref{eq:bellman_iter} see the \textit{Mathematical Appendix}, where we also prove that Eq~\ref{eq:bellman_iter} leads to exhaustive exploration of any finite space $S$ (Theorems~\ref{theorem:Z} and~\ref{theorem:convergence}).

% Axiom~\ref{ax:5} requires that learning in $M$ converge. Axiom~\ref{ax:4} requires information value increases with surprise, re-scaled by specificity. When combined with a greedy action policy like $\pi^*_E$, these axioms naturally lead to active learning \cite{Shyam2018,Pathak2019,Schwartenbeck2019} and to adversarial curiosity \cite{Schmidhuber2019a}.

\subsection*{Scheduling a way around the dilemma} Remember that the goal of reinforcement learning is to maximize reward, an objective approximated by the value function $V_R(s)$ and an action policy $\pi_R$. 

\begin{equation}\label{eq:V_R} 
	V^{\pi_R}_R(s) = \mathbb{E} \Big [ \sum_{k=0}^{\infty} R_{t+k+1} \big | s = s_t \Big ]
\end{equation}

Remember too that our overall goal is to find an algorithm that maximizes both information and reward value. To do that we imagine the policies for exploration and exploitation are possible ``jobs'' competing to control behavior. We know that, by definition, each of these jobs produces non-negative values: $E$ for information or $R$ for reinforcement learning. So our goal is to find an optimal scheduler for these two jobs. 

To do this we further simplify our assumptions. We assume each action takes a constant amount of time, and has no energetic cost. We assume the policy can only take one action at a time, and that those actions are exclusive. Most scheduling solutions also assume that the value of a job is fixed, while in our problem information value changes as the world model improves. In a general setting however, where one has no prior information about the environment, the best predictor of the next value is the last or most recent value \cite{Hocker2019,Roughgarden2019}. We assume this precept holds in all of our analysis.

With these assumptions in place, the optimal solution to this kind of scheduling problem is known to be a purely local, winner-take-all, algorithm \cite{Bellmann1954,Roughgarden2019}. We state this winner-take-all solution here as a set of inequalities where $R_t$ and $E_t$ represent the value of reward and information at the last time-point.

\begin{equation}
\label{eq:pipi} 
	\begin{split}
		\pi_{\pi}(s_t) = 
		\begin{cases}
			\pi^*_E(s_t) & : E_t - \eta > R_t \\
			\pi_R(s_t) & : E_t - \eta \le R_t \\
		\end{cases}
		\\
		\text{subject to the constraints}\\
		p(\mathbb E[R]) < 1 \\
		E - \eta \geq 0
	\end{split}
\end{equation}

To ensure that the default policy is reward maximization, Eq.~\ref{eq:pipi} breaks ties between $R_t$ and $E_t$ in favor of $\pi_R$. In stochastic environments, $M$ can show small continual fluctuations. To allow Eq.~\ref{eq:pipi} to achieve a stable solution we introduce $\eta$, a boredom threshold for exploration. Larger values of $\eta$ devalue information exploration and favor exploitation of reward.

 The worst case algorithmic run time for Eq~\ref{eq:pipi} is linear and additive in its policies. So if in isolation it takes $T_E$ steps to earn $E_{T} = \sum_{T_E} E$, and $T_R$ steps to earn $r_{T} = \sum_{T_R} R$, then the worst case training time for $\pi_{\pi}$ is $T_E + T_R$. It is worth noting that this is only true if neither policy can learn from the other's actions. There is, however, no reason that each policy cannot observe the transitions $(s_t, a_t, R, s_{t+dt})$ caused by the other. If this is allowed, worst case training time improves to $\max(T_E, T_R)$.

\subsection*{Exploration without regret} Suboptimal exploration strategies will lead to a loss of potential rewards by wasting time on actions that have a lower expected value. Regret $G$ measures the value loss caused by such exploration. $G = \hat V - V_a$, where $\hat V$ represents the maximum value and $V_a$ represents the value found by taking an exploratory action rather than an exploitative one \cite{Sutton2018}. 

Optimal strategies for a solution to the exploration-exploitation dilemma should maximize total value with zero total regret. 

\begin{figure}
	[tbhp] \centering 
	\includegraphics[width=.95\linewidth]{figures/fig2.png} 
	\caption{ \label{fig:f2} Bandits. Reward probabilities for each arm in bandit tasks I-IV. Grey dots highlight the optimal (i.e., highest reward probability) arm. See main text for a complete description.} 
\end{figure}

\begin{figure}
	[tbhp] \centering 
	\includegraphics[width=.75\linewidth]{figures/fig3.png} 
	\caption{ \label{fig:f3} Regret and total accumulated reward across models and bandit task. Median total regret (left column) and median total reward (right column) for simulations of each model type ($N=100$ experiments per model). See main text and Table~\ref{tab:agents} for description of each model. Error bars in all plots represent median absolute deviation.} 
\end{figure}

To evaluate dual value learning (Eq.~\ref{eq:pipi}) we compared total reward and regret across a range of both simple, and challenging multi-armed bandit tasks. Despite its apparent simplicity, the essential aspects of the exploration-exploitation dilemma exist in the multi-armed bandit task \cite{Sutton2018}. Here the problem to be learned is the distribution of reward probabilities across arms (Figure ~\ref{fig:f2}).  To estimate the value of any observation $s_t$, we compare sequential changes in this probabilistic memory, $M_{t+dt}$ and $M_t$ using the KL divergence (i.e. relative entropy; Figure \ref{fig:supf1}\textbf{A}-\textbf{B}). The KL divergence is a standard way to measure the distance between two distributions \cite{MacKay2003} and is, by design, consistent with our axioms (see the \textit{Supplementary Materials} for a more thorough discussion). 

We start with a simple experiment involving a single high value arm. The rest of the arms have a uniform reward probability (Bandit \textbf{I}). This represents a trivial problem. Next we tried a basic exploration test (Bandit \textbf{II}), with one winning arm and one distractor arm whose value is close to but less than the optimal choice. We then move on to a more difficult sparse exploration problem (Bandit \textbf{III}), where the world has a single winning arm, but the overall probability of receiving any reward is very low ($p(R) = 0.02$ for the winning arm, $p(R) = 0.01$ for all others). Sparse reward problems are notoriously difficult to solve, and are a common feature of both the real world and artificial environments like Go, chess, and class Atari video games \cite{Mniha,Silver2016b,Silver2018}. Finally, we tested a complex, large world exploration problem (Bandit (\textbf{IV}) with 121 arms, and a complex, randomly generated reward structure. Bandits of this type and size are near the limit of human performance \cite{Wu2018}. 

We compared the reward and regret performance of 6 artificial agents. All agents used the same temporal difference learning algorithm (TD(0), \cite{Sutton2018}); see \textit{Supplementary materials}). The only difference between the agents was their exploration mechanism (Table~\ref{tab:agents}).  The e-greedy algorithm is a classic exploration mechanism \cite{Sutton2018}. Its annealed variant is common in state-of-the-art reinforcement learning papers, like Mnih \emph{et al} (\cite{Mniha}). Other state-of-the-art exploration methods are models that treat Bayesian information gain as an intrinsic reward and the goal of all exploration is to maximize total reward (extrinsic plus intrinsic) \cite{Jaegle2019,Schmidhuber1991}. To provide a lower bound benchmark of performance we included an agent with a purely random exploration policy.

\newcolumntype{L}{>{\arraybackslash}m{4cm}} 
\begin{table}[] 
    \centering 
	\caption{Artificial agents.} \label{tab:agents} 
	\begin{tabular}
		{|l|L|} \hline \textbf{Agent} & \textbf{Exploration mechanism} \\
		\hline Dual value & Our algorithm (Eq~\ref{eq:pipi}). \\
		\hline E-greedy & With probability $1-\epsilon$ follow a greedy policy. With probability $\epsilon$ follow a random policy. \\
		\hline Annealed e-greedy & Identical to E-greedy, but $\epsilon$ is decayed at fixed rate. \\
		\hline Bayesian reward & Use the KL divergence as a weighted intrinsic reward, sampling actions by a soft-max policy. $\sum_T R_t + \beta E_t$ \\
		\hline Random & Action are selected with a random policy (no learning) \\
		\hline 
	\end{tabular}
\end{table}

All of the classic and state-of-the-art algorithms performed well at the different tasks in terms of accumulation of rewards (right column, Figure \ref{fig:f3}). The one exception to this being the sparse low reward probability condition (Bandit \textbf{III}), where the dual value algorithm consistently returned more rewards than the other models. In contrast, most of the traditional models still had substantial amounts of regret in most of the tasks, with the exception of the annealed variant of the e-greedy algorithm during the sparse, low reward probability task (left column, Figure~\ref{fig:f3}). In contrast, the dual value learning algorithm consistently was able to maximize total reward with zero or near zero (Bandit \textbf{III}) regret, as would be expected by an optimal exploration policy.


\section*{Discussion}
\subsection*{Past work}
We are certainly not the first to quantify information value \cite{Kolchinsky2018,CogliatiDezza2017}, or use that value to optimize reward learning \cite{Kelly1956,Schmidhuber1991,Dayan1996,deAbril2018,Itti2009}. Information value though is typically framed as a means to maximize the amount of tangible rewards (e.g., food, water, money) accrued over time \cite{Sutton2018}. This means that information is treated as an analog of these tangible or external rewards (i.e., an \textit{intrinsic reward}) \cite{Schmidhuber1991,Berger-Tal2014,Itti2009,Friston2016}. This approximation does drive exploration in a practical and useful way, but doesn't change the intractability of the dilemma \cite{Thrun1992a,Dayan1996,Findling2018,Gershman2018b}. 

% Many accounts of information value rely on both Bayesian reasoning, and information theory \cite{Kelly1956,Itti2009,Friston2016}. In a formal, descriptive, or mathematical world, using these makes sense. However for a naturalistic theory of learning these are strong assumptions, that may not hold up. For example, if it turns out that animals are not in fact general Bayesian reasoning systems. 

At the other extreme from reinforcement learning are pure exploration methods, like curiosity \cite{Berlyne1950,Jaegle2019,Pathak2017} or PAC approaches \cite{Valiant1984}. Curiosity learning is not generally known to converge on rewarding actions with certainty, but never-the-less can be an effective heuristic \cite{Pathak2017,Burda2018,Colas2019}. Within some bounded error, PAC learning is certain to converge \cite{Valiant1984}. For example, it will find the most rewarding arm in a bandit, and do so with a bounded number of samples \cite{Even-Dar2002}. However, the number of samples is fixed and based on the size of the environment (but see \cite{Even-Dar2006,Strehl2009}). So while PAC will give the right answer, eventually, its exploration strategy also guarantees high regret.

% Cisek (2019) traced the evolution of perception, cognition, and action circuits from the Metazoan to the modern age \cite{Cisek2019}. The circuits for reward exploitation and observation-driven exploration appear to have evolved separately, and act competitively--exactly the model we suggest. In particular he notes that exploration circuits in early animals were closely tied to the primary sense organs (i.e. information) and, historically anyway, had no input from the homeostatic circuits needed for reward valuation \cite{Keramati2014,Cisek2019,Juechems2019}. 


\subsection*{Animal behavior}
In psychology and neuroscience, curiosity and reinforcement learning have developed as separate disciplines \cite{Berlyne1950,Kidd2015,Sutton2018}. And they are separate problems, with links to different basic needs: gathering resources to maintain physiological homeostasis \cite{Keramati2014,Juechems2019} and gathering information to plan for the future \cite{Valiant1984,Sutton2018}. Here we suggest that though they are separate problems, they are problems that can, in large part, solve one another.

The theoretical description of exploration in scientific settings is probabilistic \cite{Calhoun2014,Song2019a,Gershman2018b,Schulz2018a}. By definition probabilistic models can't make exact predictions of behavior, only statistical ones. Our approach is deterministic, and so does make exact predictions. Our theory predicts that it should be possible to guide exploration in real-time using, for example, optogenetic methods in neuroscience, or well timed stimulus manipulations in economics or other behavioral sciences. 

In species ranging from human to \textit{Caenorhabditis elegans}, there are hundreds perhaps thousands of exploration-exploitation experiments. Analysis of their behavior has been generally been limited to aggregate distributions. A deterministic theory can, in principle, open up entirely new avenues for reanalysis--using our model to make exact temporal predictions.

\subsection*{Exploration in the brain}
% TODO - some animals have exploration modules which seem to be orthoonal from reward.

\subsection*{Artificial intelligence}
Progress in reinforcement learning and artificial intelligence research is limited by three factors: data efficiency, exploration efficiency, and transfer learning \cite{Ha2018}. Our algorithm speaks directly to all three of these limits. 

In the large and complex environments that modern machine learning operates in random exploration takes too long to be practical. So there is a critical need for more efficient exploration strategies \cite{Ha2018}. These are often known as active sampling, or directed exploration. A range of heuristics for this kind of exploration have been explored \cite{Gottlieb2018,Epshteyn2008,Thrun1992b,Ishii2002a,Bellemare2016,Haarnoja2018}. But our dual value algorithm offers a principled approach. Designing efficient exploration reduces to two questions: what should our agent remember? How should we measure change change in that memory? This is subject to the axioms and Eq.~\ref{eq:pipi}, of course.

\subsection*{In the classroom}


\subsection*{Everyday life} The uncertainty of the unknown can always be recast as an opportunity to learn. But rather than being a trick of psychology, we prove this view is--in our formalism--mathematically optimal.


\subsection*{Acknowledgments.}
EP and TV wish to thank Jack Burgess, Matt Clapp, Kyle ``Donovank'' Dunovan, Richard Gao, Roberta Klatzky, Jayanth Koushik, Alp Muyesser, Jonathan Rubin, and Rachel Storer for their comments on earlier drafts. EP also wishes to thank Richard Grant for his illustration work in Figure 1.

The research was sponsored by the Air Force Research Laboratory (AFRL/AFOSR) award FA9550-18-1-0251. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. government. TV was supported by the Pennsylvania Department of Health Formula Award SAP4100062201, and National Science Foundation CAREER Award 1351748.

\bibliography{library}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% --------------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\include{supplement}
\include{appendix} 

\end{document} 
\bibliography{sample}
