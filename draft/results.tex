\section*{Results} 
\subsection*{Theory}
For our theoretical work, we first introduce reinforcement learning, and its basis in the Bellman equation. Next we develop a new mathematical view of information value, curiosity, and a new understanding of ideal curious search. We then use these results to argue exploration-for-curiosity leads to optimal value solutions for all explore-exploit problems, with deterministic dynamics.


\subsubsection*{The reward collection problem}
Here we consider an animal who interacts with an environment over a sequence of states, actions and rewards. The animal's goal is to select actions in a way that maximizes the total reward collected. How should an animal do this? 

When the environment is unknown, to maximize reward an animal must trade-off between exploration, and exploitation. The standard way of approaching this tradeoff is to assume, 

\begin{quote}
The agent must try a variety of actions and progressively favor those that appear to be best'' \cite{Sutton2018}. 
\end{quote}

A conventent way to study reward collection is with using Markov decisions.
Our Markov decision process (MDP) for reward collection consists of states of real valued vectors $\mathbf{S}$ from a finite set $\mathcal{S}$ of size $n$, as are actions $\mathbf{A}$ from a finite set $\mathcal{A}$ of size $k$. Rewards are non-negative real numbers, $R$. Transitions from state to state are handled by the transition function $\Lambda(\mathbf{S},\mathbf{A})$. In our mathematics we assume $\Lambda$ is a deterministic function of its inputs. In our simulated experiments we assume it is a stochastic function.

Markovian preliminaries in place, we can write down the standard value function definition, and the standard Bellman equation \cite{Bellmann1954} for reward collection \cite{Sutton2018}. For simplicity we assume below that the horizon is finite, bounded by $T$.

\begin{equation} 
	\label{eq:bellman_seq}
    \begin{split}
        V^*_{R}(\mathbf{S}) &= \argmax_{\mathbf{A} \in \mathbb{A}} \Big [\sum_{k=0}^{T}  R \ \Big ]\\
                         	&= \argmax_{\mathbf{A} \in \mathbb{A}} \Big [ R_{t} + \sum_{k=0}^{T} R_{t+1} \ | \ \mathbf{S_t} = \mathbf{S},\ \mathbf{A_t} = \mathbf{A} \Big ]\\
							&= \argmax_{\mathbf{A} \in \mathbb{A}} \Big [ R_{t} + V^*_{R}(\mathbf{S_{t+1}}) \ | \ \mathbf{S_t} = \mathbf{S},\ \mathbf{A_t} = \mathbf{A} \Big ]\\
                         	&= R_0 + V^*_{R}(\mathbf{S_{t}}) + V^*_{R}(\mathbf{S_{t+1}}),\ \ldots
    \end{split}
\end{equation}

The second to last equation is the optimal Bellman equation for $V^*_R$. This can be restated more compactly as,

\begin{equation} 
\label{eq:bellman_iter}
V^*_R(\mathbf{S}) = \argmax_{\mathbf{A} \in \mathbb{A}} \Big [ R_{t}  + V^*_{R}(\Lambda(\mathbf{S},\mathbf{A})) \Big ]
\end{equation}

Having this last equation means a central challenge in reinforcement learning is efficiently estimating $V^*_{R}(.)$. This includes finding good approximate learning rules. For example, temporal difference learning or Q learning \cite{Sutton2018}. It also includes finding good exploration methods, which is of course the problem we take up here. In our eventual union, we assume the learning rule can be any nearly reinforcement learning algorithm. 


\subsubsection*{Formal regret}
Regret minimization is a standard metric of optimality in reinforcement learning \cite{Sutton2018}. Formally, given \emph{any} any optimal value solution we can define regret $G_t$ in terms of this, given by $G = V^* - V_t$, where $V_t$ is the value of the chosen action $\mathbf{A}_t$. An optimal value algorithm can then be restated, as a ``best'' series of best actions ($\mathbf{A}_t$, $\mathbf{A}_{t+1}$, $\mathbf{A}_{t+2}$, ...) for which $\sum_{k=0}^{T} G_t = 0$.

\subsubsection*{The information collection problem}
Imagine we wish to arrive at an equation to maximize curious behavior, in terms of information value. Assume, as a stand-in, $\hat E$ represents the information value of some observation. This hypothetical equation we want could be written to match Eq.~\ref{eq:bellman_iter}, as. 

\begin{equation} 
	\label{eq:bellman_iter_E}
	V^*_{\hat E}(\mathbf{S}) = \argmax_{\mathbf{A} \in \mathbb{A}} \Big [ \hat E_{t}  + V^*_{\hat E}(\Lambda(\mathbf{S},\mathbf{A})) \Big ]
\end{equation}

We add a ``hat'' to this term $E$ to indicate it is a subject value. In constrast $R$, the external reward define about, has no ``hat''. Until, that is, we dicuss homeostatic reward value $\hat R$ later on. 

So how can we arrive at Eq.~\ref{eq:bellman_iter}? How can we ensure $\hat E$ is general, and therefore suitable for use across species? How can we do so when we choose to limit ourselves to non-Markovian kinds of learning and memory? 


\emph{Observations.} Animals at different stages in development, and in different environments, express curious behavior about the environment, their own actions, and their own mental processes. We assume a parsimonious way to handle this is be assuming curious animals often ``bind'' \cite{Robertson2003} the elements of environment, the Markov process ($\mathbf{S},\mathbf{A},\mathbf{S'},R$), into single observations $\mathbf{X}$. For convenience, we assume observations are embedded in a finite real space, $\mathbf{X} \in \mathcal{X}$ of size $l$. This binding is imagined to happen via a communication channel, $T$, identical to our observation function, $T(\mathbf{S},\mathbf{A},\mathbf{S'},R,\mathbf{M})$. ($\mathbf{M}$ is the animal's memory, that we define below.)

Observations made by $T$ can be internally generated from memory $\mathbf{M}$, or externally generated from the environment ($\mathbf{S}$,$\mathbf{S'}$). Or both can be used in combination, as happens in recurrent neural circuits. Observations may contain action information $\mathbf{A}$, but this is optional. Observations may contain reward information $R$, but this optional. Though omitting reward would hamper solving explore-exploit problems. 

\emph{Memory.} Sustained firing, the strength between two synapses, elevated Calcium concentration are three simple examples of learning and memory in the nervous system. Each of these examples though can be represented into a vector space. In fact, most any memory system can be so defined. 

We define memory as a set of real valued numbers, embedded in a finite space that is closed and bounded with $p$ dimensions. A learning function is then any function $f$ that maps observations $\mathbf{X}$ into memory $\mathbf{M}$. This $f$ is considered to be a valid learning function as long as the mapping changes $\mathbf{M}$ some small amount. A non-constant mapping, in other words. Using recursive notation, this kind of learning is denoted by $\mathbf{M} \leftarrow f(\mathbf{X},\mathbf{M}) $. Though we will sometimes use $\mathbf{M'}$ to denote the updated memory in place of the recursion. Other times we will add subscripts, like ($\mathbf{M}_t,\mathbf{M}_{t+1},\ldots$), to express the path of a memory. We can also define a forgetting function that \textit{can be any function} that inverts the memory, $f^{-1}(\mathbf{X};\ \mathbf{M}') \rightarrow \mathbf{M}$. 

We do not assume memory $\mathbf{M}$, or the learning function $f$ are Markovian.

For individual animals we assume $f$ and $f^{-1}$ have been chosen by evolution and experience to be learnable, efficient, and have sufficiently useful inductive bias for their particular environment \cite{Valiant1984a,Thrun1992a}. 


\subsubsection*{Axiomatic information value} 
To make our eventual union proper, we first need to separate reward value from information value in a principled and general way. We do this axiomatically.
We reason instead that the value of any observation made by an animal depends entirely on what an animal learns by making that observation–-literally how it changes that animal's memory. This lets us sidestep a problem present in most working models of curiosity. They tend to conflate the learning rule, with curiosity itself \cite{Schmidhuber1991b,Oudeyer2018a,Burda2018,Zhang2013,deAbril2018,Zhou2020,Schwartenbeck2019,Wilson2014a,Lehman2011,Velez2014}.

Our definition of information value closely follows the motivations of Shannon, and his definition of a communication channel. If we have a memory $\mathbf{M}$ which has been learned by $f$ over a history of observations, $(\mathbf{X_0},\mathbf{X_1},...)$, can we measure how much value the next observation $\mathbf{X_t}$ should have? 

We think this, our hypothetical measure $\hat E$, should have certain intuitive properties:

\begin{axiom}[Axiom of Memory]
	$\hat E$ depends only on the difference $\delta \mathbf{M}$ between $\mathbf{M}$ and $\mathbf{M'}$.
\end{axiom}

That is, the value of an observation $\mathbf{X}$ depends only on how
the memory changes, by $f$. 

\begin{axiom}[Axiom of Novelty]
	$\hat E = 0$ if and only if $\delta M = 0$. 
\end{axiom}

That is, an observation that doesn’t change the memory has no value.

\begin{axiom}[Axiom of Scholarship]
	$\hat E \ge 0$.
\end{axiom}

That is, all (new) information is in principle valuable even if its consequences are later found to be negative.

\begin{axiom}[Axiom of Completeness]
	$\hat E$ should increase monotonically with the total change in memory. 
\end{axiom}

That is, there should be a one-to-one relationship between how memory changes and how information is valued.

\begin{axiom}[Axiom of Equilibrium]
	For the same observation $\hat E$ should approach 0 in finite time.
\end{axiom}

That is, learning on $\mathbf{M}$ makes continual progress toward equilibrium (i.e. self-consistency) with each observation, after a finite time period has passed. After this it will approach a steady-state value of $\delta \mathbf{M} \rightarrow 0$ and $\hat E \rightarrow 0$. We use the term consistency to denote a memory at or near equilibrium. We mean that the memory has become consitent it itseslf in time, and this all our axioms can promise. 

All we have really done in these axioms is remove any mention of a learning rule, its properties, or learning target, its properties from previous definitions \cite{Itti2009,Jaegle2019,Schmidhuber1991,Inglis2001,Reddy2016,Pirolli2007}. This is important not for its own sake but because with these axioms we can consider information value, and therefore curiosity, independent of any learning semantics, or any semantics at all. 

Put another way, if information should be agnostic to semantics \cite{Shannon1948a}, we reason information value should be agnostic as well.


\subsubsection*{Practical information value}
Meeting our requirements is not difficult. The practical example of axiomatic value we use throughout this paper is based on the geometric norm, $||.||$ (as shown in Fig.~\ref{fig:cartoon}). Though not necessarily a unique solution, a norm on the memory gradient is a simple, computational convenience, way to satisfy Axioms 1-4 (Eq \ref{eq:E_norm}). 

\begin{equation}
	\label{eq:E_norm}
	\hat E = || \ \nabla \mathbf{M} \ ||
\end{equation}

To satisfy Axiom 5 we further require memory dynamics eventually decelerate,  $\nabla^2 \mathbf{M} < 0$ for all $ t \ge T^*$. Informally, the idea here is that any notion of sensible and useful learning must converge. We take this to mean that in finite time bound, $T^*$, $\hat E$ must approach 0 (as shown in Fig.~\ref{fig:cartoon}). We term this consistent learning.

In practice here we will not work with the gradient, but will use a discrete time map in its place,

\begin{equation}
	\label{eq:E_norm_discrete}
	\hat E \approx || \ f(\mathbf{M},\mathbf{X}) - \mathbf{M} \ ||
\end{equation}

\begin{figure}
	\includegraphics[width=1\linewidth]{img/cartoon.pdf} 
	\caption{Practical (geometric) information value, and our constraints on its dynamics. 
	\textbf{a}. This panel illustrates a two dimensional memory. The information value of two observations $\mathbf{X}_1$ and $\mathbf{X}_2$ depends on the norm of memory with learning. Here we show this distance as a euclidean norm, denoted as a black arrow.
	\textbf{b-c} This panel illustrates learning dynamics with time (over a series of observations that are not shown). If information value becomes decelerating in finite time bound, then we say that learning is consistent with Def. 2. This is shown in panel b. The bound is depicted as a dotted line. If learning does not decelerate, then it is said to be inconsistent (Panel c). \textit{It is important to note:} our account of information value and curiosity does not work when learning is inconsistent.
  	}
	\label{fig:cartoon} 
\end{figure}

The common way to arrive at Bellman solution is to assume a Markov decision space. This is a problem. For our definition of memory we assume long-term path dependence. This breaks the Markov assumption, and its needed claim of optimal substructure. To get around this we prove instead that exact forgetting, of the last observation, is another way to derive a Bellman solution.

In the theorem below which shows the optimal substructure of $\mathbf{M}$ we implicitly assume that $\mathbf{X} = \mathbf{S}$, $\mathbf{A}$, $f$, and $\Lambda$ are given, and that $\Lambda$ is deterministic, 

\begin{theorem}[Optimal substructure] \label{theorem:opt_sub} 
   If $V^*_{\pi_{\hat E}}$ is the optimal information value given by policy $\pi_{\hat E}$, a memory $\mathbf{M}_t$ has optimal substructure if the last observation $X$ can be removed from $\mathbf{M}$, by $\mathbf{M-1}_{t} = f^{-1}(\mathbf{X}, \mathbf{M}_t)$ such that the resulting value $V^*_{t-1} = V^*_{t} - E_{t}$ is also optimal. 
\end{theorem}

Given an arbitrary starting value $E_0 > 0$, the best solution to maximizing $\hat E$ is can be given by a Bellman equation (Eq.~\ref{eq:bellman_iter_E}).

\begin{equation} 
	\label{eq:bellman_iter_E}
	V^*_{\hat E}(\mathbf{S}) = \argmax_{\mathbf{A} \in \mathbb{A}} \Big [ \hat E_{t}  + V^*_{\hat E}(\Lambda(\mathbf{S},\mathbf{A})) \Big ]
\end{equation}

As long as learning about the different observations $\mathbf{X}$ is independent, and as long as learning continues until steady-state, defined as $E_t < \eta$, there are many equally good solutions to the information collection problem above. Under these conditions we can therefore simplify Eq. \ref{eq:bellman_iter_E} further, giving Eq. \ref{eq:EE}. 

\begin{equation}
	\label{eq:EE} 
	V_E^{*}(\mathbf{X}) = \argmax_{\mathbf{A}} \Big [ E_t + E_{t+1} \Big ]
\end{equation}

\subsubsection*{A hypothesis about boredom}
The central failure mode of curiosity is what we will call minutia, defined as learning that has little to no use to the agent. A good example of this is to, ``Imagine a scenario where the agent is observing the movement of tree leaves in a breeze. Since it is inherently hard to model breeze, it is even harder to predict the location of each leaf'' \cite{Pathak2017}. This will imply that a curious agent will always remain curious about chaos in the leaves even though they have no hope of successful learning of them.

To limit curiosity we introduction a penalty term, $\eta$ which we treat as synonymous with boredom. We consider boredom an adaptive trait, in other words. Others have considered boredom arguing it is a useful way to motivate aversive tasks \cite{Bench2013}, or curiosity \cite{Loewenstein1994}. We take a slightly different view. We treat boredom as a tunable free parameter, $\eta \ge 0$ and a means to ignore marginal value by requiring curious exploration to stop once $E \le \eta$. 


\subsubsection*{Optimal exploration}
Having a policy $\pi_E^*$ that optimally maximizes $\hat E$ does not necessarily ensure that exploration is of good quality. We think it is reasonable to call an exploration good if it meets the criteria below. 

\begin{enumerate}
	\item Exploration should visit all available states of the environment at least once.
	\item Exploration should cease when learning has plateaued.
	\item Exploration should take as few steps as possible to achieve 1 and 2.
\end{enumerate}

In Theorems~\ref{theorem:Z} and~\ref{theorem:convergence} we prove that our Bellman solution $\pi_E$ satisfies these, when $\eta > 0$ and when the observations $\mathbf{X}$ contain complete state information, $\mathbf{S} \in \mathbf{X}$. See the Appendix for the proofs proper.

\begin{theorem}[Complete exploration] 
	\label{theorem:Z} 
	Given some arbitrary value $E_0$, an exploration policy governed by $\pi^*_{\hat E}$ will visit all states $\mathbf{S} \in \mathbb{S}$ in a finite number of steps $T$.
\end{theorem}

\begin{theorem}[Efficient exploration] 
	\label{theorem:convergence} 
	An exploration policy governed by $\pi^*_{\hat E}$ will only revisit states for where information value is marginally useful, defined by $\hat E > \eta$.  
\end{theorem}


\subsubsection*{A hypothesis of equal importance}
 Searching in an open-ended way for information must be less efficient than a direct search for reward? In answer to this question, we make two conjectures.

\begin{conjecture}[A hypothesis of equal importance]
	Reward value and information value are equally important for survival, in changing environments 
\end{conjecture}

It's worth reiterating here, that curiosity is a primary drive in most, if not all, animals \cite{Inglis2001}. It is as strong, if not sometimes stronger, than the drive for reward \cite{Loewenstein1994,Kidd2015,Gottlieb2018}.

If both reward and information are equally important, then time is not wasted in a curious search, and so the process is not \textit{necessarily} inefficient or even indirect. Instead, we offer a new question. Is curiosity practical enough to serve as a useful ``trick'' to solve reward collection search problems? We answer this with a second conjecture,

\begin{conjecture}[The curiosity trick]
	When learning is possible, curiosity is a sufficient solution for all exploration problems.
\end{conjecture}

It's worth noting here that curiosity, as an algorithm in machine learning, is highly effective at solving broad kinds of optimization problems \cite{Schmidhuber1991,Stanton2018,Lehman2010,Mouret2011,Fister2019,Mouret2015,Colas2020,Cully2015,Pathak2017,Laversanne-Finot2018}. 

\subsubsection*{The problem of mixing reward and information collection}
If reward and information are equally important, and curiosity is sufficient, how can an animal go about balancing them? If you accept our conjectures, then answering this question optimally is the same as solving the dilemma. To solve this dual value learning problem we’ve posed–-information and reward maximization–-we need an algorithm that can maximize the value of a mixed sequence, $V_{\hat{E}R}$. For example,

\begin{equation}
	\label{eq:bellman_seq_ER}
	V^*_{\hat{E}R}(\mathbf{S}) = \argmax_{\mathbf{A} \in \mathbb{A}} \Big [\sum_{k=0}^{T} \hat{E}_t + \hat{E}_{t+1} + R_{t+2} + \hat{E}_{t+3} + R_{t+4} + \ldots  \ \Big ]
\end{equation}

\subsubsection*{A win-stay, lose-switch solution}
A similar dual value optimization problem to Eq.~\ref{eq:bellman_seq_ER} was posed, solved, with its regret bounded, by Robbins \cite{Robbins1952} in his derivation of the win-stay lose-switch rule. This WSLS approach has since proven useful in decision making and reinforcement learning \cite{Estes1994TowardAS,Worthy2014}, Bayesian sampling \cite{Bonawitz2014}, and especially in game theory \cite{Nowak1993}. We also put it to use here. 

We have come to think about our two optimal value policies $\pi_R$ and $\pi_{\hat E}$ as two ``players'' in game for behavioral control of an animal's moment-by-moment actions \cite{Estes1994TowardAS}. The simplest solution to this game, in terms of computational complexity, is a WSLS rule. We denote this rule as $\Pi_\pi$, and define it here by the inequalities below. 

\begin{equation} 
    \label{eq:pipi}
    \begin{split}
        \Pi_{\pi} = 
        \begin{cases}
            \pi^*_{\hat{E}} & : \hat{E} - \eta > R + \rho \\
            \pi_R 	& : \hat{E} - \eta < R + \rho \\
        \end{cases}
    \end{split}
\end{equation}

We assume no matter which policy is in control in Eq.~\ref{eq:pipi}, each policy can learn from (be updated using) the other policies actions, and observations. That is, while the two policies compete for control, they learn from each other in a cooperative fashion.

Here $\eta$ is the boredom term we defined earlier. We also introduce the reward bias term $\rho$, where $\rho > \eta$. This bias ensures no ties can occur, and that as $\hat E$ decreases to 0, as it must, the default policy is reward collection. That is, by $\pi_R$. In this paper we assume $\rho$ is set to be just a small bit larger than $\eta$. It's role is then to break ties, and little else. Larger values of $\rho$ could be used to bias reward exploitation in ``irrational'' ways.

In Eq.~\ref{eq:pipi} we compare reward $R$ and information value $\hat E$ but have not made sure they have the same ``units''. We sidestep this issue, and at the same time ensure complete exploration, as well as convergence, by limiting the expected probability of having zero reward to be, itself, non-zero. That is, $\mathbb{E}[p(R_t=0) > 0]$. For the same reasons, we also limit $E_0$, the first set of values, to be positive and non-zero, $(E_0 - \eta) > 0$. 

To provide some intuition for Eq.\ref{eq:pipi}, imagine that in the previous action an animal observed, arbitrarily, 0.7 units of information value, $\hat E$, and no reward (i.e. $R = 0$). Using our rule on the next action the animal should then choose its exploration policy $\pi^*_{\hat{E}}$. If it explored last time, this is a ``stay'' action. If not, it is a ``switch''. Let's then say that with this next observation, $\hat E$ decreases to 0.2 and a reward value of 1.0 is observed. Now the animals should switch its strategy for the next round, to exploit instead. 

In general, if there was more reward value then information value last round ($R > \hat E$), our rule dictates an animal should choose the reward policy. If information value dominated, $R < \hat E$, then it should choose the information gathering policy.

The WSLS rule in Eq~\ref{eq:pipi} when applied to exploration-exploitation problems can guarantee three things, shown below. 

\begin{theorem}[No regret - reward value]
	\label{th:no_regret_R}
	When $\pi_R$ is in control under $\Pi_{\pi}$, all actions are zero regret in terms of $V_R$. That is, $\sum_{k=0}^{T} G = 0$.
\end{theorem}

\begin{theorem}[No regret - information value]
	\label{th:no_regret_E}
	When $\pi_{\hat E}$ is in control under $\Pi_{\pi}$, all actions are zero regret in terms of $V_{\hat E}$. That is, $\sum_{k=0}^{T} G = 0$.
\end{theorem}

\begin{theorem}[No regret - mixed values]
	\label{th:no_regret_ER}
	When either $\pi_{\hat E}$ or $\pi_R$ is in control under $\Pi_{\pi}$, all actions are zero regret in terms of $V_{\hat{E}R}$. That is, $\sum_{k=0}^{T} G = 0$.
\end{theorem}

In each of theses Theorems we assume our definitions for $\mathbf{X}$, $\mathbf{A}$, $\mathbf{M}$, $f$, $\Lambda$, and $\Pi_{\pi}$ are implicitly given, and a finite horizon $T$. Proofs for Theorems~\ref{th:no_regret_E} and~\ref{th:no_regret_E} follow from both reinforcement learning and our definition of information optimization having a Bellman optimality. The proof for the optimality of $V_{\hat{E}R}$ (Th~\ref{th:no_regret_ER}) is provided in the Appendix. The intuition for this proof is simple that a greedy algorithm made over two Bellman solutions is itself a Bellman solution.

In Eq.~\ref{eq:pipi} we compare reward $R$ and information value $\hat E$ but have not ensured they have the same ``units'', or are comparable valuables. We sidestep this issue by limiting the probability of having zero reward to be, itself, non-zero. That is, $p(R_t=0) > 0$. We also limit $E_0$, the first set of values, to be positive and non-zero when used with boredom, $(E_0 - \eta) \geq 0$. These constraints ensure complete ``good'' exploration (defined above) which in turn ensures optimal reward learning is possible.


\subsubsection*{Homeostatic adjustments}
We have been studying cases where reward value is fixed. That is, where the environment reward $R$ is equal to its subjective value to the animal, $\hat R$. In natural behavior though the value of reward often declines as the animal reaches satiety \cite{Keramati2014,Juechems2019,Munch2020}. This is called reward homeostasis, and it is commonly formulated as a control theory problem.

The goal in a control setting is not to maximize $R$, but to minimize the difference between collected $R$ and a set point, $\bar R$ \cite{Keramati2014,Juechems2019,Munch2020}. So, in a homeostatic setting reward value is subjective, and approximated by $\hat R \approx \bar R - R$.

Fortunately, it has been shown a reward collection policy designed for one greedy reward collection, will work for homeostasis as well \cite{Keramati2014}. However, to use a form similar to our Eq. \ref{eq:pipi} with homeostatic reward value, we need to make one modification. This is shown in Eq.~\ref{eq:pipi_h}. 

\begin{equation} 
    \label{eq:pipi_h}
    \begin{split}
        \Pi_{\pi} = 
        \begin{cases}
            \pi^*_{\hat{E}} & : \hat{E} - \eta > R - \rho \\
            \pi_R 	& : \hat{E} - \eta < R - \rho \\
        \end{cases}
    \end{split}
\end{equation}

Under homeostasis ties between values in Eq. \ref{eq:pipi} should be broken in favor of exploration. Persuing reward exploitation instead, when homeostasis is satisfied and $\hat R=0$, disturbs homeostasis. Exploitation, in the control theoretic sense of homeostatis, is therefore a suboptimal result. 

% ----------------------------------------------------------------------
\subsection*{Experiments}
Does our theory work in practice? The remainder of this paper is devoted to studying our approaches performance in simulated bandit tasks. Each task was designed to either test exploration performance in a way that matches recent experimental studies, or to test the limits of curiosity. 

\subsubsection*{Information collection}
The work so far has built up the idea that the most valuable, and most efficient, curious search will come from a deterministic algorithm. That is, every step strictly maximizes $\hat E$. It is this determinism which will let us resolve the dilemma, later on. A deterministic view of exploration seems at odds with how the problem is generally viewed today, which involves searching with some amount of randomness. Whereas if our analysis is correct, randomness is not needed or desirable, as it must lead to less value and a longer search. 

We confirm and illustrate our theoretical results for curious search using a simple information foraging task (Task 1; Fig.~\ref{fig:task_outline1}). This variation of the bandit task \cite{Sutton2018} replaces rewards with information, in this case colors. On each selection, the agent sees one of two colors according to a specific probability shown in Fig. ~\ref{fig:task_payout}a. When the relative color probabilities are about even, that arm has more entropy and so more to learn and more information value. Arms that are certain to present only one color lose their informative value quickly.

The results of this simple information seeking task are shown in Figure~\ref{fig:curiosity1}. Deterministic curiosity in this task generated more information value, in less time, and with zero regret when compared against a stochastic agent using more directed random search. As our work here predicts, noise only made the search happen slower and lead to regret. Besides offering a concrete example, performance in Task 1 is a first step in showing that even though $\pi_E$'s optimality is proven for a deterministic environment, it can perform well in other settings.

\begin{figure}

	\includegraphics[width=0.7\linewidth]{img/task_outline1.pdf} 
	\caption{A simple four choice information foraging task. The information in this task is a yellow or blue stimulus, which can change from trial to trial. A good learner in this task is one who tries to learn the probabilities of all the symbols in each of the four choices. The more random the stimuli are in a choice, the more potential information/entropy there is to learn. \textit{Note}: the information payout structure is shown below (Fig.~\ref{fig:task_payout}a}).
	\label{fig:task_outline1} 
\end{figure}

\begin{figure}
	\includegraphics[width=1.0\linewidth]{img/curiosity1.pdf} 
	\caption{Comparing deterministic versus stochastic variations of the same curiosity algorithm, in a simple information foraging task (Task 1). Deterministic results are shown in the left column, and stochastic are shown in the right. The hyperparameters for both models were found by random search, which is described in the Methods.
	\textbf{a-b}. Examples of choice behavior.
	\textbf{c-d}. Information value plotted with time for the behavior shown in a-b.
	\textbf{e-f}. Regret plotted with time for the behavior shown in a-b. Note how our only deterministic curiosity generates zero regret, inline with theoretical predictions.
	\textbf{e}. Average information value for 100 independent simulations. Large values mean a more efficient search.
	\textbf{f}. Average regret for 100 independent simulations. Ideal exploration should have no regret. 
	\textbf{g}. Number of steps it took to reach the boredom threshold $eta$. Smaller values imply a faster search.
	}
	\label{fig:curiosity1} 
\end{figure}


\subsubsection*{Reward collection} 
We cannot mathematically ensure a mixed value solution to explore-exploit problems will maximizes reward value better than other well-established methods. To find out if this is so, we measured total reward collected over 7 tasks and 10 agents. Each agent's parameters were independently optimized for each task. For a brief description of each agent, see Table~\ref{tab:agents}. They all have in common that their central goal is to maximize total reward value, though this is often supplemented by some other goal, an intrinsic reward or bonus \cite{Ng1999,Sutton1998}. 

The general form of the 7 tasks are depicted in Fig~\ref{fig:task_outline2}. As with our first task (Fig. ~\ref{fig:task_outline1}) they are all variations of the classic multi-armed bandit \cite{Sutton2018}. The payouts for each of the tasks are shown separately in Fig.~\ref{fig:task_payout}. Every trial has a set of $n$ choices. Each choice returns a ``payout’’, according to a predetermined probability. Payouts are information, a reward, or both. Note that, as in Task 1, information was symbolic, denoted by a color code, ``yellow’’ and ``blue’’ and as is custom reward was a positive real number. 

The results in full for all tasks and exploration strategies are shown in Fig.~\ref{fig:summary}. All agents, including ours, used the same exploitation policy based on the temporal difference learning rule \cite{Sutton2018} (Methods).

\begin{figure}
	\includegraphics[width=0.7\linewidth]{img/task_outline2.pdf} 
	\caption{A 4 choice reward collection task. The reward is numerical value, here a 1 or 0. A good learner in this task is one who collects the most rewards. The task depicted here matches that in Fig~\ref{fig:task_payout}b. Every other reward collection task we study has the same basic form, only the number of choices increases and the reward spaces are more complex.}
	\label{fig:task_outline2} 
\end{figure}

\begin{SCfigure*}[\sidecaptionrelwidth][t]
    \label{fig:task_payout} 
	\includegraphics[width=11.4cm]{img/task_payout.pdf} 
	\caption{Payouts for \textit{Tasks 1 - 7}. Payouts can be information, reward, or both. For comments on general task design, see Fig~\ref{fig:task_outline2}.
	\textbf{a.} A classic four-choice design for information collection. A good learner should visit each arm, but quickly discover that only arm two is information bearing.
	\textbf{b.} A classic four-choice design for testing exploration under reward collection. The learner is presented with four choices and it must discover which choice yields the highest average reward. In this task that is Choice 2. 
	\textbf{c.} A ten choice sparse reward task. The learner is presented with four choices and it must discover which choice yields the highest average reward. In this task that is Choice 8 but the very low overall rate of rewards makes this difficult to discover. Solving this task with consistency means consistent exploration. 
	\textbf{d.} A ten choice deceptive reward task. The learner is presented with 10 choices, but the choice which is the best on the long-term (>30 trials) has a lower value in the short term. This value first declines, then rises (see column 2).
	\textbf{e.} A ten choice information distraction task. The learner is presented with both information and rewards. A good learner though will realize the information does not predict reward collection, and so will ignore it.
	\textbf{f.} A 121 choice task with a complex payout structure. This task is thought to be at the limit of human performance. A good learner will eventually discover choice number 57 has the highest payout.
	\textbf{g.} This task is identical to \textit{a.}, except for the high payout choice being changed to be the lowest possible payout. This task tests how well different exploration strategies adjust to simple but sudden change in the environment.
	}
	\label{fig:task_payout} 
\end{SCfigure*}

\begin{table}[]
	\caption{Exploration strategies.}
	\label{tab:agents}
	\begin{tabular}{|p{2cm}|p{2cm}|p{3cm}|}
	\hline
	\textbf{Name} & \textbf{Class} & \textbf{Exploration strategy} \\ \hline
	Curiosity & Deterministic & Maximize information value \\ \hline
	Random/Greedy & Random & Alternates between random exploration and greedy with probability $\epsilon$. \\ \hline
	Decay/Greedy & Random & The $\epsilon$ parameter decays with a half-life \\ \hline
	Random & Random & Pure random exploration \\ \hline
	Reward & Directed & Softmax sampling of reward value \\ \hline
	Bayesian & Directed & Softmax sampling of reward value + information value \\ \hline
	Novelty & Directed & Softmax sampling of reward value + novelty signal \\ \hline
	Entropy & Directed & Softmax sampling of reward value + action entropy \\ \hline
	Count (EB) & Directed & Softmax sampling of reward value + visit counts \\ \hline
	Count (UCB) & Directed & Softmax sampling of reward value + visit counts \\ \hline
	\end{tabular}
\end{table}

\begin{SCfigure*}[\sidecaptionrelwidth][t]
	\includegraphics[width=11.4cm]{img/summary.pdf} 
	\caption{Summary of reward collection (\textit{Tasks 2-7}). The strategies in each panel are grouped according to the class of search they employed (Curiosity. Random, Extrinsic reward or Extrinsic + Intrinsic rewards). 
	\textbf{a.} Results for Task 2, which has four choices and one clear best choice.
	\textbf{b.} Results for Task 3, which has 10 choices and very sparse positive returns.
	\textbf{c.} Results for Task 4, whose best choice is initially ``deceptive'' in that it returns suboptimal reward value over the first 20 trials.
	\textbf{d.} Results for Task 5, which blends the information foraging task 1 with a larger version of Task 2. The yellow/blue stimuli are a max entropy distraction which do not predict the reward payout of each arm.
	\textbf{e.} Results for Task 6, which has 121 choices and a quite heterogeneous set of payouts but still with one best choice.
	\textbf{f.} Results for Task 7, which is identical to Task 6 except the best choice was changed to be the worst. The learners from Task 6 were trained on this Task beginning with the learned values from their prior experience -- a test of robustness to sudden change in the environment. 
	\textit{Note}: To accommodate the fact that different tasks were run different numbers of trials, we normalized total reward by trial number. This lets us plot reward collection results for all tasks on the same scale.
  	}	
	\label{fig:summary} 
\end{SCfigure*}

\begin{SCfigure*}[\sidecaptionrelwidth][t]
    \label{fig:supp_regret} 
	\includegraphics[width=11.4cm]{img/supp_regret.pdf} 
	\caption{Summary of total regret (\textit{Tasks 2-7}). See the previous figure for details.}
\end{SCfigure*}

% \begin{SCfigure*}[\sidecaptionrelwidth][t]
%     \label{fig:exploration1} 
% 	\includegraphics[width=11.4cm]{img/exploration1.pdf}
% 	\caption{Variance in exploration strategies for 10 ``animals'' (Task 1). Here we simulated a single random seed, on the same task. Model parameters were the top-10, for each strategy. In this Figure we consider each of these parameters to act as a stand in for a unique ``animal''. This figure then estimates the degree, and kind, of variability expected which would be expected in the same experiment for different (well-adapted) animals. Note the reduced variability seen when using curiosity, compared to the others.
%   	}
% \end{SCfigure*}

\textit{Task 1} is an information gathering task, which we discussed above.  We present the design and results here to contrast with the reward collection tasks. 

\textit{Task 2} was designed to examine reward collection, in probabilistic reward setting very common in the decision making literature \cite{schonberg2007reinforcement,frank2004carrot,cavanagh2014conflict,jahfari2019cross,collins2014opponent,collins2017interactions,glascher2010states}. Rewards were 0 or 1. The best choice had a payout of $p(R=1) = 0.8$. This is a much higher average payout than the others ($p(R=1) = 0.2$). At no point does the task generate symbolic information. See, Fig.~\ref{fig:task_payout}\textbf{b}. 

In contrast to the performance in Task 1, in Task 2 we expected all the exploration strategies to succeed. While this was indeed the case, our deterministic curiosity was the top-performer in terms of median rewards collected, though by a small margin (Fig.~\ref{fig:summary}\textbf{a}).

\textit{Task 3} was designed with very sparse rewards \cite{Silver2016b,Silver2018} and there were 10 choices, making this a more difficult task (Fig.~\ref{fig:task_payout}\textbf{c}). Sparse rewards are a common problem in the natural world, where an animal may have to travel and search between each meal This is a difficult but not impossible task for vertebrates \cite{anderson1984optimal} and invertebrates \cite{westphal2006foraging}. That being said, most reinforcement learning algorithms will struggle in this setting because the thing they need to learn, that is rewards, are often absent. In this task we saw quite a bit more variation in performance, with the novelty-bonus strategy taking the top slot (this is the only time it does so).

\textit{Task 4} was designed with deceptive rewards. By deceptive we mean that the best long-term option presents itself initially with a decreasing reward value (Fig.~\ref{fig:task_payout}\textbf{d}). Such small deceptions abound in many natural contexts where one must often make short-term sacrifices \cite{internicola2012bumble}. It is well known that classic reinforcement learning will often struggle in this kind of task \cite{Lehman2011a,Sutton2018}. Here our deterministic curiosity is the only strategy that reaches above chance performance. Median performance of all other strategies are similar to the random control (Fig~\ref{fig:summary}\textbf{c}). 

\textit{Task 5} was designed to fool curiosity, our algorithm, by presenting information that was utterly irrelevant to reward collection, but had very high entropy and so ``interesting'' to our algorithm. We fully anticipated that this context would fool just our algorithm, with all other strategies performing well since they are largely agnostic to entropy. However, despite being designed to fail, deterministic curiosity still produced a competitive performance to the other strategies (Fig~\ref{fig:summary}\textbf{d}). 

\textit{Tasks 6-7} were designed as a pair, with both having 121 choices, and a complex payout structure. Tasks of this size are at the limit of human performance \cite{Wu2018}. We first trained all learners on \textit{Task 6}, then tested them in Task 7 which identical to 6, except the best payout arm is reset to be worst (Fig.~\ref{fig:task_payout}\textbf{e}-\textbf{f}). In other words Tasks 6 and 7 were joined to measure learning in a high dimensional, but shifting, environments.

In Task 6 deterministic curiosity performed well, securing a second place finish. We note the Bayesian strategy  outperformed our approach. However, under the sudden non-stationarity in reward when switching tasks, the top Bayesian model became the worst on Task 7, and deterministic curiosity took the top spot. Compare Fig.~\ref{fig:summary}\textbf{e} to \textbf{f}. This is the robustness that we'd expect for any curiosity algorithm, whose main goal is to learn everything unbiased by other objectives. The environment and the objectives do change in real life, and so we must be prepared for this. Note how the other less-biased-toward-reward-value exploration models (count-based, novelty, and entropy models) also saw gains, to a lesser degree.

\subsubsection*{Robustness}
It is common to focus on best case performance, as we have above. However worst case performance is just as important in judging the usefulness of an algorithm. We therefore examined the progression from best case hyperparameters, to the worst case.  

Unlike idealized simulations, animals cannot know with perfect fidelity how their environment may change. So they cannot perfectly know the search parameters, in other words. To test the robustness of our algorithm, and all other exploration strategies we considered, we reexamined reward collection performance across all hyperparameters from our tuning set. Results for this are shown in Figure~\ref{fig:robust}. We plotted performance ranked by parameters, from best to worst.

\begin{SCfigure*}[\sidecaptionrelwidth][t]
	\includegraphics[width=11.4cm]{img/robust.pdf} 
	\caption{Parameter sensitivity (\textit{Task 2} and \textit{6}). Here we evaluated how robust performance was for random hyperparameters, rather than those which were carefully chosen. A very robustness exploration algorithm would produce strong performance with both the best parameter choices, and the worst parameter choices. 
	\textbf{a,d} Total reward for 1000 randomly choosed hyperparameters, for each of the agents.
	\textbf{b,e} Performance with the top 10 random hyperparameters, after ranking. Curiosity (green) compared to all the others.
	\textbf{c.f} Performance with the bottom 10 random hyperparameters, after ranking.
	}
	\label{fig:robust}
\end{SCfigure*}

No matter the parameter choices, exploration-as-curiosity produced top-3 performance (Figure.~\ref{fig:robust}a-bd). Most interesting to note is the performance on the worst 10 parameters. Here curiosity was markedly better, with substantially less variability.

All other exploration strategies we considered have hyperparameters to tune their \emph{degree} of exploration. For example, the temperature of noise in softmax. With our approach to curiosity, however, we tune only when exploration should stop, by $\eta$. In these examples, performance is more robust to  ``mistuning'' the stopping point, rather than the degree of exploration.
