\documentclass[9pt,twocolumn,twoside]{pnas-new}
% Use the lineno option to display guide line numbers if required.
% Note that the use of elements such as single-column equations
% may affect the guide line number alignment. 

\templatetype{pnasresearcharticle} % Choose template 
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} = Template for a one-column mathematics article
% {pnasinvited} = Template for a PNAS invited submission

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multicol}


\DeclareMathOperator*{\argmax}{argmax}
\newtheorem{corollary}{Corollary}
\newtheorem{axiom}{Axiom}
\newtheorem{statement}{Statement}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}[theorem]{Lemma}

\title{A way around the exploration-exploitation dilemma.}

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[a,1]{Erik J Peterson}
\author[a,b]{Timothy D Verstynen}
\affil[a]{Department of Psychology}
\affil[b]{Center for the Neural Basis of Cognition, Carnegie Mellon University, Pittsburgh PA}

% Please give the surname of the lead author for the running footer
\leadauthor{Peterson} 

% Please add here a significance statement to explain the relevance of your work
\significancestatement{We have derived a deterministic way for an agent to learn how to maximize reward, and to explore their world in an optimal manner. In our approach exploration is done only to maximize information value--a quantity we define axiomatically. Maximizing information value forces the animal to learn a general, reward-independent, memory of the world. An important side of this learning is that reward learning also improves, to optimality. We call this view of the reinforcement learning problem, dual value learning. Dual value learning is a simple way to avoid the exploration-exploitation dilemma. The major cost of our approach is an increase in worst-case sample efficiency.}

% Please include corresponding author, author contribution and author declaration information
% \authorcontributions{EJP?.}
\authordeclaration{The authors have no conflicts of interest to declare.}
% \equalauthors{\textsuperscript{1}A.O.(Author One) and A.T. (Author Two) contributed equally to this work (remove if not applicable).}
\correspondingauthor{\textsuperscript{1}To whom correspondence should be addressed. E-mail: Erik.Exists@gmail.com}

% Keywords are not mandatory, but authors are strongly encouraged to provide them. If provided, please include two to five keywords, separated by the pipe symbol, e.g:
% \keywords{Keyword 1 $|$ Keyword 2 $|$ Keyword 3 $|$ ...} 

\begin{abstract}
    The exploration-exploitation dilemma is a fundamental but intractable problem in the learning and decision sciences. For example,
    % TODO
    Here we challenge the common view of the dilemma--which focuses on maximizing reward--and break the problem down into two parts. We define separate mathematical objectives for exploration and exploitation. To make the objective for exploration independent, we derive a set of general axioms for information value. These let us develop two (greedy) algorithms which provably and optimally maximize information value and reward. The cost of this solution is an increase in worst-case sample efficiency when compared to a equivalent reinforcement learning problem. The severity of the cost depends on the size of the environment and the rate of rewards.
\end{abstract}

% \dates{This manuscript was compiled on \today}
% \doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}

\begin{document}
\verticaladjustment{-2pt}
\maketitle

% TODO: talk about how max E can reduce bias in ML algs, as discussed by: 
% Perspective | Published: 09 April 2019
% Lessons for artificial intelligence from the study of natural stupidity
% https://www.nature.com/articles/s42256-019-0038-z
% the r + bI approach can not reduce bias in the same way?

% TODO: make sure symbols have a colloquial def.
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}
you place a rat in maze is has never seen before, it will generally explore. Visiting each arm, sniffing the walls, and so on. This exploration often gets interpreted as curiosity, and can be formally modeled using information theory. 

On the other hand if the maze is somewhat familiar, and the animal knows to expect as reward down one of the arms, exploration often gets interpreted as search for reward, formally modeled as a reinforcement learning problem.

In the reinforcement learning problem, the rat faces a dilemma when placed in the familiar maze. Should it exploit the arm it expects a reward in, or should it explore the other unknown arms to try and find a more rewarding choice? 

Theoretical study of the dilemma proves an optimal answer is intractable. Theoretical, computational, and experimental studies have shown though it's possible to approximate an answer on average, by adding a weighted information term to the reward. Most approximate solutions are computationally complex and require one make strong assumptions about the mechanisms underlying animal cognition. 

If a problem is theoretically intractable, it is often fruitful to find a related problem that is not. To do this we challenged a basic tenant of both the dilemma and reinforcement learning--that exploration is a search for reward. We conjecture instead that exploration in a new maze (where no reward is expected) is \textit{identical} to exploration when rewards are expected (our second example). We hypothesize exploration is always motivated by the same objective--maximizing information.

\textit{If information was valuable when no reward was expected, it is just as valuable when rewards are expected.}

Our conjecture might at first seem unintuitive. It leads to questions like, ``Why would an animal explore--just for information--if it wants reward?''. We have two answers. One is practical. By removing reward as a direct objective in the search--replacing it with information--we can surprisingly guarantee the best reward is eventually found. Based on our conjecture, this paper will show how to turn the intractable dilemma into simple solvable problem in resource allocation. 

The second answer is theoretical.  The environment most animals live in is a challenging and dynamic place. In such a world, the problems of long versus short-term survival often require different strategies. We suggest the best way to ensure you eat tomorrow, is to gather maximize information today. Information seeking builds knowledge for the long-term, while greedy reward seeking often only satisfies the short-term.

Information gain, curiosity, and novelty have been studied extensively. There are a number of formalisms we could take, each of which comes with fairly strong assumptions or other requirements. As far as we can tell there is no general way to value information, and as the fewer assumptions we make the more valuable the underlying theory becomes we decided to look at the value problem fresh. Our aim is to develop a form of information value that is as general as Shannon's original formulation of information theory \cite{Shannon1948}.

To start fresh we took a new axiomatic approach to define information value $E$. Our axioms requires only two weak assumptions. 1. the animal has a memory $M$ large enough to remember its environment 2. there is a way to measure how memory changes with experience. For notational purposes we say this measurement is done with a distance $d$ (Def~\ref{def:distance}) between any two pairs of memories, $((M_{t}, M_{t+1})$. That is, we being with the assertion that information value is purely relative concept, and subjective one.

\begin{definition}
    \label{def:distance}
    We define a distance $d$ to be any function where $d(x,y) \geq 0$ and $d(x,y) = 0$ if and only if $x = y$.
\end{definition}

It is possible to define information value in Bayesian terms, by assuming $M$ is a probability distribution and using the KL divergence as $d$. This leads to an equivilance between our notion of value, and information gain as well as Bayesian surprise. Doing this means though the validity of any solution to the dilemma we find depends on a Bayesian memory, which could prove a burdensome requirement if it turns out that animals are not general Bayesian reasoning systems. Instead, we define memory as generally as possible (Def~\ref{def:memory}). 

We want to arrive at a general notion for information value $E$, based on general notion of memory $M$. The simplest memory we could try and base $E$ on would be buffer large enough for a single item. From a subjective point of view though in this memory all messages must be of equal value. For example, a message at time $t$ might have entropy $H_{high}$ with 4.02 bits. Another at $t+1$ could have a much lower entropy of $H_{low}$ = 0.23 bits.  It would be tempting for an outside observer to say the high entropy message is more valuable than the low entropy, with a value found by simple subtraction $H_{high} - H_{low}$. But our one-slot memory can't calculate this, or any other comparison. To compare it would have to remember both quantities. Note: to stay consistent with the requirement value is subjective, the $E$ can't be based on entropy. That is, $E \neq H$. This leaves the memory with no choice to assign the same (arbitrary) value to all messages.

% Not sure the perfect example leads to ours. Tighten this up.
Instead of having only a one item memory, we could let the memory have a infinite capacity. In principle this could compare any message it receives. An infinite perfect memory is not a realistic assumption for any physical system, whether it is engineered or biological. The notion of memory we use is compromise and has the following properties--it starts empty, is finite, and can forget anything it can remember (Def.~\ref{def:memory}). 

\begin{definition}
    \label{def:memory}
    Let a memory $M$ be finite set whose maximum size is $N$--defined over a finite state space $s \in S^N$--whose elements are added by an invertible encoder $f$, such that $M_{t+1} = f(M_{t}, s)$ and $M_{t} = f^{-1}(M_{t+1}, s)$ and whose elements $z$ from $M$ are recovered by a decoder $g$, such that $z = g(M, s)$. The initial memory $M_{0}$ is said to be the empty set, $M_{0} = \emptyset$.
\end{definition}

$M$ in Def.~\ref{def:memory} is abstract. To build intuition let us consider several encoder/decoder pairs, and how they relate to more concrete ideas of memory.

As a first example let's set $f$ and $g$ as identity functions; functions that return what they are given. $M$ becomes a simple record of which states are visited. It's obvious that if you can add a state to $M$, you can take it out. The requirement for invertibility / forgettability is met. One way to measure the distance between two memories of this kind is to compare their size (i.e., cardinality).

If instead $f$ and $g$ are taken to both be probability functions, then $M$ becomes a probability distribution. If you can increases the probability estimate of a state you can equally decrease it, satisfying invertibility. Likewise, there are a number of ways to measure the distance between two distributions (e.g., the KL divergence).

If $f$ is temporal encoder, which adds both the state $s$ and the time $t$ to $M$, then we have temporal memory. This is just as reversible as our first example. From this kind of memory we could imagine applying a range of $g$ decoders, including dimensionality reduction techniques (\textit{e.g.} PCA, NMF, tSNE), clustering methods (k-NN), and mixture models (GMM, kernel). Each of these is, again, is associated with one or more was to measure distance between memories (e.g., the euclidian distance).

Having a memory $M$ and a distance $d$ seem to be the minimal requirements for valuing information. We work under the idea that these two quantities are also  sufficient (Axiom~\ref{ax:1}).

Bayesian cognition and the broader field of information geometry do offer strong guidance for the kinds of dynamics and properties a fully general definition of information value should have. As a result the axioms are consistent with Bayesian-ism and information geometry, while generalizing key components. 

\begin{axiom} 
    Information value $E$ dependents only on the two most recent memories, $M_{t+1}$ and $M_{t}$.
    \label{ax:1}
\end{axiom} \\
\noindent

\begin{axiom}
    $E \geq 0$. New information about the world is always \textit{in principle} valuable.
    \label{ax:2}
\end{axiom}
\noindent

\begin{axiom}
    $E = 0$ if and only if $M_{t+1} = M_{t}$. If you remember something perfectly there is no value in learning it again. 
    \label{ax:3}
\end{axiom}
\noindent

The contents of memory are potentially arbitrary. As a result there is no way to ahead of time say what comparison to make when valuing information. The simplest and most robust solution is to simply value each state against the memory as a whole (Axiom~\ref{ax:4}. 
\begin{axiom}
    $E$ is monotonic with the average change in $M$.
    \label{ax:4}
\end{axiom}
\noindent

In general, specific information is more valuable than less specific information. As surrogate for specificity, we formalize this with the geometric notion of compactness.
\begin{axiom}
    $E$ is monotonic with the compactness $C$ of the change $M$ (Eq.~\ref{eq:compactcude}).
    \label{ax:5}
\end{axiom}
\noindent

% TODO: general fisher info axiom

As with the general definition of memory (Def~\ref{def:memory}) the axioms are abstract. It might seem natural then to fill in the details for $M$ and $d$ then discuss how to maximize $E$ for different realizations. It turns out though that the basic mathematical properties guaranteed by the axioms provide is sufficient to derive an optimal policy to maximize total $E$. 

The Bellman equation provides the optimal policy to maximize $E$ over that state-space $S$ for any memory $M$ which meets Def~\ref{def:memory}. In Theorems~\ref{theorem:opt_sub} we prove $M$ has a the key property needed to invoke the Bellman equation (i.e. optimal substructure). The proof depends critically on the idea that the memory $M$ is invertible--that items can be forgotten. It's the simple idea of (purposeful) forgetting that allows us to prove that $M$ is a valid target for dynamic programming.

% TODO Bellman: in E terms? Do F def?

Having a policy to maximize $E$ does not in principle ensure that the resulting exploration is good quality. It is however. The dynamic programming solution to maximize information value is also a dynamic programming solution for exploration. Relying on Axiom 6, we can prove that our greedy optimal policy $\pi^*_E$ leads to a complete and exhaustive exploration of any finite space $S$.

\begin{enumerate}[noitemsep,wide=0pt,leftmargin=\dimexpr\labelwidth+2\labelsep\relax]
    \item The optimal policy $\pi^*_E$ must visit each state in $s \in S$ at least once (Theorem~\ref{theorem:Z}).
    \item The optimal policy $\pi^*_E$ must revisit each $s \in S$ until learning about each state $s$ plateaus when $g(M_{t+1}, s) = g(M_{t},s)$. (Theorem~\ref{theorem:convergence})
\end{enumerate}

Eq~\ref{} gives us a recipe to maximize $E$. There exists a number of reinforcement learning algorithms. We shown one example in Eq~\ref{}. The question is how to maximize both (Eq.~\ref{}) and maintain their individual properties.

If we regard animal behavior as fixed resource--where only one action can be taken at the time--we can imagine the greedy policies in Eq~\ref{} and Eq~\ref{} are simply two possible ``jobs'' competing for ``execution'' (behavioral control). This view turns the dilemma into a problem of optimal scheduling. 

The solution to scheduling problems with 1. non-negative value and 2. identical run times is known to be a simple greedy algorithm (Theorem~\ref{}). (As each policy only results in a single action $a \sim A$ it's reasonable to say the time it takes to make an action (i.e. the run time) is a constant value).

% TODO: meta-greedy

The scheduling algorithm in Eq~\ref{} will maximize the joint value of $R$ and $E$, but does not ensure each maintains its properties. The constraints we introduce (Eq~\ref{})) ensure $E$ will plateau, and that when it does the reward policy takes control and always chooses the highest value action. 

In stochastic environments $M$ may show small continual fluctuations which get reflected in $E$. To allow Eq~\ref{pipi} to achieve a stable solution we introduce the parameter $\epsilon$, where $\epsilon > 0$ and $E + \epsilon \geq 0$. Likewise to ensure the default policy is greedy reward maximization we break ties between $R$ and $E$ in favor or reward.

In contrast to our approach, a common way to approximate a solution to the dilemma is shown in Eq~\ref{}. In this form the reward $R$ is supplemented by some fictive reward, or information term, $I$. $I$ is typically rescaled by the parameter $\beta$ (if $\beta = 1$ there is no scaling). The true aim here it maximize $R$ but to encourage exploration we add $I$. The term $I$ can be a information theoretic term (e.g., entropy, mutual information, or the KL divergence), a novelty bonus, or a curiosity/error term. Regardless of choice, all these realizations share a common limitation--finding the optimal reward is not certain. Some $I$ will succeed in some cases and $\beta$ choices while others won't. 

Trying to maximize reward creates a dilemma. Trying to maximize information doesn't. An information search will explore relentlessly and always find the best reward, eventually (Theorem~\ref{}). The cost to this search isn't a loss of value, it is time.  


\bibliography{library}
\end{document}